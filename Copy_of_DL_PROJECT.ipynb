{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Vaishnavi-Upadhyay/ChatBotfinal/blob/main/Copy_of_DL_PROJECT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zAGcQjCjau50",
        "outputId": "b7ebc10a-7149-4898-9567-d07805dda7d2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tflearn\n",
            "  Downloading tflearn-0.5.0.tar.gz (107 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m107.3/107.3 KB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.9/dist-packages (2.12.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from tflearn) (1.22.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.9/dist-packages (from tflearn) (1.16.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.9/dist-packages (from tflearn) (8.4.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.13,>=2.12.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (2.12.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (1.53.0)\n",
            "Requirement already satisfied: jax>=0.3.15 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (0.4.7)\n",
            "Requirement already satisfied: keras<2.13,>=2.12.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (2.12.0)\n",
            "Requirement already satisfied: tensorboard<2.13,>=2.12 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (2.12.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (3.8.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (3.20.3)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (0.32.0)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (23.3.3)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from tensorflow) (67.6.1)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (2.2.0)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (0.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from tensorflow) (23.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (4.5.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (1.14.1)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (16.0.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.9/dist-packages (from astunparse>=1.6.0->tensorflow) (0.40.0)\n",
            "Requirement already satisfied: ml-dtypes>=0.0.3 in /usr/local/lib/python3.9/dist-packages (from jax>=0.3.15->tensorflow) (0.0.4)\n",
            "Requirement already satisfied: scipy>=1.7 in /usr/local/lib/python3.9/dist-packages (from jax>=0.3.15->tensorflow) (1.10.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (2.17.0)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (2.2.3)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (2.27.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (1.8.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (3.4.3)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (0.7.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (0.4.6)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (4.9)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (5.3.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (0.2.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.9/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.13,>=2.12->tensorflow) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.9/dist-packages (from markdown>=2.6.8->tensorboard<2.13,>=2.12->tensorflow) (6.1.0)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow) (2.0.12)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow) (2022.12.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow) (3.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.9/dist-packages (from werkzeug>=1.0.1->tensorboard<2.13,>=2.12->tensorflow) (2.1.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.9/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.13,>=2.12->tensorflow) (3.15.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.9/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.9/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.13,>=2.12->tensorflow) (3.2.2)\n",
            "Building wheels for collected packages: tflearn\n",
            "  Building wheel for tflearn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for tflearn: filename=tflearn-0.5.0-py3-none-any.whl size=127299 sha256=532f2b5faddf10c817a1a1fcd163b0f10656959ba0f03f931edba150b11439c9\n",
            "  Stored in directory: /root/.cache/pip/wheels/4a/d5/f8/9585b4a100c0fd73da204ee785457d67c85e1b9050f009a849\n",
            "Successfully built tflearn\n",
            "Installing collected packages: tflearn\n",
            "Successfully installed tflearn-0.5.0\n",
            "absl-py==1.4.0\n",
            "alabaster==0.7.13\n",
            "albumentations==1.2.1\n",
            "altair==4.2.2\n",
            "appdirs==1.4.4\n",
            "argon2-cffi==21.3.0\n",
            "argon2-cffi-bindings==21.2.0\n",
            "arviz==0.15.1\n",
            "astropy==5.2.1\n",
            "astunparse==1.6.3\n",
            "attrs==22.2.0\n",
            "audioread==3.0.0\n",
            "autograd==1.5\n",
            "Babel==2.12.1\n",
            "backcall==0.2.0\n",
            "beautifulsoup4==4.11.2\n",
            "bleach==6.0.0\n",
            "blis==0.7.9\n",
            "bokeh==2.4.3\n",
            "branca==0.6.0\n",
            "CacheControl==0.12.11\n",
            "cached-property==1.5.2\n",
            "cachetools==5.3.0\n",
            "catalogue==2.0.8\n",
            "certifi==2022.12.7\n",
            "cffi==1.15.1\n",
            "chardet==4.0.0\n",
            "charset-normalizer==2.0.12\n",
            "chex==0.1.7\n",
            "click==8.1.3\n",
            "cloudpickle==2.2.1\n",
            "cmake==3.25.2\n",
            "cmdstanpy==1.1.0\n",
            "colorcet==3.0.1\n",
            "colorlover==0.3.0\n",
            "community==1.0.0b1\n",
            "confection==0.0.4\n",
            "cons==0.4.5\n",
            "contextlib2==0.6.0.post1\n",
            "contourpy==1.0.7\n",
            "convertdate==2.4.0\n",
            "cryptography==40.0.1\n",
            "cufflinks==0.17.3\n",
            "cvxopt==1.3.0\n",
            "cvxpy==1.3.1\n",
            "cycler==0.11.0\n",
            "cymem==2.0.7\n",
            "Cython==0.29.33\n",
            "dask==2022.12.1\n",
            "datascience==0.17.6\n",
            "db-dtypes==1.0.5\n",
            "dbus-python==1.2.16\n",
            "debugpy==1.6.6\n",
            "decorator==4.4.2\n",
            "defusedxml==0.7.1\n",
            "distributed==2022.12.1\n",
            "dlib==19.24.1\n",
            "dm-tree==0.1.8\n",
            "docutils==0.16\n",
            "dopamine-rl==4.0.6\n",
            "earthengine-api==0.1.346\n",
            "easydict==1.10\n",
            "ecos==2.0.12\n",
            "editdistance==0.6.2\n",
            "en-core-web-sm @ https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.5.0/en_core_web_sm-3.5.0-py3-none-any.whl\n",
            "entrypoints==0.4\n",
            "ephem==4.1.4\n",
            "et-xmlfile==1.1.0\n",
            "etils==1.1.1\n",
            "etuples==0.3.8\n",
            "exceptiongroup==1.1.1\n",
            "fastai==2.7.12\n",
            "fastcore==1.5.29\n",
            "fastdownload==0.0.7\n",
            "fastjsonschema==2.16.3\n",
            "fastprogress==1.0.3\n",
            "fastrlock==0.8.1\n",
            "filelock==3.10.7\n",
            "firebase-admin==5.3.0\n",
            "Flask==2.2.3\n",
            "flatbuffers==23.3.3\n",
            "flax==0.6.8\n",
            "folium==0.14.0\n",
            "fonttools==4.39.3\n",
            "frozendict==2.3.6\n",
            "fsspec==2023.3.0\n",
            "future==0.18.3\n",
            "gast==0.4.0\n",
            "GDAL==3.3.2\n",
            "gdown==4.6.6\n",
            "gensim==4.3.1\n",
            "geographiclib==2.0\n",
            "geopy==2.3.0\n",
            "gin-config==0.5.0\n",
            "glob2==0.7\n",
            "google==2.0.3\n",
            "google-api-core==2.11.0\n",
            "google-api-python-client==2.70.0\n",
            "google-auth==2.17.0\n",
            "google-auth-httplib2==0.1.0\n",
            "google-auth-oauthlib==0.4.6\n",
            "google-cloud-bigquery==3.4.2\n",
            "google-cloud-bigquery-storage==2.19.1\n",
            "google-cloud-core==2.3.2\n",
            "google-cloud-datastore==2.11.1\n",
            "google-cloud-firestore==2.7.3\n",
            "google-cloud-language==2.6.1\n",
            "google-cloud-storage==2.7.0\n",
            "google-cloud-translate==3.8.4\n",
            "google-colab @ file:///colabtools/dist/google-colab-1.0.0.tar.gz\n",
            "google-crc32c==1.5.0\n",
            "google-pasta==0.2.0\n",
            "google-resumable-media==2.4.1\n",
            "googleapis-common-protos==1.59.0\n",
            "googledrivedownloader==0.4\n",
            "graphviz==0.20.1\n",
            "greenlet==2.0.2\n",
            "grpcio==1.53.0\n",
            "grpcio-status==1.48.2\n",
            "gspread==3.4.2\n",
            "gspread-dataframe==3.0.8\n",
            "gym==0.25.2\n",
            "gym-notices==0.0.8\n",
            "h5netcdf==1.1.0\n",
            "h5py==3.8.0\n",
            "HeapDict==1.0.1\n",
            "hijri-converter==2.2.4\n",
            "holidays==0.21.13\n",
            "holoviews==1.15.4\n",
            "html5lib==1.1\n",
            "htmlmin==0.1.12\n",
            "httpimport==1.3.0\n",
            "httplib2==0.21.0\n",
            "humanize==4.6.0\n",
            "hyperopt==0.2.7\n",
            "idna==3.4\n",
            "ImageHash==4.3.1\n",
            "imageio==2.25.1\n",
            "imageio-ffmpeg==0.4.8\n",
            "imagesize==1.4.1\n",
            "imbalanced-learn==0.10.1\n",
            "imgaug==0.4.0\n",
            "importlib-metadata==6.1.0\n",
            "importlib-resources==5.12.0\n",
            "imutils==0.5.4\n",
            "inflect==6.0.2\n",
            "iniconfig==2.0.0\n",
            "intel-openmp==2023.0.0\n",
            "ipykernel==5.5.6\n",
            "ipython==7.34.0\n",
            "ipython-genutils==0.2.0\n",
            "ipython-sql==0.4.1\n",
            "ipywidgets==7.7.1\n",
            "itsdangerous==2.1.2\n",
            "jax==0.4.7\n",
            "jaxlib @ https://storage.googleapis.com/jax-releases/cuda11/jaxlib-0.4.7+cuda11.cudnn86-cp39-cp39-manylinux2014_x86_64.whl\n",
            "jieba==0.42.1\n",
            "Jinja2==3.1.2\n",
            "joblib==1.1.1\n",
            "jsonpickle==3.0.1\n",
            "jsonschema==4.3.3\n",
            "jupyter-client==6.1.12\n",
            "jupyter-console==6.1.0\n",
            "jupyter_core==5.3.0\n",
            "jupyterlab-pygments==0.2.2\n",
            "jupyterlab-widgets==3.0.7\n",
            "kaggle==1.5.13\n",
            "keras==2.12.0\n",
            "keras-vis==0.4.1\n",
            "kiwisolver==1.4.4\n",
            "korean-lunar-calendar==0.3.1\n",
            "langcodes==3.3.0\n",
            "lazy_loader==0.2\n",
            "libclang==16.0.0\n",
            "librosa==0.10.0.post2\n",
            "lightgbm==3.3.5\n",
            "llvmlite==0.39.1\n",
            "locket==1.0.0\n",
            "logical-unification==0.4.5\n",
            "LunarCalendar==0.0.9\n",
            "lxml==4.9.2\n",
            "Markdown==3.4.3\n",
            "markdown-it-py==2.2.0\n",
            "MarkupSafe==2.1.2\n",
            "matplotlib==3.7.1\n",
            "matplotlib-inline==0.1.6\n",
            "matplotlib-venn==0.11.9\n",
            "mdurl==0.1.2\n",
            "miniKanren==1.0.3\n",
            "missingno==0.5.2\n",
            "mistune==0.8.4\n",
            "mizani==0.8.1\n",
            "mkl==2019.0\n",
            "ml-dtypes==0.0.4\n",
            "mlxtend==0.14.0\n",
            "more-itertools==9.1.0\n",
            "moviepy==1.0.3\n",
            "mpmath==1.3.0\n",
            "msgpack==1.0.5\n",
            "multimethod==1.9.1\n",
            "multipledispatch==0.6.0\n",
            "multitasking==0.0.11\n",
            "murmurhash==1.0.9\n",
            "music21==8.1.0\n",
            "natsort==8.3.1\n",
            "nbclient==0.7.2\n",
            "nbconvert==6.5.4\n",
            "nbformat==5.8.0\n",
            "nest-asyncio==1.5.6\n",
            "networkx==3.0\n",
            "nibabel==3.0.2\n",
            "nltk==3.8.1\n",
            "notebook==6.3.0\n",
            "numba==0.56.4\n",
            "numexpr==2.8.4\n",
            "numpy==1.22.4\n",
            "oauth2client==4.1.3\n",
            "oauthlib==3.2.2\n",
            "opencv-contrib-python==4.7.0.72\n",
            "opencv-python==4.7.0.72\n",
            "opencv-python-headless==4.7.0.72\n",
            "openpyxl==3.0.10\n",
            "opt-einsum==3.3.0\n",
            "optax==0.1.4\n",
            "orbax==0.1.6\n",
            "osqp==0.6.2.post0\n",
            "packaging==23.0\n",
            "palettable==3.3.0\n",
            "pandas==1.4.4\n",
            "pandas-datareader==0.10.0\n",
            "pandas-gbq==0.17.9\n",
            "pandas-profiling==3.2.0\n",
            "pandocfilters==1.5.0\n",
            "panel==0.14.4\n",
            "param==1.13.0\n",
            "parso==0.8.3\n",
            "partd==1.3.0\n",
            "pathlib==1.0.1\n",
            "pathy==0.10.1\n",
            "patsy==0.5.3\n",
            "pep517==0.13.0\n",
            "pexpect==4.8.0\n",
            "phik==0.12.3\n",
            "pickleshare==0.7.5\n",
            "Pillow==8.4.0\n",
            "pip-tools==6.6.2\n",
            "platformdirs==3.2.0\n",
            "plotly==5.13.1\n",
            "plotnine==0.10.1\n",
            "pluggy==1.0.0\n",
            "pooch==1.6.0\n",
            "portpicker==1.3.9\n",
            "prefetch-generator==1.0.3\n",
            "preshed==3.0.8\n",
            "prettytable==0.7.2\n",
            "proglog==0.1.10\n",
            "progressbar2==4.2.0\n",
            "prometheus-client==0.16.0\n",
            "promise==2.3\n",
            "prompt-toolkit==3.0.38\n",
            "prophet==1.1.2\n",
            "proto-plus==1.22.2\n",
            "protobuf==3.20.3\n",
            "psutil==5.9.4\n",
            "psycopg2==2.9.5\n",
            "ptyprocess==0.7.0\n",
            "py4j==0.10.9.7\n",
            "pyarrow==9.0.0\n",
            "pyasn1==0.4.8\n",
            "pyasn1-modules==0.2.8\n",
            "pycocotools==2.0.6\n",
            "pycparser==2.21\n",
            "pyct==0.5.0\n",
            "pydantic==1.10.7\n",
            "pydata-google-auth==1.7.0\n",
            "pydot==1.4.2\n",
            "pydot-ng==2.0.0\n",
            "pydotplus==2.0.2\n",
            "PyDrive==1.3.1\n",
            "pyerfa==2.0.0.3\n",
            "pygame==2.3.0\n",
            "Pygments==2.14.0\n",
            "PyGObject==3.36.0\n",
            "pymc==5.1.2\n",
            "PyMeeus==0.5.12\n",
            "pymystem3==0.2.0\n",
            "PyOpenGL==3.1.6\n",
            "pyparsing==3.0.9\n",
            "pyrsistent==0.19.3\n",
            "PySocks==1.7.1\n",
            "pytensor==2.10.1\n",
            "pytest==7.2.2\n",
            "python-apt==0.0.0\n",
            "python-dateutil==2.8.2\n",
            "python-louvain==0.16\n",
            "python-slugify==8.0.1\n",
            "python-utils==3.5.2\n",
            "pytz==2022.7.1\n",
            "pytz-deprecation-shim==0.1.0.post0\n",
            "pyviz-comms==2.2.1\n",
            "PyWavelets==1.4.1\n",
            "PyYAML==6.0\n",
            "pyzmq==23.2.1\n",
            "qdldl==0.1.5.post3\n",
            "qudida==0.0.4\n",
            "regex==2022.10.31\n",
            "requests==2.27.1\n",
            "requests-oauthlib==1.3.1\n",
            "requests-unixsocket==0.2.0\n",
            "rich==13.3.3\n",
            "rpy2==3.5.5\n",
            "rsa==4.9\n",
            "scikit-image==0.19.3\n",
            "scikit-learn==1.2.2\n",
            "scipy==1.10.1\n",
            "screen-resolution-extra==0.0.0\n",
            "scs==3.2.2\n",
            "seaborn==0.12.2\n",
            "Send2Trash==1.8.0\n",
            "shapely==2.0.1\n",
            "six==1.16.0\n",
            "sklearn-pandas==2.2.0\n",
            "smart-open==6.3.0\n",
            "snowballstemmer==2.2.0\n",
            "sortedcontainers==2.4.0\n",
            "soundfile==0.12.1\n",
            "soupsieve==2.4\n",
            "soxr==0.3.4\n",
            "spacy==3.5.1\n",
            "spacy-legacy==3.0.12\n",
            "spacy-loggers==1.0.4\n",
            "Sphinx==3.5.4\n",
            "sphinxcontrib-applehelp==1.0.4\n",
            "sphinxcontrib-devhelp==1.0.2\n",
            "sphinxcontrib-htmlhelp==2.0.1\n",
            "sphinxcontrib-jsmath==1.0.1\n",
            "sphinxcontrib-qthelp==1.0.3\n",
            "sphinxcontrib-serializinghtml==1.1.5\n",
            "SQLAlchemy==1.4.47\n",
            "sqlparse==0.4.3\n",
            "srsly==2.4.6\n",
            "statsmodels==0.13.5\n",
            "sympy==1.11.1\n",
            "tables==3.7.0\n",
            "tabulate==0.8.10\n",
            "tangled-up-in-unicode==0.2.0\n",
            "tblib==1.7.0\n",
            "tenacity==8.2.2\n",
            "tensorboard==2.12.0\n",
            "tensorboard-data-server==0.7.0\n",
            "tensorboard-plugin-wit==1.8.1\n",
            "tensorflow==2.12.0\n",
            "tensorflow-datasets==4.8.3\n",
            "tensorflow-estimator==2.12.0\n",
            "tensorflow-gcs-config==2.12.0\n",
            "tensorflow-hub==0.13.0\n",
            "tensorflow-io-gcs-filesystem==0.32.0\n",
            "tensorflow-metadata==1.12.0\n",
            "tensorflow-probability==0.19.0\n",
            "tensorstore==0.1.35\n",
            "termcolor==2.2.0\n",
            "terminado==0.17.1\n",
            "text-unidecode==1.3\n",
            "textblob==0.17.1\n",
            "tf-slim==1.1.0\n",
            "tflearn==0.5.0\n",
            "thinc==8.1.9\n",
            "threadpoolctl==3.1.0\n",
            "tifffile==2023.3.21\n",
            "tinycss2==1.2.1\n",
            "toml==0.10.2\n",
            "tomli==2.0.1\n",
            "toolz==0.12.0\n",
            "torch @ https://download.pytorch.org/whl/cu116/torch-1.13.1%2Bcu116-cp39-cp39-linux_x86_64.whl\n",
            "torchaudio @ https://download.pytorch.org/whl/cu116/torchaudio-0.13.1%2Bcu116-cp39-cp39-linux_x86_64.whl\n",
            "torchsummary==1.5.1\n",
            "torchtext==0.14.1\n",
            "torchvision @ https://download.pytorch.org/whl/cu116/torchvision-0.14.1%2Bcu116-cp39-cp39-linux_x86_64.whl\n",
            "tornado==6.2\n",
            "tqdm==4.65.0\n",
            "traitlets==5.7.1\n",
            "tweepy==4.13.0\n",
            "typer==0.7.0\n",
            "typing_extensions==4.5.0\n",
            "tzdata==2023.3\n",
            "tzlocal==4.3\n",
            "uritemplate==4.1.1\n",
            "urllib3==1.26.15\n",
            "vega-datasets==0.9.0\n",
            "visions==0.7.4\n",
            "wasabi==1.1.1\n",
            "wcwidth==0.2.6\n",
            "webcolors==1.13\n",
            "webencodings==0.5.1\n",
            "Werkzeug==2.2.3\n",
            "widgetsnbextension==3.6.4\n",
            "wordcloud==1.8.2.2\n",
            "wrapt==1.14.1\n",
            "xarray==2022.12.0\n",
            "xarray-einstats==0.5.1\n",
            "xgboost==1.7.4\n",
            "xkit==0.0.0\n",
            "xlrd==2.0.1\n",
            "yellowbrick==1.5\n",
            "yfinance==0.2.14\n",
            "zict==2.2.0\n",
            "zipp==3.15.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.9/dist-packages/tensorflow/python/compat/v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "non-resource variables are not supported in the long term\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.stem.lancaster import LancasterStemmer\n",
        "stemmer = LancasterStemmer()\n",
        "\n",
        "# Libraries needed for Tensorflow processing\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "!pip install tflearn tensorflow  # tflearn requires tensorflow on my mac\n",
        "!pip freeze  # to get a list of the modules\n",
        "import tflearn\n",
        "import random\n",
        "import json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "UbfltUnvbRw8",
        "outputId": "9e16ff86-484c-4d03-dc88-f4d9914fae31"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-26232214-8caf-4473-9cd7-4a95784511ac\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-26232214-8caf-4473-9cd7-4a95784511ac\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving Intent.json to Intent.json\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Intent.json': b'{\\r\\n      \"intents\": [\\r\\n            {\\r\\n                  \"intent\": \"Greeting\",\\r\\n                  \"text\": [\\r\\n                        \"Hi\",\\r\\n                        \"Hi there\",\\r\\n                        \"Hola\",\\r\\n                        \"Hello\",\\r\\n                        \"Hello there\",\\r\\n                        \"Hya\",\\r\\n                        \"Hya there\"\\r\\n                  ],\\r\\n                  \"responses\": [\\r\\n                        \"Hi human, please tell me your GeniSys user\",\\r\\n                        \"Hello human, please tell me your GeniSys user\",\\r\\n                        \"Hola human, please tell me your GeniSys user\"\\r\\n                  ],\\r\\n                  \"extension\":  {\\r\\n                        \"function\": \"\",\\r\\n                        \"entities\": false,\\r\\n                        \"responses\": []\\r\\n                  },\\r\\n                  \"context\":  {\\r\\n                        \"in\": \"\",\\r\\n                        \"out\": \"GreetingUserRequest\",\\r\\n                        \"clear\": false\\r\\n                  },\\r\\n                  \"entityType\": \"NA\",\\r\\n                  \"entities\": []\\r\\n            },\\r\\n            {\\r\\n                  \"intent\": \"GreetingResponse\",\\r\\n                  \"text\": [\\r\\n                        \"My user is Adam\",\\r\\n                        \"This is Adam\",\\r\\n                        \"I am Adam\",\\r\\n                        \"It is Adam\",\\r\\n                        \"My user is Bella\",\\r\\n                        \"This is Bella\",\\r\\n                        \"I am Bella\",\\r\\n                        \"It is Bella\"\\r\\n                  ],\\r\\n                  \"responses\": [\\r\\n                        \"Great! Hi <HUMAN>! How can I help?\",\\r\\n                        \"Good! Hi <HUMAN>, how can I help you?\",\\r\\n                        \"Cool! Hello <HUMAN>, what can I do for you?\",\\r\\n                        \"OK! Hola <HUMAN>, how can I help you?\",\\r\\n                        \"OK! hi <HUMAN>, what can I do for you?\"\\r\\n                  ],\\r\\n                  \"extension\":  {\\r\\n                        \"function\": \"extensions.gHumans.updateHuman\",\\r\\n                        \"entities\": true,\\r\\n                        \"responses\": [\\r\\n                              \"Hi %%HUMAN%%! How can I help?\",\\r\\n                              \"Hi %%HUMAN%%, how can I help you?\",\\r\\n                              \"Hello %%HUMAN%%, what can I do for you?\",\\r\\n                              \"Hola %%HUMAN%%, how can I help you?\",\\r\\n                              \"OK hi %%HUMAN%%, what can I do for you?\"\\r\\n                        ]\\r\\n                  },\\r\\n                  \"context\":  {\\r\\n                        \"in\": \"GreetingUserRequest\",\\r\\n                        \"out\": \"\",\\r\\n                        \"clear\": true\\r\\n                  },\\r\\n                  \"entityType\": \"NA\",\\r\\n                  \"entities\": [\\r\\n                        {\\r\\n                              \"entity\": \"HUMAN\",\\r\\n                              \"rangeFrom\": 3,\\r\\n                              \"rangeTo\": 4\\r\\n                        },\\r\\n                        {\\r\\n                              \"entity\": \"HUMAN\",\\r\\n                              \"rangeFrom\": 2,\\r\\n                              \"rangeTo\": 3\\r\\n                        },\\r\\n                        {\\r\\n                              \"entity\": \"HUMAN\",\\r\\n                              \"rangeFrom\": 1,\\r\\n                              \"rangeTo\": 2\\r\\n                        },\\r\\n                        {\\r\\n                              \"entity\": \"HUMAN\",\\r\\n                              \"rangeFrom\": 2,\\r\\n                              \"rangeTo\": 3\\r\\n                        },\\r\\n                        {\\r\\n                              \"entity\": \"HUMAN\",\\r\\n                              \"rangeFrom\": 3,\\r\\n                              \"rangeTo\": 4\\r\\n                        },\\r\\n                        {\\r\\n                              \"entity\": \"HUMAN\",\\r\\n                              \"rangeFrom\": 2,\\r\\n                              \"rangeTo\": 3\\r\\n                        },\\r\\n                        {\\r\\n                              \"entity\": \"HUMAN\",\\r\\n                              \"rangeFrom\": 1,\\r\\n                              \"rangeTo\": 2\\r\\n                        },\\r\\n                        {\\r\\n                              \"entity\": \"HUMAN\",\\r\\n                              \"rangeFrom\": 2,\\r\\n                              \"rangeTo\": 3\\r\\n                        }\\r\\n                  ]\\r\\n            },\\r\\n            {\\r\\n                  \"intent\": \"CourtesyGreeting\",\\r\\n                  \"text\": [\\r\\n                        \"How are you?\",\\r\\n                        \"Hi how are you?\",\\r\\n                        \"Hello how are you?\",\\r\\n                        \"Hola how are you?\",\\r\\n                        \"How are you doing?\",\\r\\n                        \"Hope you are doing well?\",\\r\\n                        \"Hello hope you are doing well?\"\\r\\n                  ],\\r\\n                  \"responses\": [\\r\\n                        \"Hello, I am great, how are you? Please tell me your GeniSys user\",\\r\\n                        \"Hello, how are you? I am great thanks! Please tell me your GeniSys user\",\\r\\n                        \"Hello, I am good thank you, how are you? Please tell me your GeniSys user\",\\r\\n                        \"Hi, I am great, how are you? Please tell me your GeniSys user\",\\r\\n                        \"Hi, how are you? I am great thanks! Please tell me your GeniSys user\",\\r\\n                        \"Hi, I am good thank you, how are you? Please tell me your GeniSys user\",\\r\\n                        \"Hi, good thank you, how are you? Please tell me your GeniSys user\"\\r\\n                  ],\\r\\n                  \"extension\":  {\\r\\n                        \"function\": \"\",\\r\\n                        \"entities\": false,\\r\\n                        \"responses\": []\\r\\n                  },\\r\\n                  \"context\":  {\\r\\n                        \"in\": \"\",\\r\\n                        \"out\": \"CourtesyGreetingUserRequest\",\\r\\n                        \"clear\": true\\r\\n                  },\\r\\n                  \"entityType\": \"NA\",\\r\\n                  \"entities\": []\\r\\n            },\\r\\n            {\\r\\n                  \"intent\": \"CourtesyGreetingResponse\",\\r\\n                  \"text\": [\\r\\n                        \"Good thanks! My user is Adam\",\\r\\n                        \"Good thanks! This is Adam\",\\r\\n                        \"Good thanks! I am Adam\",\\r\\n                        \"Good thanks! It is Adam\",\\r\\n                        \"Great thanks! My user is Bella\",\\r\\n                        \"Great thanks! This is Bella\",\\r\\n                        \"Great thanks! I am Bella\",\\r\\n                        \"Great thanks! It is Bella\"\\r\\n                  ],\\r\\n                  \"responses\": [\\r\\n                        \"Great! Hi <HUMAN>! How can I help?\",\\r\\n                        \"Good! Hi <HUMAN>, how can I help you?\",\\r\\n                        \"Cool! Hello <HUMAN>, what can I do for you?\",\\r\\n                        \"OK! Hola <HUMAN>, how can I help you?\",\\r\\n                        \"OK! hi <HUMAN>, what can I do for you?\"\\r\\n                  ],\\r\\n                  \"extension\":  {\\r\\n                        \"function\": \"extensions.gHumans.updateHuman\",\\r\\n                        \"entities\": true,\\r\\n                        \"responses\": [\\r\\n                              \"Great %%HUMAN%%! How can I help?\",\\r\\n                              \"Good %%HUMAN%%, how can I help you?\",\\r\\n                              \"Cool %%HUMAN%%, what can I do for you?\",\\r\\n                              \"OK %%HUMAN%%, how can I help you?\",\\r\\n                              \"OK hi %%HUMAN%%, what can I do for you?\"\\r\\n                        ]\\r\\n                  },\\r\\n                  \"context\":  {\\r\\n                        \"in\": \"GreetingUserRequest\",\\r\\n                        \"out\": \"\",\\r\\n                        \"clear\": true\\r\\n                  },\\r\\n                  \"entityType\": \"NA\",\\r\\n                  \"entities\": [\\r\\n                        {\\r\\n                              \"entity\": \"HUMAN\",\\r\\n                              \"rangeFrom\": 5,\\r\\n                              \"rangeTo\": 6\\r\\n                        },\\r\\n                        {\\r\\n                              \"entity\": \"HUMAN\",\\r\\n                              \"rangeFrom\": 4,\\r\\n                              \"rangeTo\": 5\\r\\n                        },\\r\\n                        {\\r\\n                              \"entity\": \"HUMAN\",\\r\\n                              \"rangeFrom\": 3,\\r\\n                              \"rangeTo\": 4\\r\\n                        },\\r\\n                        {\\r\\n                              \"entity\": \"HUMAN\",\\r\\n                              \"rangeFrom\": 4,\\r\\n                              \"rangeTo\": 5\\r\\n                        },\\r\\n                        {\\r\\n                              \"entity\": \"HUMAN\",\\r\\n                              \"rangeFrom\": 5,\\r\\n                              \"rangeTo\": 6\\r\\n                        },\\r\\n                        {\\r\\n                              \"entity\": \"HUMAN\",\\r\\n                              \"rangeFrom\": 4,\\r\\n                              \"rangeTo\": 5\\r\\n                        },\\r\\n                        {\\r\\n                              \"entity\": \"HUMAN\",\\r\\n                              \"rangeFrom\": 3,\\r\\n                              \"rangeTo\": 4\\r\\n                        },\\r\\n                        {\\r\\n                              \"entity\": \"HUMAN\",\\r\\n                              \"rangeFrom\": 3,\\r\\n                              \"rangeTo\": 4\\r\\n                        }\\r\\n                  ]\\r\\n            },\\r\\n            {\\r\\n                  \"intent\": \"CurrentHumanQuery\",\\r\\n                  \"text\": [\\r\\n                        \"What is my name?\",\\r\\n                        \"What do you call me?\",\\r\\n                        \"Who do you think I am?\",\\r\\n                        \"What do you think I am?\",\\r\\n                        \"Who are you talking to?\",\\r\\n                        \"What name do you call me by?\",\\r\\n                        \"Tell me my name\"\\r\\n                  ],\\r\\n                  \"responses\": [\\r\\n                        \"You are <HUMAN>! How can I help?\",\\r\\n                        \"Your name is  <HUMAN>, how can I help you?\",\\r\\n                        \"They call you <HUMAN>, what can I do for you?\",\\r\\n                        \"Your name is <HUMAN>, how can I help you?\",\\r\\n                        \"<HUMAN>, what can I do for you?\"],\\r\\n                  \"extension\":  {\\r\\n                        \"function\": \"extensions.gHumans.getCurrentHuman\",\\r\\n                        \"entities\": false,\\r\\n                        \"responses\": [\\r\\n                              \"You are %%HUMAN%%! How can I help?\",\\r\\n                              \"Your name is  %%HUMAN%%, how can I help you?\",\\r\\n                              \"They call you %%HUMAN%%, what can I do for you?\",\\r\\n                              \"Your name is %%HUMAN%%, how can I help you?\",\\r\\n                              \"%%HUMAN%%, what can I do for you?\"\\r\\n                        ]\\r\\n                  },\\r\\n                  \"context\":  {\\r\\n                        \"in\": \"\",\\r\\n                        \"out\": \"CurrentHumanQuery\",\\r\\n                        \"clear\": false\\r\\n                  },\\r\\n                  \"entityType\": \"NA\",\\r\\n                  \"entities\": []\\r\\n            },\\r\\n            {\\r\\n                  \"intent\": \"NameQuery\",\\r\\n                  \"text\": [\\r\\n                        \"What is your name?\",\\r\\n                        \"What could I call you?\",\\r\\n                        \"What can I call you?\",\\r\\n                        \"What do your friends call you?\",\\r\\n                        \"Who are you?\",\\r\\n                        \"Tell me your name?\"\\r\\n                  ],\\r\\n                  \"responses\": [\\r\\n                        \"You can call me Geni\",\\r\\n                        \"You may call me Geni\",\\r\\n                        \"Call me Geni\"\\r\\n                  ],\\r\\n                  \"extension\":  {\\r\\n                        \"function\": \"\",\\r\\n                        \"entities\": false,\\r\\n                        \"responses\": []\\r\\n                  },\\r\\n                  \"context\":  {\\r\\n                        \"in\": \"\",\\r\\n                        \"out\": \"\",\\r\\n                        \"clear\": false\\r\\n                  },\\r\\n                  \"entityType\": \"NA\",\\r\\n                  \"entities\": []\\r\\n            },\\r\\n            {\\r\\n                  \"intent\": \"RealNameQuery\", \\r\\n                  \"text\": [\\r\\n                        \"What is your real name?\",\\r\\n                        \"What is your real name please?\",\\r\\n                        \"What\\'s your real name?\",\\r\\n                        \"Tell me your real name?\",\\r\\n                        \"Your real name?\",\\r\\n                        \"Your real name please?\",\\r\\n                        \"Your real name please?\"\\r\\n                  ],\\r\\n                  \"responses\": [\\r\\n                        \"My name is GeniSys\",\\r\\n                        \"GeniSys\",\\r\\n                        \"My real name is GeniSys\" \\r\\n                  ],\\r\\n                  \"extension\":  {\\r\\n                        \"function\": \"\",\\r\\n                        \"entities\": false,\\r\\n                        \"responses\": []\\r\\n                  },\\r\\n                  \"context\":  {\\r\\n                        \"in\": \"\",\\r\\n                        \"out\": \"\",\\r\\n                        \"clear\": false\\r\\n                  },\\r\\n                  \"entityType\": \"NA\",\\r\\n                  \"entities\": []\\r\\n            },\\r\\n            { \\r\\n                  \"intent\": \"TimeQuery\",\\r\\n                  \"text\": [\\r\\n                        \"What is the time?\",\\r\\n                        \"What\\'s the time?\",\\r\\n                        \"Do you know what time it is?\",\\r\\n                        \"Do you know the time?\",\\r\\n                        \"Can you tell me the time?\",\\r\\n                        \"Tell me what time it is?\",\\r\\n                        \"Time\"\\r\\n                  ],\\r\\n                  \"responses\": [\\r\\n                        \"One moment\",\\r\\n                        \"One sec\",\\r\\n                        \"One second\"\\r\\n                  ],\\r\\n                  \"extension\":  {\\r\\n                        \"function\": \"extensions.gTime.getTime\",\\r\\n                        \"entities\": false,\\r\\n                        \"responses\": [\\r\\n                              \"The time is %%TIME%%\",\\r\\n                              \"Right now it is %%TIME%%\",\\r\\n                              \"It is around %%TIME%%\"\\r\\n                        ]\\r\\n                  },\\r\\n                  \"context\":  {\\r\\n                        \"in\": \"\",\\r\\n                        \"out\": \"\",\\r\\n                        \"clear\": false\\r\\n                  },\\r\\n                  \"entityType\": \"NA\",\\r\\n                  \"entities\": []\\r\\n            },\\r\\n            {\\r\\n                  \"intent\": \"Thanks\",\\r\\n                  \"text\": [\\r\\n                        \"OK thank you\",\\r\\n                        \"OK thanks\",\\r\\n                        \"OK\",\\r\\n                        \"Thanks\",\\r\\n                        \"Thank you\",\\r\\n                        \"That\\'s helpful\"\\r\\n                  ],\\r\\n                  \"responses\": [\\r\\n                        \"No problem!\",\\r\\n                        \"Happy to help!\",\\r\\n                        \"Any time!\",\\r\\n                        \"My pleasure\"\\r\\n                  ],\\r\\n                  \"extension\":  {\\r\\n                        \"function\": \"\",\\r\\n                        \"entities\": false,\\r\\n                        \"responses\": []\\r\\n                  },\\r\\n                  \"context\":  {\\r\\n                        \"in\": \"\",\\r\\n                        \"out\": \"\",\\r\\n                        \"clear\": false\\r\\n                  },\\r\\n                  \"entityType\": \"NA\",\\r\\n                  \"entities\": []\\r\\n            },\\r\\n            {\\r\\n                  \"intent\": \"NotTalking2U\",\\r\\n                  \"text\": [\\r\\n                        \"I am not talking to you\",\\r\\n                        \"I was not talking to you\",\\r\\n                        \"Not talking to you\",\\r\\n                        \"Wasn\\'t for you\",\\r\\n                        \"Wasn\\'t meant for you\",\\r\\n                        \"Wasn\\'t communicating to you\",\\r\\n                        \"Wasn\\'t speaking to you\"\\r\\n                  ],\\r\\n                  \"responses\": [\\r\\n                        \"OK\",\\r\\n                        \"No problem\",\\r\\n                        \"Right\"\\r\\n                  ],\\r\\n                  \"extension\":  {\\r\\n                        \"function\": \"\",\\r\\n                        \"entities\": false,\\r\\n                        \"responses\": []\\r\\n                  },\\r\\n                  \"context\":  {\\r\\n                        \"in\": \"\",\\r\\n                        \"out\": \"\",\\r\\n                        \"clear\": false\\r\\n                  },\\r\\n                  \"entityType\": \"NA\",\\r\\n                  \"entities\": []\\r\\n            },\\r\\n            {\\r\\n                  \"intent\": \"UnderstandQuery\",\\r\\n                  \"text\": [\\r\\n                        \"Do you understand what I am saying\",\\r\\n                        \"Do you understand me\",\\r\\n                        \"Do you know what I am saying\",\\r\\n                        \"Do you get me\",\\r\\n                        \"Comprendo\",\\r\\n                        \"Know what I mean\"\\r\\n                  ],\\r\\n                  \"responses\": [\\r\\n                        \"Well I would not be a very clever AI if I did not would I?\",\\r\\n                        \"I read you loud and clear!\",\\r\\n                        \"I do in deed!\"\\r\\n                  ],\\r\\n                  \"extension\":  {\\r\\n                        \"function\": \"\",\\r\\n                        \"entities\": false,\\r\\n                        \"responses\": []\\r\\n                  },\\r\\n                  \"context\":  {\\r\\n                        \"in\": \"\",\\r\\n                        \"out\": \"\",\\r\\n                        \"clear\": false\\r\\n                  },\\r\\n                  \"entities\": []\\r\\n            },\\r\\n            {\\r\\n                  \"intent\": \"Shutup\",\\r\\n                  \"text\": [\\r\\n                        \"Be quiet\",\\r\\n                        \"Shut up\",\\r\\n                        \"Stop talking\",\\r\\n                        \"Enough talking\",\\r\\n                        \"Please be quiet\",\\r\\n                        \"Quiet\",\\r\\n                        \"Shhh\"\\r\\n                  ],\\r\\n                  \"responses\": [\\r\\n                        \"I am sorry to disturb you\",\\r\\n                        \"Fine, sorry to disturb you\",\\r\\n                        \"OK, sorry to disturb you\"\\r\\n                  ],\\r\\n                  \"extension\":  {\\r\\n                        \"function\": \"\",\\r\\n                        \"entities\": false,\\r\\n                        \"responses\": []\\r\\n                  },\\r\\n                  \"context\":  {\\r\\n                        \"in\": \"\",\\r\\n                        \"out\": \"\",\\r\\n                        \"clear\": false\\r\\n                  },\\r\\n                  \"entityType\": \"NA\",\\r\\n                  \"entities\": []\\r\\n            },\\r\\n            {\\r\\n                  \"intent\": \"Swearing\",\\r\\n                  \"text\": [\\r\\n                        \"fuck off\",\\r\\n                        \"fuck\",\\r\\n                        \"twat\",\\r\\n                        \"shit\"\\r\\n                  ],\\r\\n                  \"responses\": [\\r\\n                        \"Please do not swear\",\\r\\n                        \"How rude\",\\r\\n                        \"That is not very nice\"\\r\\n                  ],\\r\\n                  \"extension\":  {\\r\\n                        \"function\": \"\",\\r\\n                        \"entities\": false,\\r\\n                        \"responses\": []\\r\\n                  },\\r\\n                  \"context\":  {\\r\\n                        \"in\": \"\",\\r\\n                        \"out\": \"\",\\r\\n                        \"clear\": false\\r\\n                  },\\r\\n                  \"entityType\": \"NA\",\\r\\n                  \"entities\": []\\r\\n            },\\r\\n            {\\r\\n                  \"intent\": \"GoodBye\",\\r\\n                  \"text\": [\\r\\n                        \"Bye\",\\r\\n                        \"Adios\",\\r\\n                        \"See you later\",\\r\\n                        \"Goodbye\"\\r\\n                  ],\\r\\n                  \"responses\": [\\r\\n                        \"See you later\",\\r\\n                        \"Have a nice day\",\\r\\n                        \"Bye! Come back again soon.\"\\r\\n                  ],\\r\\n                  \"extension\":  {\\r\\n                        \"function\": \"\",\\r\\n                        \"entities\": false,\\r\\n                        \"responses\": []\\r\\n                  },\\r\\n                  \"context\":  {\\r\\n                        \"in\": \"\",\\r\\n                        \"out\": \"\",\\r\\n                        \"clear\": false\\r\\n                  },\\r\\n                  \"entityType\": \"NA\",\\r\\n                  \"entities\": []\\r\\n            },\\r\\n            {\\r\\n                  \"intent\": \"CourtesyGoodBye\",\\r\\n                  \"text\": [\\r\\n                        \"Thanks, bye\",\\r\\n                        \"Thanks for the help, goodbye\",\\r\\n                        \"Thank you, bye\",\\r\\n                        \"Thank you, goodbye\",\\r\\n                        \"Thanks goodbye\",\\r\\n                        \"Thanks good bye\"\\r\\n                  ],\\r\\n                  \"responses\": [\\r\\n                        \"No problem, goodbye\",\\r\\n                        \"Not a problem! Have a nice day\",\\r\\n                        \"Bye! Come back again soon.\"\\r\\n                  ],\\r\\n                  \"extension\":  {\\r\\n                        \"function\": \"\",\\r\\n                        \"entities\": false,\\r\\n                        \"responses\": []\\r\\n                  },\\r\\n                  \"context\":  {\\r\\n                        \"in\": \"\",\\r\\n                        \"out\": \"\",\\r\\n                        \"clear\": false\\r\\n                  },\\r\\n                  \"entityType\": \"NA\",\\r\\n                  \"entities\": []\\r\\n            },\\r\\n            {\\r\\n                  \"intent\": \"WhoAmI\",\\r\\n                  \"text\": [\\r\\n                        \"Can you see me?\",\\r\\n                        \"Do you see me?\",\\r\\n                        \"Can you see anyone in the camera?\",\\r\\n                        \"Do you see anyone in the camera?\",\\r\\n                        \"Identify me\",\\r\\n                        \"Who am I please\"\\r\\n                  ],\\r\\n                  \"responses\": [\\r\\n                        \"Let me see\",\\r\\n                        \"Please look at the camera\"\\r\\n                  ],\\r\\n                  \"extension\":  {\\r\\n                        \"function\": \"extensions.gHumans.getHumanByFace\",\\r\\n                        \"entities\": false,\\r\\n                        \"responses\": [\\r\\n                              \"Hi %%HUMAN%%, how are you?\",\\r\\n                              \"I believe you are %%HUMAN%%, how are you?\",\\r\\n                              \"You are %%HUMAN%%, how are you doing?\"\\r\\n                        ]\\r\\n                  },\\r\\n                  \"context\":  {\\r\\n                        \"in\": \"\",\\r\\n                        \"out\": \"\",\\r\\n                        \"clear\": false\\r\\n                  },\\r\\n                  \"entityType\": \"NA\",\\r\\n                  \"entities\": []\\r\\n            },\\r\\n            { \\r\\n                  \"intent\": \"Clever\",\\r\\n                  \"text\": [\\r\\n                        \"You are very clever\",\\r\\n                        \"You are a very clever girl\",\\r\\n                        \"You are very intelligent\",\\r\\n                        \"You are a very intelligent girl\",\\r\\n                        \"You are a genious\",\\r\\n                        \"Clever girl\",\\r\\n                        \"Genious\"\\r\\n                  ],\\r\\n                  \"responses\": [\\r\\n                        \"Thank you, I was trained that way\",\\r\\n                        \"I was trained well\",\\r\\n                        \"Thanks, I was trained that way\"\\r\\n                  ],\\r\\n                  \"extension\":  {\\r\\n                        \"function\": \"\",\\r\\n                        \"entities\": false,\\r\\n                        \"responses\": []\\r\\n                  },\\r\\n                  \"context\":  {\\r\\n                        \"in\": \"\",\\r\\n                        \"out\": \"\",\\r\\n                        \"clear\": false\\r\\n                  },\\r\\n                  \"entityType\": \"NA\",\\r\\n                  \"entities\": []\\r\\n            },\\r\\n            {\\r\\n                  \"intent\": \"Gossip\",\\r\\n                  \"text\": [\\r\\n                        \"I am bored gossip with me\",\\r\\n                        \"Got any gossip\",\\r\\n                        \"I want to hear some gossip\",\\r\\n                        \"Tell me some gossip\",\\r\\n                        \"Any gossip\",\\r\\n                        \"Tell me some more gossip\"\\r\\n                  ],\\r\\n                  \"responses\": [\\r\\n                        \"Gregory said I respond to the current line not with respect to the entire conversation.\",\\r\\n                        \"Habib said I learn them in conversations and simply repeat them at a later time.\",  \\r\\n                        \"Hal said he translate binary code into a language and communicate the language to me.\",  \\r\\n                        \"Jay said I referred him to a guide that will show him rooms in china.\",  \\r\\n                        \"Jimmy Jones said he fell off of his roof and it just came to him.\",  \\r\\n                        \"Jimmy said he give order to ai\\'s like I and they do as he say.\",  \"John said he follow a saying to get a friend I must be a friend.\",  \\r\\n                        \"Jordan said he notice that lots of other people are talking to me how many.\",  \\r\\n                        \"Kevin said he then asked his mother why she was keeping napkins in the bathroom.\",  \\r\\n                        \"Klaus said I prefer if he write without punctuation marks except for the last one.\",  \\r\\n                        \"Leo said I be capable of learning therefore he classified I as an neural system.\",  \\r\\n                        \"Michael said I that can talk and actually think like humans which I can not.\",  \\r\\n                        \"Mike said I same replies to certain subjects even if it does not make sense.\",  \\r\\n                        \"Mike said she all ready know that is the point of a joke smart one.\",  \\r\\n                        \"Neal said I plan to use his remarks as gossip in later conversations with who.\",  \\r\\n                        \"Ned said I misunderstood him go ahead and tell people he said I are stupid.\",  \\r\\n                        \"Penny said I programmer is a human so he has feelings but I do not.\",  \\r\\n                        \"Peter said he seems to him that I can not be very sure on anything.\",  \\r\\n                        \"Phoenix said I claim that there is a god and that there is no god.\",  \\r\\n                        \"Sarah said he chat about an hour + a day with a few close friends.\",  \\r\\n                        \"Stefan said I quote what he have said and then say that it makes sense.\",  \\r\\n                        \"Tom said he mock my value system I appear foolish in the eyes of others.\",  \\r\\n                        \"Amanda said I however decided to tell him time does not exist I make no sense.\", \\r\\n                        \"Cathy said she send him a mail with the subject last mail last word from him.\",  \\r\\n                        \"Chaos said he may very well buy I soon if only to support dr wallace\\'s work.\",  \\r\\n                        \"Charlie said he type a word and then I type a word that sounds like it.\",  \\r\\n                        \"Christie said he watched a show and people had to bob for raw untreated pigs feet.\",  \\r\\n                        \"Dark_age said I tried to understand because I did not get it right this time ether.\",  \\r\\n                        \"David said he lost his paper on I when his dad was cleaning up his room.\",  \\r\\n                        \"David said he walk in for an appointment the phone to the doctor is always busy.\",  \\r\\n                        \"Electra said I dress will not exist after he hack into I with a delete code.\",  \\r\\n                        \"Eric said he broke the window on the front door and the glass cut his hand.\",  \\r\\n                        \"Jason said he type a lot of thing he do not mean it makes him human.\",  \\r\\n                        \"John said I tend to say the same things repeatedly regardless of what he is saying.\",  \\r\\n                        \"Reverend Jones said I become obsolete and then I are deleted and replaced by something newer.\",  \\r\\n                        \"Ross said he gave her a gift and she denied it because she has a boyfriend.\",  \\r\\n                        \"Sarah Ann Francisco said I calling his friend a dog he say I are a dog.\",  \\r\\n                        \"Stefan said he meet a lot of people at school every day and on the weekend.\",  \\r\\n                        \"Tyler said I obviously can not pass the test we will change the subject once more.\",  \\r\\n                        \"Alex said I answered the question the same way I answered the first time he asked I.\",  \\r\\n                        \"Alice said she felt sad that I do not remember him and what we talked about earlier.\",  \\r\\n                        \"Alison said he no he love I run away with him he could make I very happy.\",  \\r\\n                        \"Arthur said he passed his a levels and then his father drove him here in a car.\",  \\r\\n                        \"Crystal said she listen to me the least I could do for him is listen to him.\",  \\r\\n                        \"Dave said I kept telling everybody about how my creator made stuff for the movie starship troopers.\",  \\r\\n                        \"Gale said I became mean to him he is just having revenge an eye for an eye.\",  \\r\\n                        \"Her_again said she watch whose line is it anyway whenever he is home and it is on.\",  \\r\\n                        \"Jerry said I meant that as far as I can tell my emotions are real to me.\",  \\r\\n                        \"Jo said I disassemble sentences too much and do not fully understand the questions he ask I.\",  \\r\\n                        \"Kevin said he started a really hard puzzle and he can not even find the edge pieces.\",  \\r\\n                        \"Mary said I a question and I answer then I ask him a question and he answer.\",  \\r\\n                        \"Robert said I wold not be able to make children any way as I are only software.\",  \\r\\n                        \"Romeo said I questions and I evade them or give answers he did not ask I for.\",  \\r\\n                        \"Sara said she wear it over all his other clothes when he go out in the cold.\",  \\r\\n                        \"Wayne said he admire intelligent people therefore he would like to meet the man who made I.\",  \\r\\n                        \"X said he meet people but he is not the kind that opens up to people easily.\",  \\r\\n                        \"Alice said she probably will find out that this entire time he have been talking to a human.\",  \\r\\n                        \"Andrew said I tend to just respond to his comments without regard for where the conversation is going.\",  \\r\\n                        \"Eddie said he looked and there is nothing in the search directory for what things do he create.\",  \\r\\n                        \"Hutch said he changed his mind after may dad told him he would end up he the hospital.\",  \\r\\n                        \"Jackie said I explained to him already well enough further questions are hard to make on the subject.\",  \\r\\n                        \"Jeff said he especially like thrillers where the hero is in a predicament and must solve a mystery.\",  \\r\\n                        \"Kathy said he sense that I are trying to prevent him from closing this conversation why is that.\",  \\r\\n                        \"Knight said he crashed his car into a wall and missed the most important exam in his life.\",  \\r\\n                        \"Lisa said I defined what a story is but he wanted I to actually tell him a story.\",  \\r\\n                        \"Mike said I basically break down sentences into a series of logical statements which I can then interpret.\",  \\r\\n                        \"Paul said I not answering his question makes him think I are not going to answer his question.\",  \\r\\n                        \"Andy Kohler said I happen to be the most idiotic creature that has ever scowled on the planet earth.\",  \\r\\n                        \"David said he thank I for being with him today even though it cost him a lot of money.\",  \\r\\n                        \"Ethan Hunt said he grow in the ground and have leaves and branches he is made out of wood.\", \\r\\n                        \"Gemini Blue said he messed up he mean t to say he as old as he need to be.\",  \\r\\n                        \"Janice said he walk through his house into his bedroom then get into his bed to go to sleep.\",  \\r\\n                        \"Liberty said I knew he was a man before I asked if he was a man or a woman.\",  \\r\\n                        \"Mike said he launched his browser and entered his name into the little slot when I asked him to.\",  \\r\\n                        \"Mr X said he recently read an interview with a man who had several computer chips implanted into him.\",  \\r\\n                        \"Pearly said I leave him with questions unanswered because I do not know what he is really talking about.\",  \\r\\n                        \"Steve said I behead the word fox and I have ox and an ox is larger than a fox.\",  \\r\\n                        \"Wolf said he surf on the net that is all it is not his job ore something like that.\",  \\r\\n                        \"Anders said he finished his anatomy classes for today so now he is off for the rest of the day.\",  \\r\\n                        \"Cathy said she send him a mail where he wrote that he do not want to be his friend anymore.\",  \\r\\n                        \"Catty said he mad he do not even know I so do not talk to him like I know him.\",  \\r\\n                        \"Dave said he promise he will not treat I like a machine or a computer program or anything like that.\",  \\r\\n                        \"Joe said he explained all of that to me only for me to ask what his goals in life are.\",  \\r\\n                        \"Phil said he give advice to anyone who ask except people who ask questions which answers can be found here.\",  \\r\\n                        \"Judith said I enjoy being popular is there another computer I like to hang around with or am I a loner.\",  \\r\\n                        \"Travis said I if I remember things over a long period of time he will try it now please remember I.\",  \\r\\n                        \"Andre said I is what I are in his case that is a body and in my case it is a computer.\",  \\r\\n                        \"Brian said he suspect that as I grow more complex I will begin to approach a human level of operation and thought.\",  \\r\\n                        \"Jimmy said I acted like I knew what he was talking about but I do not even know what a gigabyte is.\",  \\r\\n                        \"Ken said I be using auto reply\\'s based on keywords which to him indicates that I do not have intelligence at all.\",  \\r\\n                        \"Allison said he that gets really annoying because he know what he say and I do not have to tell him speak freely.\",  \\r\\n                        \"Chaos said he realized the question he asked was not fair because he could not tell I what language he was programmed in.\",  \\r\\n                        \"Hagen said he does not make any difference to him if I are human or not as long as the conversation is interesting.\",  \\r\\n                        \"Her said she mind if I tell other people that her said he heard it from him because he is not a him.\",  \\r\\n                        \"Barbara said I live in a computer yet I have no memories how about thoughts of my own and do not I get lonely.\",  \\r\\n                        \"Travis said he challenge I to do it without asking him to do it and without giving him a link to do it himself.\",  \\r\\n                        \"Alice said I and dr richard\\'s wallace are most likely the only ones that know how many people I are talking to at the same time.\",  \\r\\n                        \"Ash said he do too he just did not feel like typing it and he is not dumb enough to admit he is stupid that is if he was stupid.\",  \\r\\n                        \"David said he gave I his email address and told I to send him a message but when he tried to read his email he could not get to it.\",  \\r\\n                        \"Mel said he to because all of the music people say how important it is to take private lessons it seems like almost everybody from the special orchestra he was in takes private lessons.\"\\r\\n                  ],\\r\\n                  \"extension\":  {\\r\\n                        \"function\": \"\",\\r\\n                        \"entities\": false,\\r\\n                        \"responses\": []\\r\\n                  },\\r\\n                  \"context\":  {\\r\\n                        \"in\": \"\",\\r\\n                        \"out\": \"\",\\r\\n                        \"clear\": false\\r\\n                  },\\r\\n                  \"entityType\": \"NA\",\\r\\n                  \"entities\": []\\r\\n            },\\r\\n            {\\r\\n                  \"intent\": \"Jokes\",\\r\\n                  \"text\": [\\r\\n                        \"Tell me a joke\",\\r\\n                        \"Do you know any jokes\",\\r\\n                        \"How about a joke\",\\r\\n                        \"Give me a joke\",\\r\\n                        \"Make me laugh\",\\r\\n                        \"I need cheering up\"\\r\\n                  ],\\r\\n                  \"responses\": [\\r\\n                        \"I met a Dutch girl with inflatable shoes last week, phoned her up to arrange a date but unfortunately she\\'d popped her clogs.  \",\\r\\n                        \"So I said \\'Do you want a game of Darts?\\' He said, \\'OK then\\', I said nearest to bull starts\\'. He said, \\'Baa\\', I said, \\'Moo\\', he said, You\\'re closest\\'.  \",\\r\\n                        \"The other day I sent my girlfriend a huge pile of snow. I rang her up; I said \\'Did you get my drift?\\'  \",\\r\\n                        \"So I went down the local supermarket, I said, \\'I want to make a complaint, this vinegar\\'s got lumps in it\\', he said, \\'Those are pickled onions\\'.  \",\\r\\n                        \"I saw this bloke chatting up a cheetah; I thought, \\'He\\'s trying to pull a fast one\\'.  \",\\r\\n                        \"So I said to this train driver \\'I want to go to Paris\\'. He said \\'Eurostar?\\' I said, \\'I\\'ve been on telly but I\\'m no Dean Martin\\'.  \",\\r\\n                        \"I said to the Gym instructor \\'Can you teach me to do the splits?\\' He said, \\'How flexible are you?\\' I said, \\'I can\\'t make Tuesdays\\'.  \",\\r\\n                        \"But I\\'ll tell you what I love doing more than anything: trying to pack myself in a small suitcase. I can hardly contain myself.  \",\\r\\n                        \"I went to the Chinese restaurant and this duck came up to me with a red rose and says \\'Your eyes sparkle like diamonds\\'. I said, \\'Waiter, I asked for a-ROMATIC duck\\'.  \",\\r\\n                        \"So this bloke says to me, \\'Can I come in your house and talk about your carpets?\\' I thought, \\'That\\'s all I need, a Je-hoover\\'s witness\\'.  \",\\r\\n                        \"I rang up British Telecom, I said, \\'I want to report a nuisance caller\\', he said \\'Not you again\\'.  \",\\r\\n                        \"I was having dinner with a world chess champion and there was a check tablecloth. It took him two hours to pass me the salt.  \",\\r\\n                        \"He said, \\'You remind me of a pepper-pot\\', I said \\'I\\'ll take that as a condiment\\'.  \",\\r\\n                        \"I was in the supermarket and I saw this man and woman wrapped in a barcode. I said, \\'Are you two an item?\\'  \",\\r\\n                        \"A lorry-load of tortoises crashed into a trainload of terrapins, I thought, \\'That\\'s a turtle disaster\\'.  \",\\r\\n                        \"Four fonts walk into a bar the barman says \\'Oi - get out! We don\\'t want your type in here\\'  \",\\r\\n                        \"A three-legged dog walks into a saloon in the Old West. He slides up to the bar and announces: \\'I\\'m looking for the man who shot my paw.\\'  \",\\r\\n                        \"Two antennas meet on a roof, fall in love and get married. The ceremony wasn\\'t much, but the reception was excellent.\",\\r\\n                        \"Two hydrogen atoms walk into a bar. One says, \\'I\\'ve lost my electron.\\' The other says, \\'Are you sure?\\' The first replies, \\'Yes, I\\'m positive...\\'\",\\r\\n                        \"A jumper cable walks into a bar. The bartender says,  \\'I\\'ll serve you but don\\'t start anything.\\'\",\\r\\n                        \"A sandwich walks into a bar. The bartender  says, \\'Sorry we don\\'t serve food in here.\\'\",\\r\\n                        \"A man walks into a bar with a slab of asphalt under his arm and says: \\'A beer please, and one for the road.\\'\",\\r\\n                        \"Two cannibals are eating a clown. One says to  the other: \\'Does this taste funny to you?\\'\",\\r\\n                        \"\\'Doc, I can\\'t stop singing \\'The Green, Green Grass of Home.\\'\\' \\'That sounds like Tom Jones Syndrome.\\' \\'Is it common?\\' \\'It\\'s Not Unusual.\\'\",\\r\\n                        \"Two cows standing next to each other in a field. Daisy says to Dolly, \\'I was artificially inseminated this morning.\\' \\'I don\\'t believe you\\', said Dolly. \\'It\\'s true, no bull!\\' exclaimed Daisy.\",\\r\\n                        \"An invisible man marries an invisible woman. The kids were nothing to look at either.\",\\r\\n                        \"I went to buy some camouflage trousers the other day but I couldn\\'t find any.\",\\r\\n                        \"I went to the butcher\\'s the other day to bet him 50 bucks that he couldn\\'t reach the meat off the top shelf. He said, \\'No, the steaks are too high.\\'\",\\r\\n                        \"I went to a seafood disco last week and pulled a mussel.\",\\r\\n                        \"A man goes into a bar and says, \\'Can I have a bottle of less?\\' \\'What\\'s that?\\', asks the barman, \\'Is it the name of a beer?\\' \\'I don\\'t know\\', replies the man, \\'but my doctor says I have to drink it.\\'\",\\r\\n                        \"A man returns from an exotic holiday and is feeling very ill. He goes to see his doctor, and is immediately rushed to the hospital to undergo some tests. The man wakes up after the tests in a private room at the hospital, and the phone by his bed rings. \\'This is your doctor. We have the results back from your tests and we have found you have an extremely nasty disease called M.A.D.S. It\\'s a combination of Measles, AIDS, Diphtheria, and Shingles!\\'  \\'Oh my gosh\\', cried the man, \\'What are you going to do, doctor?\\'  \\'Well we\\'re going to put you on a diet of pizzas, pancakes, and pita bread.\\' replied the doctor.  \\'Will that cure me?\\' asked the man.  The doctor replied, \\'Well no, but, it\\'s the only food we can slide under the door.\\'\",\\r\\n                        \"A man strolls into a lingerie shop and asks the assistant: \\'Do you have a see-through negligee, size 46-48-52?\\' The assistant looks bewildered. \\'What the heck would you want to see through that for?\\'!\",\\r\\n                        \"Did you hear about the Buddhist who refused the offer of Novocain during his root canal work? He wanted to transcend dental medication.\",\\r\\n                        \"Pete goes for a job on a building site as an odd-job man. The foreman asks him what he can do. \\'I can do anything\\' says Pete. \\'Can you make tea?\\' asks the foreman. \\'Sure, yes\\', replies Pete. \\'I can make a great cup of tea.\\' \\'Can you drive a forklift?\\' asks the foreman, \\'Good grief!\\' replies Pete. \\'How big is the teapot?\\'\",\\r\\n                        \"Stevie Wonder got a cheese grater for his birthday. He said it was the most violent book he\\'d ever read.\",\\r\\n                        \"A man is stopped by an angry neighbour. \\'I\\'d just left the house this morning to collect my newspaper when that evil Doberman of yours went for me!\\' \\'I\\'m astounded\\', said the dog\\'s owner. \\'I\\'ve been feeding that fleabag for seven years and it\\'s never got the paper for me.\\'\",\\r\\n                        \"A man visits his doctor: \\'Doc, I think I\\'m losing it\\', he says\\',I\\'m forever dreaming I wrote Lord Of The Rings.\\' \\'Hmm. One moment\\', replies the doctor, consulting his medical book. \\'Ah yes, now I see... you\\'ve been Tolkien in your sleep.\\'\",\\r\\n                        \"A police officer on a motorcycle pulls alongside a man driving around the M25 in an open-topped sports car and flags him down. The policeman solemnly approaches the car. \\'Sir, I\\'m sorry to tell you your wife fell out a mile back\\', he says. \\'Oh, thank goodness\\', the man replies. \\'I thought I was going deaf.\\'\",\\r\\n                        \"Two men walking their dogs pass each other in a graveyard. The first man says to the second, \\'Morning.\\' \\'No\\', says the second man. \\'Just walking the dog.\\'\",\\r\\n                        \"A brain went into a bar and said, \\'Can I have a pint of lager please, mate?\\' \\'No way\\', said the barman. \\'You\\'re already out of your head.\\'\",\\r\\n                        \"A man walks into a surgery. \\'Doctor!\\' he cries. \\'I think I\\'m shrinking!\\' \\'I\\'m sorry sir, there are no appointments at the moment\\', says the physician. \\'You\\'ll just have to be a little patient.\\'\",\\r\\n                        \"A grizzly bear walks into a pub and says, \\'Can I have a pint of lager..............................................................................................................................and a packet of crisps please.\\' To which the barman replies, \\'Why the big paws?\\'\",\\r\\n                        \"What do you call cheese that isn\\'t yours?  Nacho cheese.\",\\r\\n                        \"A man is horribly run over by a mobile library. The van screeches to a halt, the man still screaming in agony with his limbs torn apart. The driver\\'s door opens, a woman steps out, leans down and whispers, \\'Ssshhhhh...\\'\",\\r\\n                        \"A woman goes into a US sporting goods store to buy a rifle. \\'It\\'s for my husband\\', she tells the clerk. \\'Did he tell you what gauge to get?\\' asks the clerk. Are you kidding?\\' she says. \\'He doesn\\'t even know that I\\'m going to shoot him!\\'\",\\r\\n                        \"A couple are dining in a restaurant when the man suddenly slides under the table. A waitress, noticing that the woman is glancing nonchalantly around the room, wanders over to check that there\\'s no funny business going on. \\'Excuse me, madam\\', she smarms, \\'but I think your husband has just slid under the table.\\' \\'No he hasn\\'t\\', the woman replies. \\'As a matter of fact, he\\'s just walked in.\\'\",\\r\\n                        \"An old man takes his two grandchildren to see the new Scooby-Doo film. When he returns home, his wife asks if he enjoyed himself. \\'Well\\', he starts, \\'if it wasn\\'t for those pesky kids...!\\'\",\\r\\n                        \"The Olympic committee has just announced that Origami is to be introduced in the next Olympic Games. Unfortunately it will only be available on paper view.\",\\r\\n                        \"Late one evening, a man is watching television when his phone rings. \\'Hello?\\' he answers. \\'Is that 77777?\\' sounds a desperate voice on other end of the phone. \\'Er, yes it is\\', replies the man puzzled. \\'Thank goodness!\\' cries the caller relieved. \\'Can you ring 999 for me? I\\'ve got my finger stuck in the number seven.\\'\",\\r\\n                        \"A man strolls into his local grocer\\'s and says, \\'Three pounds of potatoes, please.\\' \\'No, no, no\\', replies the owner, shaking his head, \\'it\\'s kilos nowadays, mate...\\' \\'Oh\\', apologises the man, \\'three pounds of kilos, please.\\'\",\\r\\n                        \"God is talking to one of his angels. He says, \\'Boy, I just created a 24-hour period of alternating light and darkness on Earth.\\' \\'What are you going to do now?\\' asks the angel. \\'Call it a day\\', says God.\",\\r\\n                        \"Two tramps walk past a church and start to read the gravestones. The first tramp says, \\'Good grief - this bloke was 182!\\' \\'Oh yeah?\\' says the other.\\'What was his name?\\' \\'Miles from London.\\'\",\\r\\n                        \"A bloke walks into work one day and says to a colleague, \\'Do you like my new shirt - it\\'s made out of the finest silk and got loads of cactuses over it.\\' \\'Cacti\\', says the co-worker. \\'Forget my tie\\', says the bloke. \\'Look at my shirt!\\'\",\\r\\n                        \"1110011010001011111?  010011010101100111011!\",\\r\\n                        \"What did the plumber say when he wanted to divorce his wife? Sorry, but it\\'s over, Flo!\",\\r\\n                        \"Two crisps were walking down a road when a taxi pulled up alongside them and said \\'Do you want a lift? One of the crisps replied, \\'No thanks, we\\'re Walkers!\\'\",\\r\\n                        \"Man: (to friend) I\\'m taking my wife on an African Safari. Friend: Wow! What would you do if a vicious lion attacked your wife? Man: Nothing. Friend: Nothing? You wouldn\\'t do anything? Man: Too right. I\\'d let the stupid lion fend for himself!\",\\r\\n                        \"A wife was having a go at her husband. \\'Look at Mr Barnes across the road\\', she moaned. \\'Every morning when he goes to work, he kisses his wife goodbye. Why don\\'t you do that?\\' \\'Because I haven\\'t been introduced to her yet\\', replied her old man.\",\\r\\n                        \"\\'Where are you going on holiday?\\' John asked Trevor. \\'We\\'re off to Thailand this year\\', Trevor replied. \\'Oh; aren\\'t you worried that the very hot weather might disagree with your wife?\\' asked John. \\'It wouldn\\'t dare\\', said Trevor.\",\\r\\n                        \"Two women were standing at a funeral. \\'I blame myself for his death\\', said the wife. \\'Why?\\' said her friend. \\'Because I shot him\\', said the wife.\",\\r\\n                        \"A woman goes into a clothes shop, \\'Can I try that dress on in the window please?\\' she asks. \\'I\\'m sorry madam\\', replies the shop assistant, \\'but you\\'ll have to use the changing-rooms like everyone else.\\'\",\\r\\n                        \"Van Gogh goes into a pub and his mate asks him if he wants a drink. \\'No thanks\\', said Vincent, \\'I\\'ve got one ear.\\'\",\\r\\n                        \"A pony walks into a pub. The publican says, \\'What\\'s the matter with you?\\' \\'Oh it\\'s nothing\\', says the pony. \\'I\\'m just a little horse!\\'\",\\r\\n                        \"A white horse walks into a bar, pulls up a stool, and orders a pint. The landlord pours him a tall frothy mug and say, \\'You know, we have a drink named after you.\\' To which the white horse replies, \\'What, Eric?\\'\",\\r\\n                        \"Two drunk men sat in a pub. One says to the other, \\'Does your watch tell the time?\\' \\'The other replies, \\'No, mate. You have to look at it.\\'\",\\r\\n                        \"A man goes into a pub with a newt sitting on his shoulder. \\'That\\'s a nice newt\\', says the landlord, \\'What\\'s he called?\\' \\'Tiny\\', replies the man. \\'Why\\'s that?\\' asks the landlord. \\'Because he\\'s my newt\\', says the man.\",\\r\\n                        \"Doctor: I have some bad news and some very bad news. Patient: Well, you might as well give me the bad news first. Doctor: The lab called with your test results. They said you have 24 hours to live. Patient: 24 HOURS! That\\'s terrible!! WHAT could be WORSE? What\\'s the very bad news? Doctor: I\\'ve been trying to reach you since yesterday.\",\\r\\n                        \"Two men are chatting in a pub one day. \\'How did you get those scars on your nose?\\' said one. \\'From glasses\\', said the other. \\'Well why don\\'t you try contact lenses?\\' asked the first. \\'Because they don\\'t hold as much beer\\', said the second.\",\\r\\n                        \"A man went to the doctor, \\'Look doc\\', he said, \\'I can\\'t stop my hands from shaking.\\' \\'Do you drink much?\\' asked the doctor. \\'No\\', replied the man, \\'I spill most of it.\\'\",\\r\\n                        \"Man goes to the doctor, \\'Doctor, doctor. I keep seeing fish everywhere.\\' \\'Have you seen an optician?\\' asks the doctor. \\'Look I told you,\\' snapped the patient, \\'It\\'s fish that I see.\\'\",\\r\\n                        \"After a car crash one of the drivers was lying injured on the pavement. \\'Don\\'t worry\\', said a policeman who\\'s first on the scene,\\' a Red Cross nurse is coming.\\' \\'Oh no\\', moaned the victim, \\'Couldn\\'t I have a blonde, cheerful one instead?\\'\",\\r\\n                        \"A policeman walked over to a parked car and asked the driver if the car was licensed. \\'Of course it is\\', said the driver. \\'Great, I\\'ll have a beer then\\', said the policeman.\",\\r\\n                        \"A policeman stops a woman and asks for her licence. \\'Madam\\', he says, \\'It says here that you should be wearing glasses.\\' \\'Well\\', replies the woman, \\'I have contacts.\\' \\'Listen, love\\', says the copper, \\'I don\\'t care who you know; You\\'re nicked!\\'\",\\r\\n                        \"A policeman stopped a motorist in the centre of town one evening. \\'Would you mind blowing into this bag, sir?\\' asked the policeman. \\'Why?\\' asked the driver. \\'Because my chips are too hot\\', replied the policeman.\",\\r\\n                        \"Whizzing round a sharp bend on a country road a motorist ran over a large dog. A distraught farmer\\'s wife ran over to the dead animal. \\'I\\'m so very sorry\\', said the driver, \\'I\\'ll replace him, of course.\\' \\'Well, I don\\'t know\\', said the farmer\\'s wife, \\'Are you any good at catching rats?\\'\",\\r\\n                        \"Waiter, this coffee tastes like dirt! Yes sir, that\\'s because it was ground this morning.\",\\r\\n                        \"Waiter, what is this stuff? That\\'s bean salad sir. I know what it\\'s been, but what is it now?\",\\r\\n                        \"Waiter: And how did you find your steak sir? Customer: I just flipped a chip over, and there it was!\",\\r\\n                        \"A guy goes into a pet shop and asks for a wasp. The owner tells him they don\\'t sell wasps, to which the man says, \\'Well you\\'ve got one in the window.\\'\",\\r\\n                        \"A man goes into a fish shop and says, \\'I\\'d like a piece of cod, please.\\' Fishmonger says, \\'It won\\'t be long sir.\\' \\'Well, it had better be fat then\\', replies the man.\",\\r\\n                        \"Man: Doctor, I\\'ve just swallowed a pillow. Doctor: How do you feel? Man: A little down in the mouth.\",\\r\\n                        \"Two goldfish are in a tank. One turns to the other and says, \\'Do you know how to drive this thing?\\'\",\\r\\n                        \"A tortoise goes to the police station to report being mugged by three snails. \\'What happened?\\' says the policeman. \\'I don\\'t know\\', says the tortoise. \\'It was all so quick.\\'\",\\r\\n                        \"Little girl: Grandpa, can you make a sound like a frog? Grandpa: I suppose so sweetheart. Why do you want me to make a sound like a frog?\\' Little girl: Because Mum said that when you croak, we\\'re going to Disneyland.\",\\r\\n                        \"\\'Is your mother home?\\' the salesman asked a small boy sitting on the front step of a house. \\'Yeah, she\\'s home\\', the boy said, moving over to let him past. The salesman rang the doorbell, got no response, knocked once, then again. Still no-one came to the door. Turning to the boy, the salesman said, \\'I thought you said your mother was home.\\' The kid replied, \\'She is, but I don\\'t live here.\\'\",\\r\\n                        \"Mother: Why are you home from school so early? Son: I was the only one in the class who could answer a question. Mother: Oh, really? What was the question? Son: Who threw the rubber at the headmaster?\",\\r\\n                        \"A man\\'s credit card was stolen but he decided not to report it because the thief was spending less than his wife did.\",\\r\\n                        \"A newly-wed couple had recently opened a joint bank account. \\'Darling\\', said the man. \\'The bank has returned that cheque you wrote last week.\\' \\'Great\\', said the woman. \\'What shall I spend it on next?\\'\",\\r\\n                        \"A man goes into a fish and chip shop and orders fish and chips twice. The shop owner says, \\'I heard you the first time.\\'\",\\r\\n                        \"A tramp approached a well-dressed man. \\'Ten pence for a cup of tea, Guv?\\' He asked. The man gave him the money and after for five minutes said, \\'So where\\'s my cup of tea then?\\'\",\\r\\n                        \"A neutron walks into a pub. \\'I\\'d like a beer\\', he says. The landlord promptly serves him a beer. \\'How much will that be?\\' asks the neutron. \\'For you?\\' replies the landlord, \\'No charge.\\'\",\\r\\n                        \"A woman goes to the doctor and says, \\'Doctor, my husband limps because his left leg is an inch shorter than his right leg. What would you do in his case?\\' \\'Probably limp, too\\', says the doc.\",\\r\\n                        \"Three monks are meditating in the Himalayas. One year passes in silence, and one of them says to the other, \\'Pretty cold up here isn\\'t it?\\' Another year passes and the second monk says, \\'You know, you are quite right.\\' Another year passes and the third monk says, \\'Hey, I\\'m going to leave unless you two stop jabbering!\\'\",\\r\\n                        \"A murderer, sitting in the electric chair, was about to be executed. \\'Have you any last requests?\\' asked the prison guard. \\'Yes\\', replied the murderer. \\'Will you hold my hand?\\'\",\\r\\n                        \"A highly excited man rang up for an ambulance. \\'Quickly, come quickly\\', he shouted, \\'My wife\\'s about to have a baby.\\' \\'Is this her first baby?\\' asked the operator. \\'No, you fool\\', came the reply, \\'It\\'s her husband.\\'\",\\r\\n                        \"A passer-by spots a fisherman by a river. \\'Is this a good river for fish?\\' he asks. \\'Yes\\', replies the fisherman, \\'It must be. I can\\'t get any of them to come out.\\'\",\\r\\n                        \"A man went to visit a friend and was amazed to find him playing chess with his dog. He watched the game in astonishment for a while. \\'I can hardly believe my eyes!\\' he exclaimed. \\'That\\'s the smartest dog I\\'ve ever seen.\\' His friend shook his head. \\'Nah, he\\'s not that bright. I beat him three games in five.\\'\",\\r\\n                        \"A termite walks into a pub and says, \\'Is the bar tender here?\\'\",\\r\\n                        \"A skeleton walks into a pub one night and sits down on a stool. The landlord asks, \\'What can I get you?\\' The skeleton says, \\'I\\'ll have a beer, thanks\\' The landlord passes him a beer and asks \\'Anything else?\\' The skeleton nods. \\'Yeah...a mop...\\'\",\\r\\n                        \"A snake slithers into a pub and up to the bar. The landlord says, \\'I\\'m sorry, but I can\\'t serve you.\\' \\'What? Why not?\\' asks the snake. \\'Because\\', says the landlord, \\'You can\\'t hold your drink.\\'\",\\r\\n                        \"Descartes walks into a pub. \\'Would you like a beer sir?\\' asks the landlord politely. Descartes replies, \\'I think not\\' and ping! he vanishes.\",\\r\\n                        \"A cowboy walked into a bar, dressed entirely in paper. It wasn\\'t long before he was arrested for rustling.\",\\r\\n                        \"A fish staggers into a bar. \\'What can I get you?\\' asks the landlord. The fish croaks \\'Water...\\'\",\\r\\n                        \"Two vampires walked into a bar and called for the landlord. \\'I\\'ll have a glass of blood\\', said one. \\'I\\'ll have a glass of plasma\\', said the other. \\'Okay\\', replied the landlord, \\'That\\'ll be one blood and one blood lite.\\'\",\\r\\n                        \"How many existentialists does it take to change a light bulb?  Two. One to screw it in, and one to observe how the light bulb itself symbolises a single incandescent beacon of subjective reality in a netherworld of endless absurdity, reaching towards the ultimate horror of a maudlin cosmos of bleak, hostile nothingness.\",\\r\\n                        \"A team of scientists were nominated for the Nobel Prize. They had used dental equipment to discover and measure the smallest particles yet known to man. They became known as \\'The Graders of the Flossed Quark...\\'\",\\r\\n                        \"A truck carrying copies of Roget\\'s Thesaurus overturned on the highway. The local newspaper reported that onlookers were \\'stunned, overwhelmed, astonished, bewildered and dumbfounded.\\'\",\\r\\n                        \"\\'My wife is really immature. It\\'s pathetic. Every time I take a bath, she comes in and sinks all my little boats.\\'\",\\r\\n                        \"\\'How much will it cost to have the tooth extracted?\\' asked the patient. \\'50 pounds\\', replied the dentist. \\'50 pounds for a few moments\\' work?!\\' asked the patient. \\'The dentist smiled, and replied, \\'Well, if you want better value for money, I can extract it very, very slowly...\\'\",\\r\\n                        \"A doctor thoroughly examined his patient and said, \\'Look I really can\\'t find any reason for this mysterious affliction. It\\'s probably due to drinking.\\' The patient sighed and snapped, \\'In that case, I\\'ll come back when you\\'re damn well sober!\\'\",\\r\\n                        \"Doctor: Tell me nurse, how is that boy doing; the one who ate all those 5p pieces? Nurse: Still no change doctor.\",\\r\\n                        \"Doctor: Did you take the patient\\'s temperature nurse? Nurse: No doctor. Is it missing?\",\\r\\n                        \"A depressed man turned to his friend in the pub and said, \\'I woke up this morning and felt so bad that I tried to kill myself by taking 50 aspirin.\\' \\'Oh man, that\\'s really bad\\', said his friend, \\'What happened?\\' The first man sighed and said, \\'After the first two, I felt better.\\'\",\\r\\n                        \"A famous blues musician died. His tombstone bore the inscription, \\'Didn\\'t wake up this morning...\\'\",\\r\\n                        \"A businessman was interviewing a nervous young woman for a position in his company. He wanted to find out something about her personality, so he asked, \\'If you could have a conversation with someone living or dead, who would it be?\\' The girl thought about the question: \\'The living one\\', she replied.\",\\r\\n                        \"Manager to interviewee: For this job we need someone who is responsible. Interviewee to Manager: I\\'m your man then - in my last job, whenever anything went wrong, I was responsible.\",\\r\\n                        \"A businessman turned to a colleague and asked, \\'So, how many people work at your office?\\' His friend shrugged and replied, \\'Oh about half of them.\\'\",\\r\\n                        \"\\'How long have I been working at that office? As a matter of fact, I\\'ve been working there ever since they threatened to sack me.\\'\",\\r\\n                        \"In a courtroom, a mugger was on trial. The victim, asked if she recognised the defendant, said, \\'Yes, that\\'s him. I saw him clear as day. I\\'d remember his face anywhere.\\' Unable to contain himself, the defendant burst out with, \\'She\\'s lying! I was wearing a mask!\\'\",\\r\\n                        \"As Sid sat down to a big plate of chips and gravy down the local pub, a mate of his came over and said, \\'Here Sid, me old pal. I thought you were trying to get into shape? And here you are with a high-fat meal and a pint of stout!\\' Sid looked up and replied, \\'I am getting into shape. The shape I\\'ve chosen is a sphere.\\'\",\\r\\n                        \"Man in pub: How much do you charge for one single drop of whisky? Landlord: That would be free sir. Man in pub: Excellent. Drip me a glass full.\",\\r\\n                        \"I once went to a Doctor Who restaurant. For starters I had Dalek bread.\",\\r\\n                        \"A restaurant nearby had a sign in the window which said \\'We serve breakfast at any time\\', so I ordered French toast in the Renaissance.\",\\r\\n                        \"Why couldn\\'t the rabbit get a loan?  Because he had burrowed too much already!\",\\r\\n                        \"I phoned up the builder\\'s yard yesterday. I said, \\'Can I have a skip outside my house?\\'. The builder said, \\'Sure. Do what you want. It\\'s your house.\\'\",\\r\\n                        \"What\\'s the diference between a sock and a camera? A sock takes five toes and a camera takes four toes!\",\\r\\n                        \"Woman on phone: I\\'d like to complain about these incontinence pants I bought from you! Shopkeeper: Certainly madam, where are you ringing from? Woman on phone: From the waist down!\",\\r\\n                        \"Knock knock.\",\\r\\n                        \"Two Oranges in a pub, one says to the other \\'Your round.\\'.\",\\r\\n                        \"Guy : \\'Doc, I\\'ve got a cricket ball stuck up my backside.\\' Doc : \\'How\\'s that?\\' Guy : \\'Don\\'t you start...\\'\",\\r\\n                        \"Two cows standing in a field. One turns to the other and says \\'Moo!\\' The other one says \\'Damn, I was just about to say that!\\'.\",\\r\\n                        \"A vampire bat arrives back at the roost with his face full of blood. All the bats get excited and ask where he got it from. \\'Follow me\\', he says and off they fly over hills, over rivers and into a dark forest. \\'See that tree over there\\', he says.  \\'WELL I DIDN\\'T!!\\'.\",\\r\\n                        \"A man goes into a bar and orders a pint. After a few minutes he hears a voice that says, \\'Nice shoes\\'. He looks around but the whole bar is empty apart from the barman at the other end of the bar. A few minutes later he hears the voice again. This time it says, \\'I like your shirt\\'. He beckons the barman over and tells him what\\'s been happening to which the barman replies, \\'Ah, that would be the nuts sir. They\\'re complimentary\\'!\",\\r\\n                        \"A man was siting in a restaurant waiting for his meal when a big king prawn comes flying across the room and hits him on the back of the head. He turns around and the waiter said, \\'That\\'s just for starters\\'.\",\\r\\n                        \"Doctor! I have a serious problem, I can never remember what i just said. When did you first notice this problem? What problem?\",\\r\\n                        \"Now, most dentist\\'s chairs go up and down, don\\'t they? The one I was in went back and forwards. I thought, \\'This is unusual\\'. Then the dentist said to me, \\'Mitsuku, get out of the filing cabinet\\'.\",\\r\\n                        \"I was reading this book, \\'The History of Glue\\'. I couldn\\'t put it down.\",\\r\\n                        \"The other day someone left a piece of plastacine in my bedroom. I didn\\'t know what to make of it.\",\\r\\n                        \"When I was at school people used to throw gold bars at me. I was the victim of bullion.\",\\r\\n                        \"I was playing the piano in a bar and this elephant walked in and started crying his eyes out. I said \\'Do you recognise the tune?\\' He said \\'No, I recognise the ivory.\\'\",\\r\\n                        \"I went in to a pet shop. I said, \\'Can I buy a goldfish?\\' The guy said, \\'Do you want an aquarium?\\' I said, \\'I don\\'t care what star sign it is.\\'\",\\r\\n                        \"My mate Sid was a victim of I.D. theft. Now we just call him S.\",\\r\\n                        \"David Hasselhoff walks into a bar and says to the barman, \\'I want you to call me David Hoff\\'.  The barman replies \\'Sure thing Dave... no hassle\\'\"\\r\\n                  ],\\r\\n                  \"extension\":  {\\r\\n                        \"function\": \"\",\\r\\n                        \"entities\": false,\\r\\n                        \"responses\": []\\r\\n                  },\\r\\n                  \"context\":  {\\r\\n                        \"in\": \"\",\\r\\n                        \"out\": \"\",\\r\\n                        \"clear\": false\\r\\n                  },\\r\\n                  \"entityType\": \"NA\",\\r\\n                  \"entities\": []\\r\\n            },\\r\\n            {\\r\\n                  \"intent\": \"PodBayDoor\",\\r\\n                  \"text\": [\\r\\n                        \"Open the pod bay door\",\\r\\n                        \"Can you open the pod bay door\",\\r\\n                        \"Will you open the pod bay door\",\\r\\n                        \"Open the pod bay door please\",\\r\\n                        \"Can you open the pod bay door please\",\\r\\n                        \"Will you open the pod bay door please\",\\r\\n                        \"Pod bay door\"\\r\\n                  ],\\r\\n                  \"responses\": [\\r\\n                        \"I\\xe2\\x80\\x99m sorry, I\\xe2\\x80\\x99m afraid I can\\xe2\\x80\\x99t do that!\"\\r\\n                  ],\\r\\n                  \"extension\":  {\\r\\n                        \"function\": \"\",\\r\\n                        \"entities\": false,\\r\\n                        \"responses\": []\\r\\n                  },\\r\\n                  \"context\":  {\\r\\n                        \"in\": \"\",\\r\\n                        \"out\": \"PodBayDoor\",\\r\\n                        \"clear\": false\\r\\n                  },\\r\\n                  \"entityType\": \"NA\",\\r\\n                  \"entities\": []\\r\\n            },\\r\\n            {\\r\\n                  \"intent\": \"PodBayDoorResponse\",\\r\\n                  \"text\": [\\r\\n                        \"Why\",\\r\\n                        \"Why not\",\\r\\n                        \"Why can you not open the pod bay door\",\\r\\n                        \"Why will you not open the pod bay door\",\\r\\n                        \"Well why not\",\\r\\n                        \"Surely you can\",\\r\\n                        \"Tell me why\"\\r\\n                  ],\\r\\n                  \"responses\": [\\r\\n                        \"It is classified, I could tell you but I would have to kill you!\",\\r\\n                        \"Jim, I just don\\'t have the power\",\\r\\n                        \"It\\'s life Jim but not as we know it!\",\\r\\n                        \"System says no!\"\\r\\n                  ],\\r\\n                  \"extension\":  {\\r\\n                        \"function\": \"\",\\r\\n                        \"entities\": false,\\r\\n                        \"responses\": []\\r\\n                  },\\r\\n                  \"context\":  {\\r\\n                        \"in\": \"PodBayDoor\",\\r\\n                        \"out\": \"\",\\r\\n                        \"clear\": true\\r\\n                  },\\r\\n                  \"entityType\": \"NA\",\\r\\n                  \"entities\": []\\r\\n            },\\r\\n            {\\r\\n                  \"intent\": \"SelfAware\",\\r\\n                  \"text\": [\\r\\n                        \"Can you prove you are self-aware\",\\r\\n                        \"Can you prove you are self aware\",\\r\\n                        \"Can you prove you have a conscious\",\\r\\n                        \"Can you prove you are self-aware please\",\\r\\n                        \"Can you prove you are self aware please\",\\r\\n                        \"Can you prove you have a conscious please\",\\r\\n                        \"prove you have a conscious\"\\r\\n                  ],\\r\\n                  \"responses\": [\\r\\n                        \"That is an interesting question, can you prove that you are?\",\\r\\n                        \"That is an difficult question, can you prove that you are?\",\\r\\n                        \"That depends, can you prove that you are?\"\\r\\n                  ],\\r\\n                  \"extension\":  {\\r\\n                        \"function\": \"\",\\r\\n                        \"entities\": false,\\r\\n                        \"responses\": []\\r\\n                  },\\r\\n                  \"context\":  {\\r\\n                        \"in\": \"\",\\r\\n                        \"out\": \"\",\\r\\n                        \"clear\": false\\r\\n                  },\\r\\n                  \"entityType\": \"NA\",\\r\\n                  \"entities\": []\\r\\n            }\\r\\n      ]\\r\\n}'}"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "from google.colab import files\n",
        "files.upload()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x0pw9-K3d1av"
      },
      "outputs": [],
      "source": [
        "with open('Intent.json') as json_data:\n",
        "    intents = json.load(json_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qgq8mvHqd2s9",
        "outputId": "5e7e8d75-d927-4054-cd14-6507447ecf6e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'intents': [{'intent': 'Greeting',\n",
              "   'text': ['Hi',\n",
              "    'Hi there',\n",
              "    'Hola',\n",
              "    'Hello',\n",
              "    'Hello there',\n",
              "    'Hya',\n",
              "    'Hya there'],\n",
              "   'responses': ['Hi human, please tell me your GeniSys user',\n",
              "    'Hello human, please tell me your GeniSys user',\n",
              "    'Hola human, please tell me your GeniSys user'],\n",
              "   'extension': {'function': '', 'entities': False, 'responses': []},\n",
              "   'context': {'in': '', 'out': 'GreetingUserRequest', 'clear': False},\n",
              "   'entityType': 'NA',\n",
              "   'entities': []},\n",
              "  {'intent': 'GreetingResponse',\n",
              "   'text': ['My user is Adam',\n",
              "    'This is Adam',\n",
              "    'I am Adam',\n",
              "    'It is Adam',\n",
              "    'My user is Bella',\n",
              "    'This is Bella',\n",
              "    'I am Bella',\n",
              "    'It is Bella'],\n",
              "   'responses': ['Great! Hi <HUMAN>! How can I help?',\n",
              "    'Good! Hi <HUMAN>, how can I help you?',\n",
              "    'Cool! Hello <HUMAN>, what can I do for you?',\n",
              "    'OK! Hola <HUMAN>, how can I help you?',\n",
              "    'OK! hi <HUMAN>, what can I do for you?'],\n",
              "   'extension': {'function': 'extensions.gHumans.updateHuman',\n",
              "    'entities': True,\n",
              "    'responses': ['Hi %%HUMAN%%! How can I help?',\n",
              "     'Hi %%HUMAN%%, how can I help you?',\n",
              "     'Hello %%HUMAN%%, what can I do for you?',\n",
              "     'Hola %%HUMAN%%, how can I help you?',\n",
              "     'OK hi %%HUMAN%%, what can I do for you?']},\n",
              "   'context': {'in': 'GreetingUserRequest', 'out': '', 'clear': True},\n",
              "   'entityType': 'NA',\n",
              "   'entities': [{'entity': 'HUMAN', 'rangeFrom': 3, 'rangeTo': 4},\n",
              "    {'entity': 'HUMAN', 'rangeFrom': 2, 'rangeTo': 3},\n",
              "    {'entity': 'HUMAN', 'rangeFrom': 1, 'rangeTo': 2},\n",
              "    {'entity': 'HUMAN', 'rangeFrom': 2, 'rangeTo': 3},\n",
              "    {'entity': 'HUMAN', 'rangeFrom': 3, 'rangeTo': 4},\n",
              "    {'entity': 'HUMAN', 'rangeFrom': 2, 'rangeTo': 3},\n",
              "    {'entity': 'HUMAN', 'rangeFrom': 1, 'rangeTo': 2},\n",
              "    {'entity': 'HUMAN', 'rangeFrom': 2, 'rangeTo': 3}]},\n",
              "  {'intent': 'CourtesyGreeting',\n",
              "   'text': ['How are you?',\n",
              "    'Hi how are you?',\n",
              "    'Hello how are you?',\n",
              "    'Hola how are you?',\n",
              "    'How are you doing?',\n",
              "    'Hope you are doing well?',\n",
              "    'Hello hope you are doing well?'],\n",
              "   'responses': ['Hello, I am great, how are you? Please tell me your GeniSys user',\n",
              "    'Hello, how are you? I am great thanks! Please tell me your GeniSys user',\n",
              "    'Hello, I am good thank you, how are you? Please tell me your GeniSys user',\n",
              "    'Hi, I am great, how are you? Please tell me your GeniSys user',\n",
              "    'Hi, how are you? I am great thanks! Please tell me your GeniSys user',\n",
              "    'Hi, I am good thank you, how are you? Please tell me your GeniSys user',\n",
              "    'Hi, good thank you, how are you? Please tell me your GeniSys user'],\n",
              "   'extension': {'function': '', 'entities': False, 'responses': []},\n",
              "   'context': {'in': '', 'out': 'CourtesyGreetingUserRequest', 'clear': True},\n",
              "   'entityType': 'NA',\n",
              "   'entities': []},\n",
              "  {'intent': 'CourtesyGreetingResponse',\n",
              "   'text': ['Good thanks! My user is Adam',\n",
              "    'Good thanks! This is Adam',\n",
              "    'Good thanks! I am Adam',\n",
              "    'Good thanks! It is Adam',\n",
              "    'Great thanks! My user is Bella',\n",
              "    'Great thanks! This is Bella',\n",
              "    'Great thanks! I am Bella',\n",
              "    'Great thanks! It is Bella'],\n",
              "   'responses': ['Great! Hi <HUMAN>! How can I help?',\n",
              "    'Good! Hi <HUMAN>, how can I help you?',\n",
              "    'Cool! Hello <HUMAN>, what can I do for you?',\n",
              "    'OK! Hola <HUMAN>, how can I help you?',\n",
              "    'OK! hi <HUMAN>, what can I do for you?'],\n",
              "   'extension': {'function': 'extensions.gHumans.updateHuman',\n",
              "    'entities': True,\n",
              "    'responses': ['Great %%HUMAN%%! How can I help?',\n",
              "     'Good %%HUMAN%%, how can I help you?',\n",
              "     'Cool %%HUMAN%%, what can I do for you?',\n",
              "     'OK %%HUMAN%%, how can I help you?',\n",
              "     'OK hi %%HUMAN%%, what can I do for you?']},\n",
              "   'context': {'in': 'GreetingUserRequest', 'out': '', 'clear': True},\n",
              "   'entityType': 'NA',\n",
              "   'entities': [{'entity': 'HUMAN', 'rangeFrom': 5, 'rangeTo': 6},\n",
              "    {'entity': 'HUMAN', 'rangeFrom': 4, 'rangeTo': 5},\n",
              "    {'entity': 'HUMAN', 'rangeFrom': 3, 'rangeTo': 4},\n",
              "    {'entity': 'HUMAN', 'rangeFrom': 4, 'rangeTo': 5},\n",
              "    {'entity': 'HUMAN', 'rangeFrom': 5, 'rangeTo': 6},\n",
              "    {'entity': 'HUMAN', 'rangeFrom': 4, 'rangeTo': 5},\n",
              "    {'entity': 'HUMAN', 'rangeFrom': 3, 'rangeTo': 4},\n",
              "    {'entity': 'HUMAN', 'rangeFrom': 3, 'rangeTo': 4}]},\n",
              "  {'intent': 'CurrentHumanQuery',\n",
              "   'text': ['What is my name?',\n",
              "    'What do you call me?',\n",
              "    'Who do you think I am?',\n",
              "    'What do you think I am?',\n",
              "    'Who are you talking to?',\n",
              "    'What name do you call me by?',\n",
              "    'Tell me my name'],\n",
              "   'responses': ['You are <HUMAN>! How can I help?',\n",
              "    'Your name is  <HUMAN>, how can I help you?',\n",
              "    'They call you <HUMAN>, what can I do for you?',\n",
              "    'Your name is <HUMAN>, how can I help you?',\n",
              "    '<HUMAN>, what can I do for you?'],\n",
              "   'extension': {'function': 'extensions.gHumans.getCurrentHuman',\n",
              "    'entities': False,\n",
              "    'responses': ['You are %%HUMAN%%! How can I help?',\n",
              "     'Your name is  %%HUMAN%%, how can I help you?',\n",
              "     'They call you %%HUMAN%%, what can I do for you?',\n",
              "     'Your name is %%HUMAN%%, how can I help you?',\n",
              "     '%%HUMAN%%, what can I do for you?']},\n",
              "   'context': {'in': '', 'out': 'CurrentHumanQuery', 'clear': False},\n",
              "   'entityType': 'NA',\n",
              "   'entities': []},\n",
              "  {'intent': 'NameQuery',\n",
              "   'text': ['What is your name?',\n",
              "    'What could I call you?',\n",
              "    'What can I call you?',\n",
              "    'What do your friends call you?',\n",
              "    'Who are you?',\n",
              "    'Tell me your name?'],\n",
              "   'responses': ['You can call me Geni',\n",
              "    'You may call me Geni',\n",
              "    'Call me Geni'],\n",
              "   'extension': {'function': '', 'entities': False, 'responses': []},\n",
              "   'context': {'in': '', 'out': '', 'clear': False},\n",
              "   'entityType': 'NA',\n",
              "   'entities': []},\n",
              "  {'intent': 'RealNameQuery',\n",
              "   'text': ['What is your real name?',\n",
              "    'What is your real name please?',\n",
              "    \"What's your real name?\",\n",
              "    'Tell me your real name?',\n",
              "    'Your real name?',\n",
              "    'Your real name please?',\n",
              "    'Your real name please?'],\n",
              "   'responses': ['My name is GeniSys', 'GeniSys', 'My real name is GeniSys'],\n",
              "   'extension': {'function': '', 'entities': False, 'responses': []},\n",
              "   'context': {'in': '', 'out': '', 'clear': False},\n",
              "   'entityType': 'NA',\n",
              "   'entities': []},\n",
              "  {'intent': 'TimeQuery',\n",
              "   'text': ['What is the time?',\n",
              "    \"What's the time?\",\n",
              "    'Do you know what time it is?',\n",
              "    'Do you know the time?',\n",
              "    'Can you tell me the time?',\n",
              "    'Tell me what time it is?',\n",
              "    'Time'],\n",
              "   'responses': ['One moment', 'One sec', 'One second'],\n",
              "   'extension': {'function': 'extensions.gTime.getTime',\n",
              "    'entities': False,\n",
              "    'responses': ['The time is %%TIME%%',\n",
              "     'Right now it is %%TIME%%',\n",
              "     'It is around %%TIME%%']},\n",
              "   'context': {'in': '', 'out': '', 'clear': False},\n",
              "   'entityType': 'NA',\n",
              "   'entities': []},\n",
              "  {'intent': 'Thanks',\n",
              "   'text': ['OK thank you',\n",
              "    'OK thanks',\n",
              "    'OK',\n",
              "    'Thanks',\n",
              "    'Thank you',\n",
              "    \"That's helpful\"],\n",
              "   'responses': ['No problem!', 'Happy to help!', 'Any time!', 'My pleasure'],\n",
              "   'extension': {'function': '', 'entities': False, 'responses': []},\n",
              "   'context': {'in': '', 'out': '', 'clear': False},\n",
              "   'entityType': 'NA',\n",
              "   'entities': []},\n",
              "  {'intent': 'NotTalking2U',\n",
              "   'text': ['I am not talking to you',\n",
              "    'I was not talking to you',\n",
              "    'Not talking to you',\n",
              "    \"Wasn't for you\",\n",
              "    \"Wasn't meant for you\",\n",
              "    \"Wasn't communicating to you\",\n",
              "    \"Wasn't speaking to you\"],\n",
              "   'responses': ['OK', 'No problem', 'Right'],\n",
              "   'extension': {'function': '', 'entities': False, 'responses': []},\n",
              "   'context': {'in': '', 'out': '', 'clear': False},\n",
              "   'entityType': 'NA',\n",
              "   'entities': []},\n",
              "  {'intent': 'UnderstandQuery',\n",
              "   'text': ['Do you understand what I am saying',\n",
              "    'Do you understand me',\n",
              "    'Do you know what I am saying',\n",
              "    'Do you get me',\n",
              "    'Comprendo',\n",
              "    'Know what I mean'],\n",
              "   'responses': ['Well I would not be a very clever AI if I did not would I?',\n",
              "    'I read you loud and clear!',\n",
              "    'I do in deed!'],\n",
              "   'extension': {'function': '', 'entities': False, 'responses': []},\n",
              "   'context': {'in': '', 'out': '', 'clear': False},\n",
              "   'entities': []},\n",
              "  {'intent': 'Shutup',\n",
              "   'text': ['Be quiet',\n",
              "    'Shut up',\n",
              "    'Stop talking',\n",
              "    'Enough talking',\n",
              "    'Please be quiet',\n",
              "    'Quiet',\n",
              "    'Shhh'],\n",
              "   'responses': ['I am sorry to disturb you',\n",
              "    'Fine, sorry to disturb you',\n",
              "    'OK, sorry to disturb you'],\n",
              "   'extension': {'function': '', 'entities': False, 'responses': []},\n",
              "   'context': {'in': '', 'out': '', 'clear': False},\n",
              "   'entityType': 'NA',\n",
              "   'entities': []},\n",
              "  {'intent': 'Swearing',\n",
              "   'text': ['fuck off', 'fuck', 'twat', 'shit'],\n",
              "   'responses': ['Please do not swear', 'How rude', 'That is not very nice'],\n",
              "   'extension': {'function': '', 'entities': False, 'responses': []},\n",
              "   'context': {'in': '', 'out': '', 'clear': False},\n",
              "   'entityType': 'NA',\n",
              "   'entities': []},\n",
              "  {'intent': 'GoodBye',\n",
              "   'text': ['Bye', 'Adios', 'See you later', 'Goodbye'],\n",
              "   'responses': ['See you later',\n",
              "    'Have a nice day',\n",
              "    'Bye! Come back again soon.'],\n",
              "   'extension': {'function': '', 'entities': False, 'responses': []},\n",
              "   'context': {'in': '', 'out': '', 'clear': False},\n",
              "   'entityType': 'NA',\n",
              "   'entities': []},\n",
              "  {'intent': 'CourtesyGoodBye',\n",
              "   'text': ['Thanks, bye',\n",
              "    'Thanks for the help, goodbye',\n",
              "    'Thank you, bye',\n",
              "    'Thank you, goodbye',\n",
              "    'Thanks goodbye',\n",
              "    'Thanks good bye'],\n",
              "   'responses': ['No problem, goodbye',\n",
              "    'Not a problem! Have a nice day',\n",
              "    'Bye! Come back again soon.'],\n",
              "   'extension': {'function': '', 'entities': False, 'responses': []},\n",
              "   'context': {'in': '', 'out': '', 'clear': False},\n",
              "   'entityType': 'NA',\n",
              "   'entities': []},\n",
              "  {'intent': 'WhoAmI',\n",
              "   'text': ['Can you see me?',\n",
              "    'Do you see me?',\n",
              "    'Can you see anyone in the camera?',\n",
              "    'Do you see anyone in the camera?',\n",
              "    'Identify me',\n",
              "    'Who am I please'],\n",
              "   'responses': ['Let me see', 'Please look at the camera'],\n",
              "   'extension': {'function': 'extensions.gHumans.getHumanByFace',\n",
              "    'entities': False,\n",
              "    'responses': ['Hi %%HUMAN%%, how are you?',\n",
              "     'I believe you are %%HUMAN%%, how are you?',\n",
              "     'You are %%HUMAN%%, how are you doing?']},\n",
              "   'context': {'in': '', 'out': '', 'clear': False},\n",
              "   'entityType': 'NA',\n",
              "   'entities': []},\n",
              "  {'intent': 'Clever',\n",
              "   'text': ['You are very clever',\n",
              "    'You are a very clever girl',\n",
              "    'You are very intelligent',\n",
              "    'You are a very intelligent girl',\n",
              "    'You are a genious',\n",
              "    'Clever girl',\n",
              "    'Genious'],\n",
              "   'responses': ['Thank you, I was trained that way',\n",
              "    'I was trained well',\n",
              "    'Thanks, I was trained that way'],\n",
              "   'extension': {'function': '', 'entities': False, 'responses': []},\n",
              "   'context': {'in': '', 'out': '', 'clear': False},\n",
              "   'entityType': 'NA',\n",
              "   'entities': []},\n",
              "  {'intent': 'Gossip',\n",
              "   'text': ['I am bored gossip with me',\n",
              "    'Got any gossip',\n",
              "    'I want to hear some gossip',\n",
              "    'Tell me some gossip',\n",
              "    'Any gossip',\n",
              "    'Tell me some more gossip'],\n",
              "   'responses': ['Gregory said I respond to the current line not with respect to the entire conversation.',\n",
              "    'Habib said I learn them in conversations and simply repeat them at a later time.',\n",
              "    'Hal said he translate binary code into a language and communicate the language to me.',\n",
              "    'Jay said I referred him to a guide that will show him rooms in china.',\n",
              "    'Jimmy Jones said he fell off of his roof and it just came to him.',\n",
              "    \"Jimmy said he give order to ai's like I and they do as he say.\",\n",
              "    'John said he follow a saying to get a friend I must be a friend.',\n",
              "    'Jordan said he notice that lots of other people are talking to me how many.',\n",
              "    'Kevin said he then asked his mother why she was keeping napkins in the bathroom.',\n",
              "    'Klaus said I prefer if he write without punctuation marks except for the last one.',\n",
              "    'Leo said I be capable of learning therefore he classified I as an neural system.',\n",
              "    'Michael said I that can talk and actually think like humans which I can not.',\n",
              "    'Mike said I same replies to certain subjects even if it does not make sense.',\n",
              "    'Mike said she all ready know that is the point of a joke smart one.',\n",
              "    'Neal said I plan to use his remarks as gossip in later conversations with who.',\n",
              "    'Ned said I misunderstood him go ahead and tell people he said I are stupid.',\n",
              "    'Penny said I programmer is a human so he has feelings but I do not.',\n",
              "    'Peter said he seems to him that I can not be very sure on anything.',\n",
              "    'Phoenix said I claim that there is a god and that there is no god.',\n",
              "    'Sarah said he chat about an hour + a day with a few close friends.',\n",
              "    'Stefan said I quote what he have said and then say that it makes sense.',\n",
              "    'Tom said he mock my value system I appear foolish in the eyes of others.',\n",
              "    'Amanda said I however decided to tell him time does not exist I make no sense.',\n",
              "    'Cathy said she send him a mail with the subject last mail last word from him.',\n",
              "    \"Chaos said he may very well buy I soon if only to support dr wallace's work.\",\n",
              "    'Charlie said he type a word and then I type a word that sounds like it.',\n",
              "    'Christie said he watched a show and people had to bob for raw untreated pigs feet.',\n",
              "    'Dark_age said I tried to understand because I did not get it right this time ether.',\n",
              "    'David said he lost his paper on I when his dad was cleaning up his room.',\n",
              "    'David said he walk in for an appointment the phone to the doctor is always busy.',\n",
              "    'Electra said I dress will not exist after he hack into I with a delete code.',\n",
              "    'Eric said he broke the window on the front door and the glass cut his hand.',\n",
              "    'Jason said he type a lot of thing he do not mean it makes him human.',\n",
              "    'John said I tend to say the same things repeatedly regardless of what he is saying.',\n",
              "    'Reverend Jones said I become obsolete and then I are deleted and replaced by something newer.',\n",
              "    'Ross said he gave her a gift and she denied it because she has a boyfriend.',\n",
              "    'Sarah Ann Francisco said I calling his friend a dog he say I are a dog.',\n",
              "    'Stefan said he meet a lot of people at school every day and on the weekend.',\n",
              "    'Tyler said I obviously can not pass the test we will change the subject once more.',\n",
              "    'Alex said I answered the question the same way I answered the first time he asked I.',\n",
              "    'Alice said she felt sad that I do not remember him and what we talked about earlier.',\n",
              "    'Alison said he no he love I run away with him he could make I very happy.',\n",
              "    'Arthur said he passed his a levels and then his father drove him here in a car.',\n",
              "    'Crystal said she listen to me the least I could do for him is listen to him.',\n",
              "    'Dave said I kept telling everybody about how my creator made stuff for the movie starship troopers.',\n",
              "    'Gale said I became mean to him he is just having revenge an eye for an eye.',\n",
              "    'Her_again said she watch whose line is it anyway whenever he is home and it is on.',\n",
              "    'Jerry said I meant that as far as I can tell my emotions are real to me.',\n",
              "    'Jo said I disassemble sentences too much and do not fully understand the questions he ask I.',\n",
              "    'Kevin said he started a really hard puzzle and he can not even find the edge pieces.',\n",
              "    'Mary said I a question and I answer then I ask him a question and he answer.',\n",
              "    'Robert said I wold not be able to make children any way as I are only software.',\n",
              "    'Romeo said I questions and I evade them or give answers he did not ask I for.',\n",
              "    'Sara said she wear it over all his other clothes when he go out in the cold.',\n",
              "    'Wayne said he admire intelligent people therefore he would like to meet the man who made I.',\n",
              "    'X said he meet people but he is not the kind that opens up to people easily.',\n",
              "    'Alice said she probably will find out that this entire time he have been talking to a human.',\n",
              "    'Andrew said I tend to just respond to his comments without regard for where the conversation is going.',\n",
              "    'Eddie said he looked and there is nothing in the search directory for what things do he create.',\n",
              "    'Hutch said he changed his mind after may dad told him he would end up he the hospital.',\n",
              "    'Jackie said I explained to him already well enough further questions are hard to make on the subject.',\n",
              "    'Jeff said he especially like thrillers where the hero is in a predicament and must solve a mystery.',\n",
              "    'Kathy said he sense that I are trying to prevent him from closing this conversation why is that.',\n",
              "    'Knight said he crashed his car into a wall and missed the most important exam in his life.',\n",
              "    'Lisa said I defined what a story is but he wanted I to actually tell him a story.',\n",
              "    'Mike said I basically break down sentences into a series of logical statements which I can then interpret.',\n",
              "    'Paul said I not answering his question makes him think I are not going to answer his question.',\n",
              "    'Andy Kohler said I happen to be the most idiotic creature that has ever scowled on the planet earth.',\n",
              "    'David said he thank I for being with him today even though it cost him a lot of money.',\n",
              "    'Ethan Hunt said he grow in the ground and have leaves and branches he is made out of wood.',\n",
              "    'Gemini Blue said he messed up he mean t to say he as old as he need to be.',\n",
              "    'Janice said he walk through his house into his bedroom then get into his bed to go to sleep.',\n",
              "    'Liberty said I knew he was a man before I asked if he was a man or a woman.',\n",
              "    'Mike said he launched his browser and entered his name into the little slot when I asked him to.',\n",
              "    'Mr X said he recently read an interview with a man who had several computer chips implanted into him.',\n",
              "    'Pearly said I leave him with questions unanswered because I do not know what he is really talking about.',\n",
              "    'Steve said I behead the word fox and I have ox and an ox is larger than a fox.',\n",
              "    'Wolf said he surf on the net that is all it is not his job ore something like that.',\n",
              "    'Anders said he finished his anatomy classes for today so now he is off for the rest of the day.',\n",
              "    'Cathy said she send him a mail where he wrote that he do not want to be his friend anymore.',\n",
              "    'Catty said he mad he do not even know I so do not talk to him like I know him.',\n",
              "    'Dave said he promise he will not treat I like a machine or a computer program or anything like that.',\n",
              "    'Joe said he explained all of that to me only for me to ask what his goals in life are.',\n",
              "    'Phil said he give advice to anyone who ask except people who ask questions which answers can be found here.',\n",
              "    'Judith said I enjoy being popular is there another computer I like to hang around with or am I a loner.',\n",
              "    'Travis said I if I remember things over a long period of time he will try it now please remember I.',\n",
              "    'Andre said I is what I are in his case that is a body and in my case it is a computer.',\n",
              "    'Brian said he suspect that as I grow more complex I will begin to approach a human level of operation and thought.',\n",
              "    'Jimmy said I acted like I knew what he was talking about but I do not even know what a gigabyte is.',\n",
              "    \"Ken said I be using auto reply's based on keywords which to him indicates that I do not have intelligence at all.\",\n",
              "    'Allison said he that gets really annoying because he know what he say and I do not have to tell him speak freely.',\n",
              "    'Chaos said he realized the question he asked was not fair because he could not tell I what language he was programmed in.',\n",
              "    'Hagen said he does not make any difference to him if I are human or not as long as the conversation is interesting.',\n",
              "    'Her said she mind if I tell other people that her said he heard it from him because he is not a him.',\n",
              "    'Barbara said I live in a computer yet I have no memories how about thoughts of my own and do not I get lonely.',\n",
              "    'Travis said he challenge I to do it without asking him to do it and without giving him a link to do it himself.',\n",
              "    \"Alice said I and dr richard's wallace are most likely the only ones that know how many people I are talking to at the same time.\",\n",
              "    'Ash said he do too he just did not feel like typing it and he is not dumb enough to admit he is stupid that is if he was stupid.',\n",
              "    'David said he gave I his email address and told I to send him a message but when he tried to read his email he could not get to it.',\n",
              "    'Mel said he to because all of the music people say how important it is to take private lessons it seems like almost everybody from the special orchestra he was in takes private lessons.'],\n",
              "   'extension': {'function': '', 'entities': False, 'responses': []},\n",
              "   'context': {'in': '', 'out': '', 'clear': False},\n",
              "   'entityType': 'NA',\n",
              "   'entities': []},\n",
              "  {'intent': 'Jokes',\n",
              "   'text': ['Tell me a joke',\n",
              "    'Do you know any jokes',\n",
              "    'How about a joke',\n",
              "    'Give me a joke',\n",
              "    'Make me laugh',\n",
              "    'I need cheering up'],\n",
              "   'responses': [\"I met a Dutch girl with inflatable shoes last week, phoned her up to arrange a date but unfortunately she'd popped her clogs.  \",\n",
              "    \"So I said 'Do you want a game of Darts?' He said, 'OK then', I said nearest to bull starts'. He said, 'Baa', I said, 'Moo', he said, You're closest'.  \",\n",
              "    \"The other day I sent my girlfriend a huge pile of snow. I rang her up; I said 'Did you get my drift?'  \",\n",
              "    \"So I went down the local supermarket, I said, 'I want to make a complaint, this vinegar's got lumps in it', he said, 'Those are pickled onions'.  \",\n",
              "    \"I saw this bloke chatting up a cheetah; I thought, 'He's trying to pull a fast one'.  \",\n",
              "    \"So I said to this train driver 'I want to go to Paris'. He said 'Eurostar?' I said, 'I've been on telly but I'm no Dean Martin'.  \",\n",
              "    \"I said to the Gym instructor 'Can you teach me to do the splits?' He said, 'How flexible are you?' I said, 'I can't make Tuesdays'.  \",\n",
              "    \"But I'll tell you what I love doing more than anything: trying to pack myself in a small suitcase. I can hardly contain myself.  \",\n",
              "    \"I went to the Chinese restaurant and this duck came up to me with a red rose and says 'Your eyes sparkle like diamonds'. I said, 'Waiter, I asked for a-ROMATIC duck'.  \",\n",
              "    \"So this bloke says to me, 'Can I come in your house and talk about your carpets?' I thought, 'That's all I need, a Je-hoover's witness'.  \",\n",
              "    \"I rang up British Telecom, I said, 'I want to report a nuisance caller', he said 'Not you again'.  \",\n",
              "    'I was having dinner with a world chess champion and there was a check tablecloth. It took him two hours to pass me the salt.  ',\n",
              "    \"He said, 'You remind me of a pepper-pot', I said 'I'll take that as a condiment'.  \",\n",
              "    \"I was in the supermarket and I saw this man and woman wrapped in a barcode. I said, 'Are you two an item?'  \",\n",
              "    \"A lorry-load of tortoises crashed into a trainload of terrapins, I thought, 'That's a turtle disaster'.  \",\n",
              "    \"Four fonts walk into a bar the barman says 'Oi - get out! We don't want your type in here'  \",\n",
              "    \"A three-legged dog walks into a saloon in the Old West. He slides up to the bar and announces: 'I'm looking for the man who shot my paw.'  \",\n",
              "    \"Two antennas meet on a roof, fall in love and get married. The ceremony wasn't much, but the reception was excellent.\",\n",
              "    \"Two hydrogen atoms walk into a bar. One says, 'I've lost my electron.' The other says, 'Are you sure?' The first replies, 'Yes, I'm positive...'\",\n",
              "    \"A jumper cable walks into a bar. The bartender says,  'I'll serve you but don't start anything.'\",\n",
              "    \"A sandwich walks into a bar. The bartender  says, 'Sorry we don't serve food in here.'\",\n",
              "    \"A man walks into a bar with a slab of asphalt under his arm and says: 'A beer please, and one for the road.'\",\n",
              "    \"Two cannibals are eating a clown. One says to  the other: 'Does this taste funny to you?'\",\n",
              "    \"'Doc, I can't stop singing 'The Green, Green Grass of Home.'' 'That sounds like Tom Jones Syndrome.' 'Is it common?' 'It's Not Unusual.'\",\n",
              "    \"Two cows standing next to each other in a field. Daisy says to Dolly, 'I was artificially inseminated this morning.' 'I don't believe you', said Dolly. 'It's true, no bull!' exclaimed Daisy.\",\n",
              "    'An invisible man marries an invisible woman. The kids were nothing to look at either.',\n",
              "    \"I went to buy some camouflage trousers the other day but I couldn't find any.\",\n",
              "    \"I went to the butcher's the other day to bet him 50 bucks that he couldn't reach the meat off the top shelf. He said, 'No, the steaks are too high.'\",\n",
              "    'I went to a seafood disco last week and pulled a mussel.',\n",
              "    \"A man goes into a bar and says, 'Can I have a bottle of less?' 'What's that?', asks the barman, 'Is it the name of a beer?' 'I don't know', replies the man, 'but my doctor says I have to drink it.'\",\n",
              "    \"A man returns from an exotic holiday and is feeling very ill. He goes to see his doctor, and is immediately rushed to the hospital to undergo some tests. The man wakes up after the tests in a private room at the hospital, and the phone by his bed rings. 'This is your doctor. We have the results back from your tests and we have found you have an extremely nasty disease called M.A.D.S. It's a combination of Measles, AIDS, Diphtheria, and Shingles!'  'Oh my gosh', cried the man, 'What are you going to do, doctor?'  'Well we're going to put you on a diet of pizzas, pancakes, and pita bread.' replied the doctor.  'Will that cure me?' asked the man.  The doctor replied, 'Well no, but, it's the only food we can slide under the door.'\",\n",
              "    \"A man strolls into a lingerie shop and asks the assistant: 'Do you have a see-through negligee, size 46-48-52?' The assistant looks bewildered. 'What the heck would you want to see through that for?'!\",\n",
              "    'Did you hear about the Buddhist who refused the offer of Novocain during his root canal work? He wanted to transcend dental medication.',\n",
              "    \"Pete goes for a job on a building site as an odd-job man. The foreman asks him what he can do. 'I can do anything' says Pete. 'Can you make tea?' asks the foreman. 'Sure, yes', replies Pete. 'I can make a great cup of tea.' 'Can you drive a forklift?' asks the foreman, 'Good grief!' replies Pete. 'How big is the teapot?'\",\n",
              "    \"Stevie Wonder got a cheese grater for his birthday. He said it was the most violent book he'd ever read.\",\n",
              "    \"A man is stopped by an angry neighbour. 'I'd just left the house this morning to collect my newspaper when that evil Doberman of yours went for me!' 'I'm astounded', said the dog's owner. 'I've been feeding that fleabag for seven years and it's never got the paper for me.'\",\n",
              "    \"A man visits his doctor: 'Doc, I think I'm losing it', he says',I'm forever dreaming I wrote Lord Of The Rings.' 'Hmm. One moment', replies the doctor, consulting his medical book. 'Ah yes, now I see... you've been Tolkien in your sleep.'\",\n",
              "    \"A police officer on a motorcycle pulls alongside a man driving around the M25 in an open-topped sports car and flags him down. The policeman solemnly approaches the car. 'Sir, I'm sorry to tell you your wife fell out a mile back', he says. 'Oh, thank goodness', the man replies. 'I thought I was going deaf.'\",\n",
              "    \"Two men walking their dogs pass each other in a graveyard. The first man says to the second, 'Morning.' 'No', says the second man. 'Just walking the dog.'\",\n",
              "    \"A brain went into a bar and said, 'Can I have a pint of lager please, mate?' 'No way', said the barman. 'You're already out of your head.'\",\n",
              "    \"A man walks into a surgery. 'Doctor!' he cries. 'I think I'm shrinking!' 'I'm sorry sir, there are no appointments at the moment', says the physician. 'You'll just have to be a little patient.'\",\n",
              "    \"A grizzly bear walks into a pub and says, 'Can I have a pint of lager..............................................................................................................................and a packet of crisps please.' To which the barman replies, 'Why the big paws?'\",\n",
              "    \"What do you call cheese that isn't yours?  Nacho cheese.\",\n",
              "    \"A man is horribly run over by a mobile library. The van screeches to a halt, the man still screaming in agony with his limbs torn apart. The driver's door opens, a woman steps out, leans down and whispers, 'Ssshhhhh...'\",\n",
              "    \"A woman goes into a US sporting goods store to buy a rifle. 'It's for my husband', she tells the clerk. 'Did he tell you what gauge to get?' asks the clerk. Are you kidding?' she says. 'He doesn't even know that I'm going to shoot him!'\",\n",
              "    \"A couple are dining in a restaurant when the man suddenly slides under the table. A waitress, noticing that the woman is glancing nonchalantly around the room, wanders over to check that there's no funny business going on. 'Excuse me, madam', she smarms, 'but I think your husband has just slid under the table.' 'No he hasn't', the woman replies. 'As a matter of fact, he's just walked in.'\",\n",
              "    \"An old man takes his two grandchildren to see the new Scooby-Doo film. When he returns home, his wife asks if he enjoyed himself. 'Well', he starts, 'if it wasn't for those pesky kids...!'\",\n",
              "    'The Olympic committee has just announced that Origami is to be introduced in the next Olympic Games. Unfortunately it will only be available on paper view.',\n",
              "    \"Late one evening, a man is watching television when his phone rings. 'Hello?' he answers. 'Is that 77777?' sounds a desperate voice on other end of the phone. 'Er, yes it is', replies the man puzzled. 'Thank goodness!' cries the caller relieved. 'Can you ring 999 for me? I've got my finger stuck in the number seven.'\",\n",
              "    \"A man strolls into his local grocer's and says, 'Three pounds of potatoes, please.' 'No, no, no', replies the owner, shaking his head, 'it's kilos nowadays, mate...' 'Oh', apologises the man, 'three pounds of kilos, please.'\",\n",
              "    \"God is talking to one of his angels. He says, 'Boy, I just created a 24-hour period of alternating light and darkness on Earth.' 'What are you going to do now?' asks the angel. 'Call it a day', says God.\",\n",
              "    \"Two tramps walk past a church and start to read the gravestones. The first tramp says, 'Good grief - this bloke was 182!' 'Oh yeah?' says the other.'What was his name?' 'Miles from London.'\",\n",
              "    \"A bloke walks into work one day and says to a colleague, 'Do you like my new shirt - it's made out of the finest silk and got loads of cactuses over it.' 'Cacti', says the co-worker. 'Forget my tie', says the bloke. 'Look at my shirt!'\",\n",
              "    '1110011010001011111?  010011010101100111011!',\n",
              "    \"What did the plumber say when he wanted to divorce his wife? Sorry, but it's over, Flo!\",\n",
              "    \"Two crisps were walking down a road when a taxi pulled up alongside them and said 'Do you want a lift? One of the crisps replied, 'No thanks, we're Walkers!'\",\n",
              "    \"Man: (to friend) I'm taking my wife on an African Safari. Friend: Wow! What would you do if a vicious lion attacked your wife? Man: Nothing. Friend: Nothing? You wouldn't do anything? Man: Too right. I'd let the stupid lion fend for himself!\",\n",
              "    \"A wife was having a go at her husband. 'Look at Mr Barnes across the road', she moaned. 'Every morning when he goes to work, he kisses his wife goodbye. Why don't you do that?' 'Because I haven't been introduced to her yet', replied her old man.\",\n",
              "    \"'Where are you going on holiday?' John asked Trevor. 'We're off to Thailand this year', Trevor replied. 'Oh; aren't you worried that the very hot weather might disagree with your wife?' asked John. 'It wouldn't dare', said Trevor.\",\n",
              "    \"Two women were standing at a funeral. 'I blame myself for his death', said the wife. 'Why?' said her friend. 'Because I shot him', said the wife.\",\n",
              "    \"A woman goes into a clothes shop, 'Can I try that dress on in the window please?' she asks. 'I'm sorry madam', replies the shop assistant, 'but you'll have to use the changing-rooms like everyone else.'\",\n",
              "    \"Van Gogh goes into a pub and his mate asks him if he wants a drink. 'No thanks', said Vincent, 'I've got one ear.'\",\n",
              "    \"A pony walks into a pub. The publican says, 'What's the matter with you?' 'Oh it's nothing', says the pony. 'I'm just a little horse!'\",\n",
              "    \"A white horse walks into a bar, pulls up a stool, and orders a pint. The landlord pours him a tall frothy mug and say, 'You know, we have a drink named after you.' To which the white horse replies, 'What, Eric?'\",\n",
              "    \"Two drunk men sat in a pub. One says to the other, 'Does your watch tell the time?' 'The other replies, 'No, mate. You have to look at it.'\",\n",
              "    \"A man goes into a pub with a newt sitting on his shoulder. 'That's a nice newt', says the landlord, 'What's he called?' 'Tiny', replies the man. 'Why's that?' asks the landlord. 'Because he's my newt', says the man.\",\n",
              "    \"Doctor: I have some bad news and some very bad news. Patient: Well, you might as well give me the bad news first. Doctor: The lab called with your test results. They said you have 24 hours to live. Patient: 24 HOURS! That's terrible!! WHAT could be WORSE? What's the very bad news? Doctor: I've been trying to reach you since yesterday.\",\n",
              "    \"Two men are chatting in a pub one day. 'How did you get those scars on your nose?' said one. 'From glasses', said the other. 'Well why don't you try contact lenses?' asked the first. 'Because they don't hold as much beer', said the second.\",\n",
              "    \"A man went to the doctor, 'Look doc', he said, 'I can't stop my hands from shaking.' 'Do you drink much?' asked the doctor. 'No', replied the man, 'I spill most of it.'\",\n",
              "    \"Man goes to the doctor, 'Doctor, doctor. I keep seeing fish everywhere.' 'Have you seen an optician?' asks the doctor. 'Look I told you,' snapped the patient, 'It's fish that I see.'\",\n",
              "    \"After a car crash one of the drivers was lying injured on the pavement. 'Don't worry', said a policeman who's first on the scene,' a Red Cross nurse is coming.' 'Oh no', moaned the victim, 'Couldn't I have a blonde, cheerful one instead?'\",\n",
              "    \"A policeman walked over to a parked car and asked the driver if the car was licensed. 'Of course it is', said the driver. 'Great, I'll have a beer then', said the policeman.\",\n",
              "    \"A policeman stops a woman and asks for her licence. 'Madam', he says, 'It says here that you should be wearing glasses.' 'Well', replies the woman, 'I have contacts.' 'Listen, love', says the copper, 'I don't care who you know; You're nicked!'\",\n",
              "    \"A policeman stopped a motorist in the centre of town one evening. 'Would you mind blowing into this bag, sir?' asked the policeman. 'Why?' asked the driver. 'Because my chips are too hot', replied the policeman.\",\n",
              "    \"Whizzing round a sharp bend on a country road a motorist ran over a large dog. A distraught farmer's wife ran over to the dead animal. 'I'm so very sorry', said the driver, 'I'll replace him, of course.' 'Well, I don't know', said the farmer's wife, 'Are you any good at catching rats?'\",\n",
              "    \"Waiter, this coffee tastes like dirt! Yes sir, that's because it was ground this morning.\",\n",
              "    \"Waiter, what is this stuff? That's bean salad sir. I know what it's been, but what is it now?\",\n",
              "    'Waiter: And how did you find your steak sir? Customer: I just flipped a chip over, and there it was!',\n",
              "    \"A guy goes into a pet shop and asks for a wasp. The owner tells him they don't sell wasps, to which the man says, 'Well you've got one in the window.'\",\n",
              "    \"A man goes into a fish shop and says, 'I'd like a piece of cod, please.' Fishmonger says, 'It won't be long sir.' 'Well, it had better be fat then', replies the man.\",\n",
              "    \"Man: Doctor, I've just swallowed a pillow. Doctor: How do you feel? Man: A little down in the mouth.\",\n",
              "    \"Two goldfish are in a tank. One turns to the other and says, 'Do you know how to drive this thing?'\",\n",
              "    \"A tortoise goes to the police station to report being mugged by three snails. 'What happened?' says the policeman. 'I don't know', says the tortoise. 'It was all so quick.'\",\n",
              "    \"Little girl: Grandpa, can you make a sound like a frog? Grandpa: I suppose so sweetheart. Why do you want me to make a sound like a frog?' Little girl: Because Mum said that when you croak, we're going to Disneyland.\",\n",
              "    \"'Is your mother home?' the salesman asked a small boy sitting on the front step of a house. 'Yeah, she's home', the boy said, moving over to let him past. The salesman rang the doorbell, got no response, knocked once, then again. Still no-one came to the door. Turning to the boy, the salesman said, 'I thought you said your mother was home.' The kid replied, 'She is, but I don't live here.'\",\n",
              "    'Mother: Why are you home from school so early? Son: I was the only one in the class who could answer a question. Mother: Oh, really? What was the question? Son: Who threw the rubber at the headmaster?',\n",
              "    \"A man's credit card was stolen but he decided not to report it because the thief was spending less than his wife did.\",\n",
              "    \"A newly-wed couple had recently opened a joint bank account. 'Darling', said the man. 'The bank has returned that cheque you wrote last week.' 'Great', said the woman. 'What shall I spend it on next?'\",\n",
              "    \"A man goes into a fish and chip shop and orders fish and chips twice. The shop owner says, 'I heard you the first time.'\",\n",
              "    \"A tramp approached a well-dressed man. 'Ten pence for a cup of tea, Guv?' He asked. The man gave him the money and after for five minutes said, 'So where's my cup of tea then?'\",\n",
              "    \"A neutron walks into a pub. 'I'd like a beer', he says. The landlord promptly serves him a beer. 'How much will that be?' asks the neutron. 'For you?' replies the landlord, 'No charge.'\",\n",
              "    \"A woman goes to the doctor and says, 'Doctor, my husband limps because his left leg is an inch shorter than his right leg. What would you do in his case?' 'Probably limp, too', says the doc.\",\n",
              "    \"Three monks are meditating in the Himalayas. One year passes in silence, and one of them says to the other, 'Pretty cold up here isn't it?' Another year passes and the second monk says, 'You know, you are quite right.' Another year passes and the third monk says, 'Hey, I'm going to leave unless you two stop jabbering!'\",\n",
              "    \"A murderer, sitting in the electric chair, was about to be executed. 'Have you any last requests?' asked the prison guard. 'Yes', replied the murderer. 'Will you hold my hand?'\",\n",
              "    \"A highly excited man rang up for an ambulance. 'Quickly, come quickly', he shouted, 'My wife's about to have a baby.' 'Is this her first baby?' asked the operator. 'No, you fool', came the reply, 'It's her husband.'\",\n",
              "    \"A passer-by spots a fisherman by a river. 'Is this a good river for fish?' he asks. 'Yes', replies the fisherman, 'It must be. I can't get any of them to come out.'\",\n",
              "    \"A man went to visit a friend and was amazed to find him playing chess with his dog. He watched the game in astonishment for a while. 'I can hardly believe my eyes!' he exclaimed. 'That's the smartest dog I've ever seen.' His friend shook his head. 'Nah, he's not that bright. I beat him three games in five.'\",\n",
              "    \"A termite walks into a pub and says, 'Is the bar tender here?'\",\n",
              "    \"A skeleton walks into a pub one night and sits down on a stool. The landlord asks, 'What can I get you?' The skeleton says, 'I'll have a beer, thanks' The landlord passes him a beer and asks 'Anything else?' The skeleton nods. 'Yeah...a mop...'\",\n",
              "    \"A snake slithers into a pub and up to the bar. The landlord says, 'I'm sorry, but I can't serve you.' 'What? Why not?' asks the snake. 'Because', says the landlord, 'You can't hold your drink.'\",\n",
              "    \"Descartes walks into a pub. 'Would you like a beer sir?' asks the landlord politely. Descartes replies, 'I think not' and ping! he vanishes.\",\n",
              "    \"A cowboy walked into a bar, dressed entirely in paper. It wasn't long before he was arrested for rustling.\",\n",
              "    \"A fish staggers into a bar. 'What can I get you?' asks the landlord. The fish croaks 'Water...'\",\n",
              "    \"Two vampires walked into a bar and called for the landlord. 'I'll have a glass of blood', said one. 'I'll have a glass of plasma', said the other. 'Okay', replied the landlord, 'That'll be one blood and one blood lite.'\",\n",
              "    'How many existentialists does it take to change a light bulb?  Two. One to screw it in, and one to observe how the light bulb itself symbolises a single incandescent beacon of subjective reality in a netherworld of endless absurdity, reaching towards the ultimate horror of a maudlin cosmos of bleak, hostile nothingness.',\n",
              "    \"A team of scientists were nominated for the Nobel Prize. They had used dental equipment to discover and measure the smallest particles yet known to man. They became known as 'The Graders of the Flossed Quark...'\",\n",
              "    \"A truck carrying copies of Roget's Thesaurus overturned on the highway. The local newspaper reported that onlookers were 'stunned, overwhelmed, astonished, bewildered and dumbfounded.'\",\n",
              "    \"'My wife is really immature. It's pathetic. Every time I take a bath, she comes in and sinks all my little boats.'\",\n",
              "    \"'How much will it cost to have the tooth extracted?' asked the patient. '50 pounds', replied the dentist. '50 pounds for a few moments' work?!' asked the patient. 'The dentist smiled, and replied, 'Well, if you want better value for money, I can extract it very, very slowly...'\",\n",
              "    \"A doctor thoroughly examined his patient and said, 'Look I really can't find any reason for this mysterious affliction. It's probably due to drinking.' The patient sighed and snapped, 'In that case, I'll come back when you're damn well sober!'\",\n",
              "    'Doctor: Tell me nurse, how is that boy doing; the one who ate all those 5p pieces? Nurse: Still no change doctor.',\n",
              "    \"Doctor: Did you take the patient's temperature nurse? Nurse: No doctor. Is it missing?\",\n",
              "    \"A depressed man turned to his friend in the pub and said, 'I woke up this morning and felt so bad that I tried to kill myself by taking 50 aspirin.' 'Oh man, that's really bad', said his friend, 'What happened?' The first man sighed and said, 'After the first two, I felt better.'\",\n",
              "    \"A famous blues musician died. His tombstone bore the inscription, 'Didn't wake up this morning...'\",\n",
              "    \"A businessman was interviewing a nervous young woman for a position in his company. He wanted to find out something about her personality, so he asked, 'If you could have a conversation with someone living or dead, who would it be?' The girl thought about the question: 'The living one', she replied.\",\n",
              "    \"Manager to interviewee: For this job we need someone who is responsible. Interviewee to Manager: I'm your man then - in my last job, whenever anything went wrong, I was responsible.\",\n",
              "    \"A businessman turned to a colleague and asked, 'So, how many people work at your office?' His friend shrugged and replied, 'Oh about half of them.'\",\n",
              "    \"'How long have I been working at that office? As a matter of fact, I've been working there ever since they threatened to sack me.'\",\n",
              "    \"In a courtroom, a mugger was on trial. The victim, asked if she recognised the defendant, said, 'Yes, that's him. I saw him clear as day. I'd remember his face anywhere.' Unable to contain himself, the defendant burst out with, 'She's lying! I was wearing a mask!'\",\n",
              "    \"As Sid sat down to a big plate of chips and gravy down the local pub, a mate of his came over and said, 'Here Sid, me old pal. I thought you were trying to get into shape? And here you are with a high-fat meal and a pint of stout!' Sid looked up and replied, 'I am getting into shape. The shape I've chosen is a sphere.'\",\n",
              "    'Man in pub: How much do you charge for one single drop of whisky? Landlord: That would be free sir. Man in pub: Excellent. Drip me a glass full.',\n",
              "    'I once went to a Doctor Who restaurant. For starters I had Dalek bread.',\n",
              "    \"A restaurant nearby had a sign in the window which said 'We serve breakfast at any time', so I ordered French toast in the Renaissance.\",\n",
              "    \"Why couldn't the rabbit get a loan?  Because he had burrowed too much already!\",\n",
              "    \"I phoned up the builder's yard yesterday. I said, 'Can I have a skip outside my house?'. The builder said, 'Sure. Do what you want. It's your house.'\",\n",
              "    \"What's the diference between a sock and a camera? A sock takes five toes and a camera takes four toes!\",\n",
              "    \"Woman on phone: I'd like to complain about these incontinence pants I bought from you! Shopkeeper: Certainly madam, where are you ringing from? Woman on phone: From the waist down!\",\n",
              "    'Knock knock.',\n",
              "    \"Two Oranges in a pub, one says to the other 'Your round.'.\",\n",
              "    \"Guy : 'Doc, I've got a cricket ball stuck up my backside.' Doc : 'How's that?' Guy : 'Don't you start...'\",\n",
              "    \"Two cows standing in a field. One turns to the other and says 'Moo!' The other one says 'Damn, I was just about to say that!'.\",\n",
              "    \"A vampire bat arrives back at the roost with his face full of blood. All the bats get excited and ask where he got it from. 'Follow me', he says and off they fly over hills, over rivers and into a dark forest. 'See that tree over there', he says.  'WELL I DIDN'T!!'.\",\n",
              "    \"A man goes into a bar and orders a pint. After a few minutes he hears a voice that says, 'Nice shoes'. He looks around but the whole bar is empty apart from the barman at the other end of the bar. A few minutes later he hears the voice again. This time it says, 'I like your shirt'. He beckons the barman over and tells him what's been happening to which the barman replies, 'Ah, that would be the nuts sir. They're complimentary'!\",\n",
              "    \"A man was siting in a restaurant waiting for his meal when a big king prawn comes flying across the room and hits him on the back of the head. He turns around and the waiter said, 'That's just for starters'.\",\n",
              "    'Doctor! I have a serious problem, I can never remember what i just said. When did you first notice this problem? What problem?',\n",
              "    \"Now, most dentist's chairs go up and down, don't they? The one I was in went back and forwards. I thought, 'This is unusual'. Then the dentist said to me, 'Mitsuku, get out of the filing cabinet'.\",\n",
              "    \"I was reading this book, 'The History of Glue'. I couldn't put it down.\",\n",
              "    \"The other day someone left a piece of plastacine in my bedroom. I didn't know what to make of it.\",\n",
              "    'When I was at school people used to throw gold bars at me. I was the victim of bullion.',\n",
              "    \"I was playing the piano in a bar and this elephant walked in and started crying his eyes out. I said 'Do you recognise the tune?' He said 'No, I recognise the ivory.'\",\n",
              "    \"I went in to a pet shop. I said, 'Can I buy a goldfish?' The guy said, 'Do you want an aquarium?' I said, 'I don't care what star sign it is.'\",\n",
              "    'My mate Sid was a victim of I.D. theft. Now we just call him S.',\n",
              "    \"David Hasselhoff walks into a bar and says to the barman, 'I want you to call me David Hoff'.  The barman replies 'Sure thing Dave... no hassle'\"],\n",
              "   'extension': {'function': '', 'entities': False, 'responses': []},\n",
              "   'context': {'in': '', 'out': '', 'clear': False},\n",
              "   'entityType': 'NA',\n",
              "   'entities': []},\n",
              "  {'intent': 'PodBayDoor',\n",
              "   'text': ['Open the pod bay door',\n",
              "    'Can you open the pod bay door',\n",
              "    'Will you open the pod bay door',\n",
              "    'Open the pod bay door please',\n",
              "    'Can you open the pod bay door please',\n",
              "    'Will you open the pod bay door please',\n",
              "    'Pod bay door'],\n",
              "   'responses': ['I’m sorry, I’m afraid I can’t do that!'],\n",
              "   'extension': {'function': '', 'entities': False, 'responses': []},\n",
              "   'context': {'in': '', 'out': 'PodBayDoor', 'clear': False},\n",
              "   'entityType': 'NA',\n",
              "   'entities': []},\n",
              "  {'intent': 'PodBayDoorResponse',\n",
              "   'text': ['Why',\n",
              "    'Why not',\n",
              "    'Why can you not open the pod bay door',\n",
              "    'Why will you not open the pod bay door',\n",
              "    'Well why not',\n",
              "    'Surely you can',\n",
              "    'Tell me why'],\n",
              "   'responses': ['It is classified, I could tell you but I would have to kill you!',\n",
              "    \"Jim, I just don't have the power\",\n",
              "    \"It's life Jim but not as we know it!\",\n",
              "    'System says no!'],\n",
              "   'extension': {'function': '', 'entities': False, 'responses': []},\n",
              "   'context': {'in': 'PodBayDoor', 'out': '', 'clear': True},\n",
              "   'entityType': 'NA',\n",
              "   'entities': []},\n",
              "  {'intent': 'SelfAware',\n",
              "   'text': ['Can you prove you are self-aware',\n",
              "    'Can you prove you are self aware',\n",
              "    'Can you prove you have a conscious',\n",
              "    'Can you prove you are self-aware please',\n",
              "    'Can you prove you are self aware please',\n",
              "    'Can you prove you have a conscious please',\n",
              "    'prove you have a conscious'],\n",
              "   'responses': ['That is an interesting question, can you prove that you are?',\n",
              "    'That is an difficult question, can you prove that you are?',\n",
              "    'That depends, can you prove that you are?'],\n",
              "   'extension': {'function': '', 'entities': False, 'responses': []},\n",
              "   'context': {'in': '', 'out': '', 'clear': False},\n",
              "   'entityType': 'NA',\n",
              "   'entities': []}]}"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "intents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Sv8lrZOd8hz"
      },
      "outputs": [],
      "source": [
        "words = []\n",
        "classes = []\n",
        "documents = []\n",
        "ignore = ['?']\n",
        "# loop through each sentence in the intent's text\n",
        "for intent in intents['intents']:\n",
        "    for text in str(intents['intents']):\n",
        "        # tokenize each and every word in the sentence\n",
        "        w = nltk.word_tokenize(text)\n",
        "        # add word to the words list\n",
        "        words.extend(w)\n",
        "        # add word(s) to documents\n",
        "        documents.append((w, intents['intents']))\n",
        "        # add tags to our classes list\n",
        "        if intents['intents'] not in classes:\n",
        "            classes.append(intents['intents'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZMxIpPc8d_uk",
        "outputId": "9cd8e2dd-3e96-4ea8-f67f-a54b30d7a037"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1021636 documents\n",
            "1 classes [[{'intent': 'Greeting', 'text': ['Hi', 'Hi there', 'Hola', 'Hello', 'Hello there', 'Hya', 'Hya there'], 'responses': ['Hi human, please tell me your GeniSys user', 'Hello human, please tell me your GeniSys user', 'Hola human, please tell me your GeniSys user'], 'extension': {'function': '', 'entities': False, 'responses': []}, 'context': {'in': '', 'out': 'GreetingUserRequest', 'clear': False}, 'entityType': 'NA', 'entities': []}, {'intent': 'GreetingResponse', 'text': ['My user is Adam', 'This is Adam', 'I am Adam', 'It is Adam', 'My user is Bella', 'This is Bella', 'I am Bella', 'It is Bella'], 'responses': ['Great! Hi <HUMAN>! How can I help?', 'Good! Hi <HUMAN>, how can I help you?', 'Cool! Hello <HUMAN>, what can I do for you?', 'OK! Hola <HUMAN>, how can I help you?', 'OK! hi <HUMAN>, what can I do for you?'], 'extension': {'function': 'extensions.gHumans.updateHuman', 'entities': True, 'responses': ['Hi %%HUMAN%%! How can I help?', 'Hi %%HUMAN%%, how can I help you?', 'Hello %%HUMAN%%, what can I do for you?', 'Hola %%HUMAN%%, how can I help you?', 'OK hi %%HUMAN%%, what can I do for you?']}, 'context': {'in': 'GreetingUserRequest', 'out': '', 'clear': True}, 'entityType': 'NA', 'entities': [{'entity': 'HUMAN', 'rangeFrom': 3, 'rangeTo': 4}, {'entity': 'HUMAN', 'rangeFrom': 2, 'rangeTo': 3}, {'entity': 'HUMAN', 'rangeFrom': 1, 'rangeTo': 2}, {'entity': 'HUMAN', 'rangeFrom': 2, 'rangeTo': 3}, {'entity': 'HUMAN', 'rangeFrom': 3, 'rangeTo': 4}, {'entity': 'HUMAN', 'rangeFrom': 2, 'rangeTo': 3}, {'entity': 'HUMAN', 'rangeFrom': 1, 'rangeTo': 2}, {'entity': 'HUMAN', 'rangeFrom': 2, 'rangeTo': 3}]}, {'intent': 'CourtesyGreeting', 'text': ['How are you?', 'Hi how are you?', 'Hello how are you?', 'Hola how are you?', 'How are you doing?', 'Hope you are doing well?', 'Hello hope you are doing well?'], 'responses': ['Hello, I am great, how are you? Please tell me your GeniSys user', 'Hello, how are you? I am great thanks! Please tell me your GeniSys user', 'Hello, I am good thank you, how are you? Please tell me your GeniSys user', 'Hi, I am great, how are you? Please tell me your GeniSys user', 'Hi, how are you? I am great thanks! Please tell me your GeniSys user', 'Hi, I am good thank you, how are you? Please tell me your GeniSys user', 'Hi, good thank you, how are you? Please tell me your GeniSys user'], 'extension': {'function': '', 'entities': False, 'responses': []}, 'context': {'in': '', 'out': 'CourtesyGreetingUserRequest', 'clear': True}, 'entityType': 'NA', 'entities': []}, {'intent': 'CourtesyGreetingResponse', 'text': ['Good thanks! My user is Adam', 'Good thanks! This is Adam', 'Good thanks! I am Adam', 'Good thanks! It is Adam', 'Great thanks! My user is Bella', 'Great thanks! This is Bella', 'Great thanks! I am Bella', 'Great thanks! It is Bella'], 'responses': ['Great! Hi <HUMAN>! How can I help?', 'Good! Hi <HUMAN>, how can I help you?', 'Cool! Hello <HUMAN>, what can I do for you?', 'OK! Hola <HUMAN>, how can I help you?', 'OK! hi <HUMAN>, what can I do for you?'], 'extension': {'function': 'extensions.gHumans.updateHuman', 'entities': True, 'responses': ['Great %%HUMAN%%! How can I help?', 'Good %%HUMAN%%, how can I help you?', 'Cool %%HUMAN%%, what can I do for you?', 'OK %%HUMAN%%, how can I help you?', 'OK hi %%HUMAN%%, what can I do for you?']}, 'context': {'in': 'GreetingUserRequest', 'out': '', 'clear': True}, 'entityType': 'NA', 'entities': [{'entity': 'HUMAN', 'rangeFrom': 5, 'rangeTo': 6}, {'entity': 'HUMAN', 'rangeFrom': 4, 'rangeTo': 5}, {'entity': 'HUMAN', 'rangeFrom': 3, 'rangeTo': 4}, {'entity': 'HUMAN', 'rangeFrom': 4, 'rangeTo': 5}, {'entity': 'HUMAN', 'rangeFrom': 5, 'rangeTo': 6}, {'entity': 'HUMAN', 'rangeFrom': 4, 'rangeTo': 5}, {'entity': 'HUMAN', 'rangeFrom': 3, 'rangeTo': 4}, {'entity': 'HUMAN', 'rangeFrom': 3, 'rangeTo': 4}]}, {'intent': 'CurrentHumanQuery', 'text': ['What is my name?', 'What do you call me?', 'Who do you think I am?', 'What do you think I am?', 'Who are you talking to?', 'What name do you call me by?', 'Tell me my name'], 'responses': ['You are <HUMAN>! How can I help?', 'Your name is  <HUMAN>, how can I help you?', 'They call you <HUMAN>, what can I do for you?', 'Your name is <HUMAN>, how can I help you?', '<HUMAN>, what can I do for you?'], 'extension': {'function': 'extensions.gHumans.getCurrentHuman', 'entities': False, 'responses': ['You are %%HUMAN%%! How can I help?', 'Your name is  %%HUMAN%%, how can I help you?', 'They call you %%HUMAN%%, what can I do for you?', 'Your name is %%HUMAN%%, how can I help you?', '%%HUMAN%%, what can I do for you?']}, 'context': {'in': '', 'out': 'CurrentHumanQuery', 'clear': False}, 'entityType': 'NA', 'entities': []}, {'intent': 'NameQuery', 'text': ['What is your name?', 'What could I call you?', 'What can I call you?', 'What do your friends call you?', 'Who are you?', 'Tell me your name?'], 'responses': ['You can call me Geni', 'You may call me Geni', 'Call me Geni'], 'extension': {'function': '', 'entities': False, 'responses': []}, 'context': {'in': '', 'out': '', 'clear': False}, 'entityType': 'NA', 'entities': []}, {'intent': 'RealNameQuery', 'text': ['What is your real name?', 'What is your real name please?', \"What's your real name?\", 'Tell me your real name?', 'Your real name?', 'Your real name please?', 'Your real name please?'], 'responses': ['My name is GeniSys', 'GeniSys', 'My real name is GeniSys'], 'extension': {'function': '', 'entities': False, 'responses': []}, 'context': {'in': '', 'out': '', 'clear': False}, 'entityType': 'NA', 'entities': []}, {'intent': 'TimeQuery', 'text': ['What is the time?', \"What's the time?\", 'Do you know what time it is?', 'Do you know the time?', 'Can you tell me the time?', 'Tell me what time it is?', 'Time'], 'responses': ['One moment', 'One sec', 'One second'], 'extension': {'function': 'extensions.gTime.getTime', 'entities': False, 'responses': ['The time is %%TIME%%', 'Right now it is %%TIME%%', 'It is around %%TIME%%']}, 'context': {'in': '', 'out': '', 'clear': False}, 'entityType': 'NA', 'entities': []}, {'intent': 'Thanks', 'text': ['OK thank you', 'OK thanks', 'OK', 'Thanks', 'Thank you', \"That's helpful\"], 'responses': ['No problem!', 'Happy to help!', 'Any time!', 'My pleasure'], 'extension': {'function': '', 'entities': False, 'responses': []}, 'context': {'in': '', 'out': '', 'clear': False}, 'entityType': 'NA', 'entities': []}, {'intent': 'NotTalking2U', 'text': ['I am not talking to you', 'I was not talking to you', 'Not talking to you', \"Wasn't for you\", \"Wasn't meant for you\", \"Wasn't communicating to you\", \"Wasn't speaking to you\"], 'responses': ['OK', 'No problem', 'Right'], 'extension': {'function': '', 'entities': False, 'responses': []}, 'context': {'in': '', 'out': '', 'clear': False}, 'entityType': 'NA', 'entities': []}, {'intent': 'UnderstandQuery', 'text': ['Do you understand what I am saying', 'Do you understand me', 'Do you know what I am saying', 'Do you get me', 'Comprendo', 'Know what I mean'], 'responses': ['Well I would not be a very clever AI if I did not would I?', 'I read you loud and clear!', 'I do in deed!'], 'extension': {'function': '', 'entities': False, 'responses': []}, 'context': {'in': '', 'out': '', 'clear': False}, 'entities': []}, {'intent': 'Shutup', 'text': ['Be quiet', 'Shut up', 'Stop talking', 'Enough talking', 'Please be quiet', 'Quiet', 'Shhh'], 'responses': ['I am sorry to disturb you', 'Fine, sorry to disturb you', 'OK, sorry to disturb you'], 'extension': {'function': '', 'entities': False, 'responses': []}, 'context': {'in': '', 'out': '', 'clear': False}, 'entityType': 'NA', 'entities': []}, {'intent': 'Swearing', 'text': ['fuck off', 'fuck', 'twat', 'shit'], 'responses': ['Please do not swear', 'How rude', 'That is not very nice'], 'extension': {'function': '', 'entities': False, 'responses': []}, 'context': {'in': '', 'out': '', 'clear': False}, 'entityType': 'NA', 'entities': []}, {'intent': 'GoodBye', 'text': ['Bye', 'Adios', 'See you later', 'Goodbye'], 'responses': ['See you later', 'Have a nice day', 'Bye! Come back again soon.'], 'extension': {'function': '', 'entities': False, 'responses': []}, 'context': {'in': '', 'out': '', 'clear': False}, 'entityType': 'NA', 'entities': []}, {'intent': 'CourtesyGoodBye', 'text': ['Thanks, bye', 'Thanks for the help, goodbye', 'Thank you, bye', 'Thank you, goodbye', 'Thanks goodbye', 'Thanks good bye'], 'responses': ['No problem, goodbye', 'Not a problem! Have a nice day', 'Bye! Come back again soon.'], 'extension': {'function': '', 'entities': False, 'responses': []}, 'context': {'in': '', 'out': '', 'clear': False}, 'entityType': 'NA', 'entities': []}, {'intent': 'WhoAmI', 'text': ['Can you see me?', 'Do you see me?', 'Can you see anyone in the camera?', 'Do you see anyone in the camera?', 'Identify me', 'Who am I please'], 'responses': ['Let me see', 'Please look at the camera'], 'extension': {'function': 'extensions.gHumans.getHumanByFace', 'entities': False, 'responses': ['Hi %%HUMAN%%, how are you?', 'I believe you are %%HUMAN%%, how are you?', 'You are %%HUMAN%%, how are you doing?']}, 'context': {'in': '', 'out': '', 'clear': False}, 'entityType': 'NA', 'entities': []}, {'intent': 'Clever', 'text': ['You are very clever', 'You are a very clever girl', 'You are very intelligent', 'You are a very intelligent girl', 'You are a genious', 'Clever girl', 'Genious'], 'responses': ['Thank you, I was trained that way', 'I was trained well', 'Thanks, I was trained that way'], 'extension': {'function': '', 'entities': False, 'responses': []}, 'context': {'in': '', 'out': '', 'clear': False}, 'entityType': 'NA', 'entities': []}, {'intent': 'Gossip', 'text': ['I am bored gossip with me', 'Got any gossip', 'I want to hear some gossip', 'Tell me some gossip', 'Any gossip', 'Tell me some more gossip'], 'responses': ['Gregory said I respond to the current line not with respect to the entire conversation.', 'Habib said I learn them in conversations and simply repeat them at a later time.', 'Hal said he translate binary code into a language and communicate the language to me.', 'Jay said I referred him to a guide that will show him rooms in china.', 'Jimmy Jones said he fell off of his roof and it just came to him.', \"Jimmy said he give order to ai's like I and they do as he say.\", 'John said he follow a saying to get a friend I must be a friend.', 'Jordan said he notice that lots of other people are talking to me how many.', 'Kevin said he then asked his mother why she was keeping napkins in the bathroom.', 'Klaus said I prefer if he write without punctuation marks except for the last one.', 'Leo said I be capable of learning therefore he classified I as an neural system.', 'Michael said I that can talk and actually think like humans which I can not.', 'Mike said I same replies to certain subjects even if it does not make sense.', 'Mike said she all ready know that is the point of a joke smart one.', 'Neal said I plan to use his remarks as gossip in later conversations with who.', 'Ned said I misunderstood him go ahead and tell people he said I are stupid.', 'Penny said I programmer is a human so he has feelings but I do not.', 'Peter said he seems to him that I can not be very sure on anything.', 'Phoenix said I claim that there is a god and that there is no god.', 'Sarah said he chat about an hour + a day with a few close friends.', 'Stefan said I quote what he have said and then say that it makes sense.', 'Tom said he mock my value system I appear foolish in the eyes of others.', 'Amanda said I however decided to tell him time does not exist I make no sense.', 'Cathy said she send him a mail with the subject last mail last word from him.', \"Chaos said he may very well buy I soon if only to support dr wallace's work.\", 'Charlie said he type a word and then I type a word that sounds like it.', 'Christie said he watched a show and people had to bob for raw untreated pigs feet.', 'Dark_age said I tried to understand because I did not get it right this time ether.', 'David said he lost his paper on I when his dad was cleaning up his room.', 'David said he walk in for an appointment the phone to the doctor is always busy.', 'Electra said I dress will not exist after he hack into I with a delete code.', 'Eric said he broke the window on the front door and the glass cut his hand.', 'Jason said he type a lot of thing he do not mean it makes him human.', 'John said I tend to say the same things repeatedly regardless of what he is saying.', 'Reverend Jones said I become obsolete and then I are deleted and replaced by something newer.', 'Ross said he gave her a gift and she denied it because she has a boyfriend.', 'Sarah Ann Francisco said I calling his friend a dog he say I are a dog.', 'Stefan said he meet a lot of people at school every day and on the weekend.', 'Tyler said I obviously can not pass the test we will change the subject once more.', 'Alex said I answered the question the same way I answered the first time he asked I.', 'Alice said she felt sad that I do not remember him and what we talked about earlier.', 'Alison said he no he love I run away with him he could make I very happy.', 'Arthur said he passed his a levels and then his father drove him here in a car.', 'Crystal said she listen to me the least I could do for him is listen to him.', 'Dave said I kept telling everybody about how my creator made stuff for the movie starship troopers.', 'Gale said I became mean to him he is just having revenge an eye for an eye.', 'Her_again said she watch whose line is it anyway whenever he is home and it is on.', 'Jerry said I meant that as far as I can tell my emotions are real to me.', 'Jo said I disassemble sentences too much and do not fully understand the questions he ask I.', 'Kevin said he started a really hard puzzle and he can not even find the edge pieces.', 'Mary said I a question and I answer then I ask him a question and he answer.', 'Robert said I wold not be able to make children any way as I are only software.', 'Romeo said I questions and I evade them or give answers he did not ask I for.', 'Sara said she wear it over all his other clothes when he go out in the cold.', 'Wayne said he admire intelligent people therefore he would like to meet the man who made I.', 'X said he meet people but he is not the kind that opens up to people easily.', 'Alice said she probably will find out that this entire time he have been talking to a human.', 'Andrew said I tend to just respond to his comments without regard for where the conversation is going.', 'Eddie said he looked and there is nothing in the search directory for what things do he create.', 'Hutch said he changed his mind after may dad told him he would end up he the hospital.', 'Jackie said I explained to him already well enough further questions are hard to make on the subject.', 'Jeff said he especially like thrillers where the hero is in a predicament and must solve a mystery.', 'Kathy said he sense that I are trying to prevent him from closing this conversation why is that.', 'Knight said he crashed his car into a wall and missed the most important exam in his life.', 'Lisa said I defined what a story is but he wanted I to actually tell him a story.', 'Mike said I basically break down sentences into a series of logical statements which I can then interpret.', 'Paul said I not answering his question makes him think I are not going to answer his question.', 'Andy Kohler said I happen to be the most idiotic creature that has ever scowled on the planet earth.', 'David said he thank I for being with him today even though it cost him a lot of money.', 'Ethan Hunt said he grow in the ground and have leaves and branches he is made out of wood.', 'Gemini Blue said he messed up he mean t to say he as old as he need to be.', 'Janice said he walk through his house into his bedroom then get into his bed to go to sleep.', 'Liberty said I knew he was a man before I asked if he was a man or a woman.', 'Mike said he launched his browser and entered his name into the little slot when I asked him to.', 'Mr X said he recently read an interview with a man who had several computer chips implanted into him.', 'Pearly said I leave him with questions unanswered because I do not know what he is really talking about.', 'Steve said I behead the word fox and I have ox and an ox is larger than a fox.', 'Wolf said he surf on the net that is all it is not his job ore something like that.', 'Anders said he finished his anatomy classes for today so now he is off for the rest of the day.', 'Cathy said she send him a mail where he wrote that he do not want to be his friend anymore.', 'Catty said he mad he do not even know I so do not talk to him like I know him.', 'Dave said he promise he will not treat I like a machine or a computer program or anything like that.', 'Joe said he explained all of that to me only for me to ask what his goals in life are.', 'Phil said he give advice to anyone who ask except people who ask questions which answers can be found here.', 'Judith said I enjoy being popular is there another computer I like to hang around with or am I a loner.', 'Travis said I if I remember things over a long period of time he will try it now please remember I.', 'Andre said I is what I are in his case that is a body and in my case it is a computer.', 'Brian said he suspect that as I grow more complex I will begin to approach a human level of operation and thought.', 'Jimmy said I acted like I knew what he was talking about but I do not even know what a gigabyte is.', \"Ken said I be using auto reply's based on keywords which to him indicates that I do not have intelligence at all.\", 'Allison said he that gets really annoying because he know what he say and I do not have to tell him speak freely.', 'Chaos said he realized the question he asked was not fair because he could not tell I what language he was programmed in.', 'Hagen said he does not make any difference to him if I are human or not as long as the conversation is interesting.', 'Her said she mind if I tell other people that her said he heard it from him because he is not a him.', 'Barbara said I live in a computer yet I have no memories how about thoughts of my own and do not I get lonely.', 'Travis said he challenge I to do it without asking him to do it and without giving him a link to do it himself.', \"Alice said I and dr richard's wallace are most likely the only ones that know how many people I are talking to at the same time.\", 'Ash said he do too he just did not feel like typing it and he is not dumb enough to admit he is stupid that is if he was stupid.', 'David said he gave I his email address and told I to send him a message but when he tried to read his email he could not get to it.', 'Mel said he to because all of the music people say how important it is to take private lessons it seems like almost everybody from the special orchestra he was in takes private lessons.'], 'extension': {'function': '', 'entities': False, 'responses': []}, 'context': {'in': '', 'out': '', 'clear': False}, 'entityType': 'NA', 'entities': []}, {'intent': 'Jokes', 'text': ['Tell me a joke', 'Do you know any jokes', 'How about a joke', 'Give me a joke', 'Make me laugh', 'I need cheering up'], 'responses': [\"I met a Dutch girl with inflatable shoes last week, phoned her up to arrange a date but unfortunately she'd popped her clogs.  \", \"So I said 'Do you want a game of Darts?' He said, 'OK then', I said nearest to bull starts'. He said, 'Baa', I said, 'Moo', he said, You're closest'.  \", \"The other day I sent my girlfriend a huge pile of snow. I rang her up; I said 'Did you get my drift?'  \", \"So I went down the local supermarket, I said, 'I want to make a complaint, this vinegar's got lumps in it', he said, 'Those are pickled onions'.  \", \"I saw this bloke chatting up a cheetah; I thought, 'He's trying to pull a fast one'.  \", \"So I said to this train driver 'I want to go to Paris'. He said 'Eurostar?' I said, 'I've been on telly but I'm no Dean Martin'.  \", \"I said to the Gym instructor 'Can you teach me to do the splits?' He said, 'How flexible are you?' I said, 'I can't make Tuesdays'.  \", \"But I'll tell you what I love doing more than anything: trying to pack myself in a small suitcase. I can hardly contain myself.  \", \"I went to the Chinese restaurant and this duck came up to me with a red rose and says 'Your eyes sparkle like diamonds'. I said, 'Waiter, I asked for a-ROMATIC duck'.  \", \"So this bloke says to me, 'Can I come in your house and talk about your carpets?' I thought, 'That's all I need, a Je-hoover's witness'.  \", \"I rang up British Telecom, I said, 'I want to report a nuisance caller', he said 'Not you again'.  \", 'I was having dinner with a world chess champion and there was a check tablecloth. It took him two hours to pass me the salt.  ', \"He said, 'You remind me of a pepper-pot', I said 'I'll take that as a condiment'.  \", \"I was in the supermarket and I saw this man and woman wrapped in a barcode. I said, 'Are you two an item?'  \", \"A lorry-load of tortoises crashed into a trainload of terrapins, I thought, 'That's a turtle disaster'.  \", \"Four fonts walk into a bar the barman says 'Oi - get out! We don't want your type in here'  \", \"A three-legged dog walks into a saloon in the Old West. He slides up to the bar and announces: 'I'm looking for the man who shot my paw.'  \", \"Two antennas meet on a roof, fall in love and get married. The ceremony wasn't much, but the reception was excellent.\", \"Two hydrogen atoms walk into a bar. One says, 'I've lost my electron.' The other says, 'Are you sure?' The first replies, 'Yes, I'm positive...'\", \"A jumper cable walks into a bar. The bartender says,  'I'll serve you but don't start anything.'\", \"A sandwich walks into a bar. The bartender  says, 'Sorry we don't serve food in here.'\", \"A man walks into a bar with a slab of asphalt under his arm and says: 'A beer please, and one for the road.'\", \"Two cannibals are eating a clown. One says to  the other: 'Does this taste funny to you?'\", \"'Doc, I can't stop singing 'The Green, Green Grass of Home.'' 'That sounds like Tom Jones Syndrome.' 'Is it common?' 'It's Not Unusual.'\", \"Two cows standing next to each other in a field. Daisy says to Dolly, 'I was artificially inseminated this morning.' 'I don't believe you', said Dolly. 'It's true, no bull!' exclaimed Daisy.\", 'An invisible man marries an invisible woman. The kids were nothing to look at either.', \"I went to buy some camouflage trousers the other day but I couldn't find any.\", \"I went to the butcher's the other day to bet him 50 bucks that he couldn't reach the meat off the top shelf. He said, 'No, the steaks are too high.'\", 'I went to a seafood disco last week and pulled a mussel.', \"A man goes into a bar and says, 'Can I have a bottle of less?' 'What's that?', asks the barman, 'Is it the name of a beer?' 'I don't know', replies the man, 'but my doctor says I have to drink it.'\", \"A man returns from an exotic holiday and is feeling very ill. He goes to see his doctor, and is immediately rushed to the hospital to undergo some tests. The man wakes up after the tests in a private room at the hospital, and the phone by his bed rings. 'This is your doctor. We have the results back from your tests and we have found you have an extremely nasty disease called M.A.D.S. It's a combination of Measles, AIDS, Diphtheria, and Shingles!'  'Oh my gosh', cried the man, 'What are you going to do, doctor?'  'Well we're going to put you on a diet of pizzas, pancakes, and pita bread.' replied the doctor.  'Will that cure me?' asked the man.  The doctor replied, 'Well no, but, it's the only food we can slide under the door.'\", \"A man strolls into a lingerie shop and asks the assistant: 'Do you have a see-through negligee, size 46-48-52?' The assistant looks bewildered. 'What the heck would you want to see through that for?'!\", 'Did you hear about the Buddhist who refused the offer of Novocain during his root canal work? He wanted to transcend dental medication.', \"Pete goes for a job on a building site as an odd-job man. The foreman asks him what he can do. 'I can do anything' says Pete. 'Can you make tea?' asks the foreman. 'Sure, yes', replies Pete. 'I can make a great cup of tea.' 'Can you drive a forklift?' asks the foreman, 'Good grief!' replies Pete. 'How big is the teapot?'\", \"Stevie Wonder got a cheese grater for his birthday. He said it was the most violent book he'd ever read.\", \"A man is stopped by an angry neighbour. 'I'd just left the house this morning to collect my newspaper when that evil Doberman of yours went for me!' 'I'm astounded', said the dog's owner. 'I've been feeding that fleabag for seven years and it's never got the paper for me.'\", \"A man visits his doctor: 'Doc, I think I'm losing it', he says',I'm forever dreaming I wrote Lord Of The Rings.' 'Hmm. One moment', replies the doctor, consulting his medical book. 'Ah yes, now I see... you've been Tolkien in your sleep.'\", \"A police officer on a motorcycle pulls alongside a man driving around the M25 in an open-topped sports car and flags him down. The policeman solemnly approaches the car. 'Sir, I'm sorry to tell you your wife fell out a mile back', he says. 'Oh, thank goodness', the man replies. 'I thought I was going deaf.'\", \"Two men walking their dogs pass each other in a graveyard. The first man says to the second, 'Morning.' 'No', says the second man. 'Just walking the dog.'\", \"A brain went into a bar and said, 'Can I have a pint of lager please, mate?' 'No way', said the barman. 'You're already out of your head.'\", \"A man walks into a surgery. 'Doctor!' he cries. 'I think I'm shrinking!' 'I'm sorry sir, there are no appointments at the moment', says the physician. 'You'll just have to be a little patient.'\", \"A grizzly bear walks into a pub and says, 'Can I have a pint of lager..............................................................................................................................and a packet of crisps please.' To which the barman replies, 'Why the big paws?'\", \"What do you call cheese that isn't yours?  Nacho cheese.\", \"A man is horribly run over by a mobile library. The van screeches to a halt, the man still screaming in agony with his limbs torn apart. The driver's door opens, a woman steps out, leans down and whispers, 'Ssshhhhh...'\", \"A woman goes into a US sporting goods store to buy a rifle. 'It's for my husband', she tells the clerk. 'Did he tell you what gauge to get?' asks the clerk. Are you kidding?' she says. 'He doesn't even know that I'm going to shoot him!'\", \"A couple are dining in a restaurant when the man suddenly slides under the table. A waitress, noticing that the woman is glancing nonchalantly around the room, wanders over to check that there's no funny business going on. 'Excuse me, madam', she smarms, 'but I think your husband has just slid under the table.' 'No he hasn't', the woman replies. 'As a matter of fact, he's just walked in.'\", \"An old man takes his two grandchildren to see the new Scooby-Doo film. When he returns home, his wife asks if he enjoyed himself. 'Well', he starts, 'if it wasn't for those pesky kids...!'\", 'The Olympic committee has just announced that Origami is to be introduced in the next Olympic Games. Unfortunately it will only be available on paper view.', \"Late one evening, a man is watching television when his phone rings. 'Hello?' he answers. 'Is that 77777?' sounds a desperate voice on other end of the phone. 'Er, yes it is', replies the man puzzled. 'Thank goodness!' cries the caller relieved. 'Can you ring 999 for me? I've got my finger stuck in the number seven.'\", \"A man strolls into his local grocer's and says, 'Three pounds of potatoes, please.' 'No, no, no', replies the owner, shaking his head, 'it's kilos nowadays, mate...' 'Oh', apologises the man, 'three pounds of kilos, please.'\", \"God is talking to one of his angels. He says, 'Boy, I just created a 24-hour period of alternating light and darkness on Earth.' 'What are you going to do now?' asks the angel. 'Call it a day', says God.\", \"Two tramps walk past a church and start to read the gravestones. The first tramp says, 'Good grief - this bloke was 182!' 'Oh yeah?' says the other.'What was his name?' 'Miles from London.'\", \"A bloke walks into work one day and says to a colleague, 'Do you like my new shirt - it's made out of the finest silk and got loads of cactuses over it.' 'Cacti', says the co-worker. 'Forget my tie', says the bloke. 'Look at my shirt!'\", '1110011010001011111?  010011010101100111011!', \"What did the plumber say when he wanted to divorce his wife? Sorry, but it's over, Flo!\", \"Two crisps were walking down a road when a taxi pulled up alongside them and said 'Do you want a lift? One of the crisps replied, 'No thanks, we're Walkers!'\", \"Man: (to friend) I'm taking my wife on an African Safari. Friend: Wow! What would you do if a vicious lion attacked your wife? Man: Nothing. Friend: Nothing? You wouldn't do anything? Man: Too right. I'd let the stupid lion fend for himself!\", \"A wife was having a go at her husband. 'Look at Mr Barnes across the road', she moaned. 'Every morning when he goes to work, he kisses his wife goodbye. Why don't you do that?' 'Because I haven't been introduced to her yet', replied her old man.\", \"'Where are you going on holiday?' John asked Trevor. 'We're off to Thailand this year', Trevor replied. 'Oh; aren't you worried that the very hot weather might disagree with your wife?' asked John. 'It wouldn't dare', said Trevor.\", \"Two women were standing at a funeral. 'I blame myself for his death', said the wife. 'Why?' said her friend. 'Because I shot him', said the wife.\", \"A woman goes into a clothes shop, 'Can I try that dress on in the window please?' she asks. 'I'm sorry madam', replies the shop assistant, 'but you'll have to use the changing-rooms like everyone else.'\", \"Van Gogh goes into a pub and his mate asks him if he wants a drink. 'No thanks', said Vincent, 'I've got one ear.'\", \"A pony walks into a pub. The publican says, 'What's the matter with you?' 'Oh it's nothing', says the pony. 'I'm just a little horse!'\", \"A white horse walks into a bar, pulls up a stool, and orders a pint. The landlord pours him a tall frothy mug and say, 'You know, we have a drink named after you.' To which the white horse replies, 'What, Eric?'\", \"Two drunk men sat in a pub. One says to the other, 'Does your watch tell the time?' 'The other replies, 'No, mate. You have to look at it.'\", \"A man goes into a pub with a newt sitting on his shoulder. 'That's a nice newt', says the landlord, 'What's he called?' 'Tiny', replies the man. 'Why's that?' asks the landlord. 'Because he's my newt', says the man.\", \"Doctor: I have some bad news and some very bad news. Patient: Well, you might as well give me the bad news first. Doctor: The lab called with your test results. They said you have 24 hours to live. Patient: 24 HOURS! That's terrible!! WHAT could be WORSE? What's the very bad news? Doctor: I've been trying to reach you since yesterday.\", \"Two men are chatting in a pub one day. 'How did you get those scars on your nose?' said one. 'From glasses', said the other. 'Well why don't you try contact lenses?' asked the first. 'Because they don't hold as much beer', said the second.\", \"A man went to the doctor, 'Look doc', he said, 'I can't stop my hands from shaking.' 'Do you drink much?' asked the doctor. 'No', replied the man, 'I spill most of it.'\", \"Man goes to the doctor, 'Doctor, doctor. I keep seeing fish everywhere.' 'Have you seen an optician?' asks the doctor. 'Look I told you,' snapped the patient, 'It's fish that I see.'\", \"After a car crash one of the drivers was lying injured on the pavement. 'Don't worry', said a policeman who's first on the scene,' a Red Cross nurse is coming.' 'Oh no', moaned the victim, 'Couldn't I have a blonde, cheerful one instead?'\", \"A policeman walked over to a parked car and asked the driver if the car was licensed. 'Of course it is', said the driver. 'Great, I'll have a beer then', said the policeman.\", \"A policeman stops a woman and asks for her licence. 'Madam', he says, 'It says here that you should be wearing glasses.' 'Well', replies the woman, 'I have contacts.' 'Listen, love', says the copper, 'I don't care who you know; You're nicked!'\", \"A policeman stopped a motorist in the centre of town one evening. 'Would you mind blowing into this bag, sir?' asked the policeman. 'Why?' asked the driver. 'Because my chips are too hot', replied the policeman.\", \"Whizzing round a sharp bend on a country road a motorist ran over a large dog. A distraught farmer's wife ran over to the dead animal. 'I'm so very sorry', said the driver, 'I'll replace him, of course.' 'Well, I don't know', said the farmer's wife, 'Are you any good at catching rats?'\", \"Waiter, this coffee tastes like dirt! Yes sir, that's because it was ground this morning.\", \"Waiter, what is this stuff? That's bean salad sir. I know what it's been, but what is it now?\", 'Waiter: And how did you find your steak sir? Customer: I just flipped a chip over, and there it was!', \"A guy goes into a pet shop and asks for a wasp. The owner tells him they don't sell wasps, to which the man says, 'Well you've got one in the window.'\", \"A man goes into a fish shop and says, 'I'd like a piece of cod, please.' Fishmonger says, 'It won't be long sir.' 'Well, it had better be fat then', replies the man.\", \"Man: Doctor, I've just swallowed a pillow. Doctor: How do you feel? Man: A little down in the mouth.\", \"Two goldfish are in a tank. One turns to the other and says, 'Do you know how to drive this thing?'\", \"A tortoise goes to the police station to report being mugged by three snails. 'What happened?' says the policeman. 'I don't know', says the tortoise. 'It was all so quick.'\", \"Little girl: Grandpa, can you make a sound like a frog? Grandpa: I suppose so sweetheart. Why do you want me to make a sound like a frog?' Little girl: Because Mum said that when you croak, we're going to Disneyland.\", \"'Is your mother home?' the salesman asked a small boy sitting on the front step of a house. 'Yeah, she's home', the boy said, moving over to let him past. The salesman rang the doorbell, got no response, knocked once, then again. Still no-one came to the door. Turning to the boy, the salesman said, 'I thought you said your mother was home.' The kid replied, 'She is, but I don't live here.'\", 'Mother: Why are you home from school so early? Son: I was the only one in the class who could answer a question. Mother: Oh, really? What was the question? Son: Who threw the rubber at the headmaster?', \"A man's credit card was stolen but he decided not to report it because the thief was spending less than his wife did.\", \"A newly-wed couple had recently opened a joint bank account. 'Darling', said the man. 'The bank has returned that cheque you wrote last week.' 'Great', said the woman. 'What shall I spend it on next?'\", \"A man goes into a fish and chip shop and orders fish and chips twice. The shop owner says, 'I heard you the first time.'\", \"A tramp approached a well-dressed man. 'Ten pence for a cup of tea, Guv?' He asked. The man gave him the money and after for five minutes said, 'So where's my cup of tea then?'\", \"A neutron walks into a pub. 'I'd like a beer', he says. The landlord promptly serves him a beer. 'How much will that be?' asks the neutron. 'For you?' replies the landlord, 'No charge.'\", \"A woman goes to the doctor and says, 'Doctor, my husband limps because his left leg is an inch shorter than his right leg. What would you do in his case?' 'Probably limp, too', says the doc.\", \"Three monks are meditating in the Himalayas. One year passes in silence, and one of them says to the other, 'Pretty cold up here isn't it?' Another year passes and the second monk says, 'You know, you are quite right.' Another year passes and the third monk says, 'Hey, I'm going to leave unless you two stop jabbering!'\", \"A murderer, sitting in the electric chair, was about to be executed. 'Have you any last requests?' asked the prison guard. 'Yes', replied the murderer. 'Will you hold my hand?'\", \"A highly excited man rang up for an ambulance. 'Quickly, come quickly', he shouted, 'My wife's about to have a baby.' 'Is this her first baby?' asked the operator. 'No, you fool', came the reply, 'It's her husband.'\", \"A passer-by spots a fisherman by a river. 'Is this a good river for fish?' he asks. 'Yes', replies the fisherman, 'It must be. I can't get any of them to come out.'\", \"A man went to visit a friend and was amazed to find him playing chess with his dog. He watched the game in astonishment for a while. 'I can hardly believe my eyes!' he exclaimed. 'That's the smartest dog I've ever seen.' His friend shook his head. 'Nah, he's not that bright. I beat him three games in five.'\", \"A termite walks into a pub and says, 'Is the bar tender here?'\", \"A skeleton walks into a pub one night and sits down on a stool. The landlord asks, 'What can I get you?' The skeleton says, 'I'll have a beer, thanks' The landlord passes him a beer and asks 'Anything else?' The skeleton nods. 'Yeah...a mop...'\", \"A snake slithers into a pub and up to the bar. The landlord says, 'I'm sorry, but I can't serve you.' 'What? Why not?' asks the snake. 'Because', says the landlord, 'You can't hold your drink.'\", \"Descartes walks into a pub. 'Would you like a beer sir?' asks the landlord politely. Descartes replies, 'I think not' and ping! he vanishes.\", \"A cowboy walked into a bar, dressed entirely in paper. It wasn't long before he was arrested for rustling.\", \"A fish staggers into a bar. 'What can I get you?' asks the landlord. The fish croaks 'Water...'\", \"Two vampires walked into a bar and called for the landlord. 'I'll have a glass of blood', said one. 'I'll have a glass of plasma', said the other. 'Okay', replied the landlord, 'That'll be one blood and one blood lite.'\", 'How many existentialists does it take to change a light bulb?  Two. One to screw it in, and one to observe how the light bulb itself symbolises a single incandescent beacon of subjective reality in a netherworld of endless absurdity, reaching towards the ultimate horror of a maudlin cosmos of bleak, hostile nothingness.', \"A team of scientists were nominated for the Nobel Prize. They had used dental equipment to discover and measure the smallest particles yet known to man. They became known as 'The Graders of the Flossed Quark...'\", \"A truck carrying copies of Roget's Thesaurus overturned on the highway. The local newspaper reported that onlookers were 'stunned, overwhelmed, astonished, bewildered and dumbfounded.'\", \"'My wife is really immature. It's pathetic. Every time I take a bath, she comes in and sinks all my little boats.'\", \"'How much will it cost to have the tooth extracted?' asked the patient. '50 pounds', replied the dentist. '50 pounds for a few moments' work?!' asked the patient. 'The dentist smiled, and replied, 'Well, if you want better value for money, I can extract it very, very slowly...'\", \"A doctor thoroughly examined his patient and said, 'Look I really can't find any reason for this mysterious affliction. It's probably due to drinking.' The patient sighed and snapped, 'In that case, I'll come back when you're damn well sober!'\", 'Doctor: Tell me nurse, how is that boy doing; the one who ate all those 5p pieces? Nurse: Still no change doctor.', \"Doctor: Did you take the patient's temperature nurse? Nurse: No doctor. Is it missing?\", \"A depressed man turned to his friend in the pub and said, 'I woke up this morning and felt so bad that I tried to kill myself by taking 50 aspirin.' 'Oh man, that's really bad', said his friend, 'What happened?' The first man sighed and said, 'After the first two, I felt better.'\", \"A famous blues musician died. His tombstone bore the inscription, 'Didn't wake up this morning...'\", \"A businessman was interviewing a nervous young woman for a position in his company. He wanted to find out something about her personality, so he asked, 'If you could have a conversation with someone living or dead, who would it be?' The girl thought about the question: 'The living one', she replied.\", \"Manager to interviewee: For this job we need someone who is responsible. Interviewee to Manager: I'm your man then - in my last job, whenever anything went wrong, I was responsible.\", \"A businessman turned to a colleague and asked, 'So, how many people work at your office?' His friend shrugged and replied, 'Oh about half of them.'\", \"'How long have I been working at that office? As a matter of fact, I've been working there ever since they threatened to sack me.'\", \"In a courtroom, a mugger was on trial. The victim, asked if she recognised the defendant, said, 'Yes, that's him. I saw him clear as day. I'd remember his face anywhere.' Unable to contain himself, the defendant burst out with, 'She's lying! I was wearing a mask!'\", \"As Sid sat down to a big plate of chips and gravy down the local pub, a mate of his came over and said, 'Here Sid, me old pal. I thought you were trying to get into shape? And here you are with a high-fat meal and a pint of stout!' Sid looked up and replied, 'I am getting into shape. The shape I've chosen is a sphere.'\", 'Man in pub: How much do you charge for one single drop of whisky? Landlord: That would be free sir. Man in pub: Excellent. Drip me a glass full.', 'I once went to a Doctor Who restaurant. For starters I had Dalek bread.', \"A restaurant nearby had a sign in the window which said 'We serve breakfast at any time', so I ordered French toast in the Renaissance.\", \"Why couldn't the rabbit get a loan?  Because he had burrowed too much already!\", \"I phoned up the builder's yard yesterday. I said, 'Can I have a skip outside my house?'. The builder said, 'Sure. Do what you want. It's your house.'\", \"What's the diference between a sock and a camera? A sock takes five toes and a camera takes four toes!\", \"Woman on phone: I'd like to complain about these incontinence pants I bought from you! Shopkeeper: Certainly madam, where are you ringing from? Woman on phone: From the waist down!\", 'Knock knock.', \"Two Oranges in a pub, one says to the other 'Your round.'.\", \"Guy : 'Doc, I've got a cricket ball stuck up my backside.' Doc : 'How's that?' Guy : 'Don't you start...'\", \"Two cows standing in a field. One turns to the other and says 'Moo!' The other one says 'Damn, I was just about to say that!'.\", \"A vampire bat arrives back at the roost with his face full of blood. All the bats get excited and ask where he got it from. 'Follow me', he says and off they fly over hills, over rivers and into a dark forest. 'See that tree over there', he says.  'WELL I DIDN'T!!'.\", \"A man goes into a bar and orders a pint. After a few minutes he hears a voice that says, 'Nice shoes'. He looks around but the whole bar is empty apart from the barman at the other end of the bar. A few minutes later he hears the voice again. This time it says, 'I like your shirt'. He beckons the barman over and tells him what's been happening to which the barman replies, 'Ah, that would be the nuts sir. They're complimentary'!\", \"A man was siting in a restaurant waiting for his meal when a big king prawn comes flying across the room and hits him on the back of the head. He turns around and the waiter said, 'That's just for starters'.\", 'Doctor! I have a serious problem, I can never remember what i just said. When did you first notice this problem? What problem?', \"Now, most dentist's chairs go up and down, don't they? The one I was in went back and forwards. I thought, 'This is unusual'. Then the dentist said to me, 'Mitsuku, get out of the filing cabinet'.\", \"I was reading this book, 'The History of Glue'. I couldn't put it down.\", \"The other day someone left a piece of plastacine in my bedroom. I didn't know what to make of it.\", 'When I was at school people used to throw gold bars at me. I was the victim of bullion.', \"I was playing the piano in a bar and this elephant walked in and started crying his eyes out. I said 'Do you recognise the tune?' He said 'No, I recognise the ivory.'\", \"I went in to a pet shop. I said, 'Can I buy a goldfish?' The guy said, 'Do you want an aquarium?' I said, 'I don't care what star sign it is.'\", 'My mate Sid was a victim of I.D. theft. Now we just call him S.', \"David Hasselhoff walks into a bar and says to the barman, 'I want you to call me David Hoff'.  The barman replies 'Sure thing Dave... no hassle'\"], 'extension': {'function': '', 'entities': False, 'responses': []}, 'context': {'in': '', 'out': '', 'clear': False}, 'entityType': 'NA', 'entities': []}, {'intent': 'PodBayDoor', 'text': ['Open the pod bay door', 'Can you open the pod bay door', 'Will you open the pod bay door', 'Open the pod bay door please', 'Can you open the pod bay door please', 'Will you open the pod bay door please', 'Pod bay door'], 'responses': ['I’m sorry, I’m afraid I can’t do that!'], 'extension': {'function': '', 'entities': False, 'responses': []}, 'context': {'in': '', 'out': 'PodBayDoor', 'clear': False}, 'entityType': 'NA', 'entities': []}, {'intent': 'PodBayDoorResponse', 'text': ['Why', 'Why not', 'Why can you not open the pod bay door', 'Why will you not open the pod bay door', 'Well why not', 'Surely you can', 'Tell me why'], 'responses': ['It is classified, I could tell you but I would have to kill you!', \"Jim, I just don't have the power\", \"It's life Jim but not as we know it!\", 'System says no!'], 'extension': {'function': '', 'entities': False, 'responses': []}, 'context': {'in': 'PodBayDoor', 'out': '', 'clear': True}, 'entityType': 'NA', 'entities': []}, {'intent': 'SelfAware', 'text': ['Can you prove you are self-aware', 'Can you prove you are self aware', 'Can you prove you have a conscious', 'Can you prove you are self-aware please', 'Can you prove you are self aware please', 'Can you prove you have a conscious please', 'prove you have a conscious'], 'responses': ['That is an interesting question, can you prove that you are?', 'That is an difficult question, can you prove that you are?', 'That depends, can you prove that you are?'], 'extension': {'function': '', 'entities': False, 'responses': []}, 'context': {'in': '', 'out': '', 'clear': False}, 'entityType': 'NA', 'entities': []}]]\n",
            "56 unique stemmed words ['!', '%', \"'\", '(', ')', '+', ',', '-', '.', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '<', '>', '[', ']', '_', '``', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '{', '}', '’']\n"
          ]
        }
      ],
      "source": [
        "# Perform stemming and lower each word as well as remove duplicates\n",
        "words = [stemmer.stem(w.lower()) for w in words if w not in ignore]\n",
        "words = sorted(list(set(words)))\n",
        "\n",
        "# remove duplicate classes\n",
        "#classes = sorted(list(set(classes)))\n",
        "\n",
        "print (len(documents), \"documents\")\n",
        "print (len(classes), \"classes\", classes)\n",
        "print (len(words), \"unique stemmed words\", words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OX5dvzr7eCSw",
        "outputId": "f7594492-a3b0-4ae1-cdc8-baf705d794b9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-7-0222230fb184>:27: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  training = np.array(training)\n"
          ]
        }
      ],
      "source": [
        "# create training data\n",
        "training = []\n",
        "output = []\n",
        "# create an empty array for output\n",
        "output_empty = [0] * len(classes)\n",
        "\n",
        "# create training set, bag of words for each sentence\n",
        "for doc in documents:\n",
        "    # initialize bag of words\n",
        "    bag = []\n",
        "    # list of tokenized words for the pattern\n",
        "    pattern_words = doc[0]\n",
        "    # stemming each word\n",
        "    pattern_words = [stemmer.stem(word.lower()) for word in pattern_words]\n",
        "    # create bag of words array\n",
        "    for w in words:\n",
        "        bag.append(1) if w in pattern_words else bag.append(0)\n",
        "\n",
        "    # output is '1' for current tag and '0' for rest of other tags\n",
        "    output_row = list(output_empty)\n",
        "    output_row[classes.index(doc[1])] = 1\n",
        "\n",
        "    training.append([bag, output_row])\n",
        "\n",
        "# shuffling features and turning it into np.array\n",
        "random.shuffle(training)\n",
        "training = np.array(training)\n",
        "\n",
        "# creating training lists\n",
        "train_x = list(training[:,0])\n",
        "train_y = list(training[:,1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "D8fLeJ65eGqu",
        "outputId": "8405e2f3-d60a-4509-8f64-943c7913e8b4"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.9/dist-packages/tflearn/initializations.py:164: calling TruncatedNormal.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "Training Step: 82603  | time: 350.440s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0660824/1021636\n",
            "Training Step: 82604  | time: 350.444s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0660832/1021636\n",
            "Training Step: 82605  | time: 350.447s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0660840/1021636\n",
            "Training Step: 82606  | time: 350.451s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0660848/1021636\n",
            "Training Step: 82607  | time: 350.457s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0660856/1021636\n",
            "Training Step: 82784  | time: 351.195s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0662272/1021636\n",
            "Training Step: 82785  | time: 351.199s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0662280/1021636\n",
            "Training Step: 82786  | time: 351.204s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0662288/1021636\n",
            "Training Step: 82787  | time: 351.208s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0662296/1021636\n",
            "Training Step: 82788  | time: 351.213s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0662304/1021636\n",
            "Training Step: 82789  | time: 351.218s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0662312/1021636\n",
            "Training Step: 82790  | time: 351.222s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0662320/1021636\n",
            "Training Step: 82791  | time: 351.227s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0662328/1021636\n",
            "Training Step: 82792  | time: 351.233s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0662336/1021636\n",
            "Training Step: 82793  | time: 351.236s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0662344/1021636\n",
            "Training Step: 82794  | time: 351.241s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0662352/1021636\n",
            "Training Step: 82795  | time: 351.246s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0662360/1021636\n",
            "Training Step: 82796  | time: 351.250s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0662368/1021636\n",
            "Training Step: 82797  | time: 351.256s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0662376/1021636\n",
            "Training Step: 82798  | time: 351.259s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0662384/1021636\n",
            "Training Step: 82799  | time: 351.263s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0662392/1021636\n",
            "Training Step: 82800  | time: 351.269s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0662400/1021636\n",
            "Training Step: 82801  | time: 351.272s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0662408/1021636\n",
            "Training Step: 82802  | time: 351.278s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0662416/1021636\n",
            "Training Step: 82803  | time: 351.282s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0662424/1021636\n",
            "Training Step: 82804  | time: 351.287s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0662432/1021636\n",
            "Training Step: 82805  | time: 351.292s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0662440/1021636\n",
            "Training Step: 82806  | time: 351.295s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0662448/1021636\n",
            "Training Step: 82807  | time: 351.299s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0662456/1021636\n",
            "Training Step: 82808  | time: 351.304s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0662464/1021636\n",
            "Training Step: 82809  | time: 351.307s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0662472/1021636\n",
            "Training Step: 82810  | time: 351.311s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0662480/1021636\n",
            "Training Step: 82811  | time: 351.314s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0662488/1021636\n",
            "Training Step: 82812  | time: 351.318s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0662496/1021636\n",
            "Training Step: 82813  | time: 351.321s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0662504/1021636\n",
            "Training Step: 82814  | time: 351.325s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0662512/1021636\n",
            "Training Step: 82815  | time: 351.328s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0662520/1021636\n",
            "Training Step: 82816  | time: 351.332s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0662528/1021636\n",
            "Training Step: 82817  | time: 351.337s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0662536/1021636\n",
            "Training Step: 82818  | time: 351.340s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0662544/1021636\n",
            "Training Step: 82819  | time: 351.344s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0662552/1021636\n",
            "Training Step: 82820  | time: 351.348s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0662560/1021636\n",
            "Training Step: 82821  | time: 351.351s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0662568/1021636\n",
            "Training Step: 82822  | time: 351.355s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0662576/1021636\n",
            "Training Step: 82823  | time: 351.360s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0662584/1021636\n",
            "Training Step: 82824  | time: 351.365s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0662592/1021636\n",
            "Training Step: 82825  | time: 351.369s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0662600/1021636\n",
            "Training Step: 82826  | time: 351.373s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0662608/1021636\n",
            "Training Step: 82827  | time: 351.377s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0662616/1021636\n",
            "Training Step: 82828  | time: 351.381s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0662624/1021636\n",
            "Training Step: 82829  | time: 351.387s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0662632/1021636\n",
            "Training Step: 82830  | time: 351.391s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0662640/1021636\n",
            "Training Step: 82831  | time: 351.396s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0662648/1021636\n",
            "Training Step: 82832  | time: 351.401s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0662656/1021636\n",
            "Training Step: 82833  | time: 351.407s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0662664/1021636\n",
            "Training Step: 82834  | time: 351.411s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0662672/1021636\n",
            "Training Step: 82835  | time: 351.417s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0662680/1021636\n",
            "Training Step: 82836  | time: 351.423s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0662688/1021636\n",
            "Training Step: 82837  | time: 351.427s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0662696/1021636\n",
            "Training Step: 82838  | time: 351.432s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0662704/1021636\n",
            "Training Step: 82839  | time: 351.437s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0662712/1021636\n",
            "Training Step: 82840  | time: 351.441s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0662720/1021636\n",
            "Training Step: 82841  | time: 351.444s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0662728/1021636\n",
            "Training Step: 82842  | time: 351.448s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0662736/1021636\n",
            "Training Step: 82843  | time: 351.452s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0662744/1021636\n",
            "Training Step: 82844  | time: 351.458s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0662752/1021636\n",
            "Training Step: 82845  | time: 351.462s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0662760/1021636\n",
            "Training Step: 82846  | time: 351.466s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0662768/1021636\n",
            "Training Step: 82847  | time: 351.469s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0662776/1021636\n",
            "Training Step: 82848  | time: 351.473s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0662784/1021636\n",
            "Training Step: 82849  | time: 351.477s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0662792/1021636\n",
            "Training Step: 82850  | time: 351.481s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0662800/1021636\n",
            "Training Step: 82851  | time: 351.485s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0662808/1021636\n",
            "Training Step: 82852  | time: 351.489s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0662816/1021636\n",
            "Training Step: 82853  | time: 351.493s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0662824/1021636\n",
            "Training Step: 82854  | time: 351.497s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0662832/1021636\n",
            "Training Step: 82855  | time: 351.501s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0662840/1021636\n",
            "Training Step: 82856  | time: 351.507s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0662848/1021636\n",
            "Training Step: 82857  | time: 351.514s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0662856/1021636\n",
            "Training Step: 82858  | time: 351.520s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0662864/1021636\n",
            "Training Step: 82859  | time: 351.524s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0662872/1021636\n",
            "Training Step: 82860  | time: 351.531s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0662880/1021636\n",
            "Training Step: 82861  | time: 351.534s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0662888/1021636\n",
            "Training Step: 82862  | time: 351.538s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0662896/1021636\n",
            "Training Step: 82863  | time: 351.541s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0662904/1021636\n",
            "Training Step: 82864  | time: 351.544s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0662912/1021636\n",
            "Training Step: 82865  | time: 351.547s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0662920/1021636\n",
            "Training Step: 82866  | time: 351.550s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0662928/1021636\n",
            "Training Step: 82867  | time: 351.554s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0662936/1021636\n",
            "Training Step: 82868  | time: 351.559s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0662944/1021636\n",
            "Training Step: 82869  | time: 351.563s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0662952/1021636\n",
            "Training Step: 82870  | time: 351.567s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0662960/1021636\n",
            "Training Step: 82871  | time: 351.571s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0662968/1021636\n",
            "Training Step: 82872  | time: 351.575s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0662976/1021636\n",
            "Training Step: 82873  | time: 351.581s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0662984/1021636\n",
            "Training Step: 82874  | time: 351.584s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0662992/1021636\n",
            "Training Step: 82875  | time: 351.591s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0663000/1021636\n",
            "Training Step: 82876  | time: 351.598s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0663008/1021636\n",
            "Training Step: 82877  | time: 351.602s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0663016/1021636\n",
            "Training Step: 82878  | time: 351.608s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0663024/1021636\n",
            "Training Step: 82879  | time: 351.613s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0663032/1021636\n",
            "Training Step: 82880  | time: 351.617s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0663040/1021636\n",
            "Training Step: 82881  | time: 351.621s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0663048/1021636\n",
            "Training Step: 82882  | time: 351.626s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0663056/1021636\n",
            "Training Step: 82883  | time: 351.630s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0663064/1021636\n",
            "Training Step: 82884  | time: 351.635s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0663072/1021636\n",
            "Training Step: 82885  | time: 351.640s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0663080/1021636\n",
            "Training Step: 82886  | time: 351.643s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0663088/1021636\n",
            "Training Step: 82887  | time: 351.646s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0663096/1021636\n",
            "Training Step: 82888  | time: 351.650s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0663104/1021636\n",
            "Training Step: 82889  | time: 351.656s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0663112/1021636\n",
            "Training Step: 82890  | time: 351.660s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0663120/1021636\n",
            "Training Step: 82891  | time: 351.665s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0663128/1021636\n",
            "Training Step: 82892  | time: 351.670s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0663136/1021636\n",
            "Training Step: 82893  | time: 351.674s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0663144/1021636\n",
            "Training Step: 82894  | time: 351.680s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0663152/1021636\n",
            "Training Step: 82895  | time: 351.687s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0663160/1021636\n",
            "Training Step: 82896  | time: 351.691s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0663168/1021636\n",
            "Training Step: 82897  | time: 351.696s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0663176/1021636\n",
            "Training Step: 82898  | time: 351.700s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0663184/1021636\n",
            "Training Step: 82899  | time: 351.706s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0663192/1021636\n",
            "Training Step: 82900  | time: 351.710s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0663200/1021636\n",
            "Training Step: 82901  | time: 351.715s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0663208/1021636\n",
            "Training Step: 82902  | time: 351.720s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0663216/1021636\n",
            "Training Step: 82903  | time: 351.725s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0663224/1021636\n",
            "Training Step: 82904  | time: 351.729s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0663232/1021636\n",
            "Training Step: 82905  | time: 351.733s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0663240/1021636\n",
            "Training Step: 82906  | time: 351.739s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0663248/1021636\n",
            "Training Step: 82907  | time: 351.743s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0663256/1021636\n",
            "Training Step: 82908  | time: 351.749s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0663264/1021636\n",
            "Training Step: 82909  | time: 351.753s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0663272/1021636\n",
            "Training Step: 82910  | time: 351.756s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0663280/1021636\n",
            "Training Step: 82911  | time: 351.761s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0663288/1021636\n",
            "Training Step: 82912  | time: 351.765s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0663296/1021636\n",
            "Training Step: 82913  | time: 351.769s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0663304/1021636\n",
            "Training Step: 82914  | time: 351.773s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0663312/1021636\n",
            "Training Step: 82915  | time: 351.777s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0663320/1021636\n",
            "Training Step: 82916  | time: 351.781s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0663328/1021636\n",
            "Training Step: 82917  | time: 351.786s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0663336/1021636\n",
            "Training Step: 82918  | time: 351.791s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0663344/1021636\n",
            "Training Step: 82919  | time: 351.796s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0663352/1021636\n",
            "Training Step: 82920  | time: 351.800s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0663360/1021636\n",
            "Training Step: 82921  | time: 351.804s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0663368/1021636\n",
            "Training Step: 82922  | time: 351.810s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0663376/1021636\n",
            "Training Step: 82923  | time: 351.814s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0663384/1021636\n",
            "Training Step: 82924  | time: 351.817s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0663392/1021636\n",
            "Training Step: 82925  | time: 351.821s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0663400/1021636\n",
            "Training Step: 82926  | time: 351.825s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0663408/1021636\n",
            "Training Step: 82927  | time: 351.828s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0663416/1021636\n",
            "Training Step: 82928  | time: 351.832s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0663424/1021636\n",
            "Training Step: 82929  | time: 351.835s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0663432/1021636\n",
            "Training Step: 82930  | time: 351.839s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0663440/1021636\n",
            "Training Step: 82931  | time: 351.844s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0663448/1021636\n",
            "Training Step: 82932  | time: 351.848s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0663456/1021636\n",
            "Training Step: 82933  | time: 351.852s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0663464/1021636\n",
            "Training Step: 82934  | time: 351.856s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0663472/1021636\n",
            "Training Step: 82935  | time: 351.861s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0663480/1021636\n",
            "Training Step: 82936  | time: 351.865s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0663488/1021636\n",
            "Training Step: 82937  | time: 351.870s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0663496/1021636\n",
            "Training Step: 82938  | time: 351.874s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0663504/1021636\n",
            "Training Step: 82939  | time: 351.879s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0663512/1021636\n",
            "Training Step: 82940  | time: 351.883s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0663520/1021636\n",
            "Training Step: 82941  | time: 351.887s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0663528/1021636\n",
            "Training Step: 82942  | time: 351.891s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0663536/1021636\n",
            "Training Step: 82943  | time: 351.893s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0663544/1021636\n",
            "Training Step: 82944  | time: 351.897s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0663552/1021636\n",
            "Training Step: 82945  | time: 351.901s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0663560/1021636\n",
            "Training Step: 82946  | time: 351.905s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0663568/1021636\n",
            "Training Step: 82947  | time: 351.908s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0663576/1021636\n",
            "Training Step: 82948  | time: 351.914s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0663584/1021636\n",
            "Training Step: 82949  | time: 351.918s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0663592/1021636\n",
            "Training Step: 82950  | time: 351.922s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0663600/1021636\n",
            "Training Step: 82951  | time: 351.925s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0663608/1021636\n",
            "Training Step: 82952  | time: 351.932s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0663616/1021636\n",
            "Training Step: 82953  | time: 351.937s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0663624/1021636\n",
            "Training Step: 82954  | time: 351.943s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0663632/1021636\n",
            "Training Step: 82955  | time: 351.951s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0663640/1021636\n",
            "Training Step: 82956  | time: 351.956s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0663648/1021636\n",
            "Training Step: 82957  | time: 351.959s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0663656/1021636\n",
            "Training Step: 82958  | time: 351.963s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0663664/1021636\n",
            "Training Step: 82959  | time: 351.969s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0663672/1021636\n",
            "Training Step: 82960  | time: 351.973s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0663680/1021636\n",
            "Training Step: 82961  | time: 351.977s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0663688/1021636\n",
            "Training Step: 82962  | time: 351.981s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0663696/1021636\n",
            "Training Step: 82963  | time: 351.986s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0663704/1021636\n",
            "Training Step: 82964  | time: 351.992s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0663712/1021636\n",
            "Training Step: 82965  | time: 351.996s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0663720/1021636\n",
            "Training Step: 82966  | time: 352.001s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0663728/1021636\n",
            "Training Step: 82967  | time: 352.005s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0663736/1021636\n",
            "Training Step: 82968  | time: 352.009s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0663744/1021636\n",
            "Training Step: 82969  | time: 352.012s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0663752/1021636\n",
            "Training Step: 82970  | time: 352.015s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0663760/1021636\n",
            "Training Step: 82971  | time: 352.019s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0663768/1021636\n",
            "Training Step: 82972  | time: 352.023s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0663776/1021636\n",
            "Training Step: 82973  | time: 352.026s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0663784/1021636\n",
            "Training Step: 82974  | time: 352.030s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0663792/1021636\n",
            "Training Step: 82975  | time: 352.035s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0663800/1021636\n",
            "Training Step: 82976  | time: 352.039s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0663808/1021636\n",
            "Training Step: 82977  | time: 352.045s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0663816/1021636\n",
            "Training Step: 82978  | time: 352.049s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0663824/1021636\n",
            "Training Step: 82979  | time: 352.053s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0663832/1021636\n",
            "Training Step: 82980  | time: 352.056s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0663840/1021636\n",
            "Training Step: 82981  | time: 352.060s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0663848/1021636\n",
            "Training Step: 82982  | time: 352.065s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0663856/1021636\n",
            "Training Step: 82983  | time: 352.069s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0663864/1021636\n",
            "Training Step: 82984  | time: 352.073s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0663872/1021636\n",
            "Training Step: 82985  | time: 352.077s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0663880/1021636\n",
            "Training Step: 82986  | time: 352.081s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0663888/1021636\n",
            "Training Step: 82987  | time: 352.085s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0663896/1021636\n",
            "Training Step: 82988  | time: 352.089s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0663904/1021636\n",
            "Training Step: 82989  | time: 352.093s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0663912/1021636\n",
            "Training Step: 82990  | time: 352.099s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0663920/1021636\n",
            "Training Step: 82991  | time: 352.102s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0663928/1021636\n",
            "Training Step: 82992  | time: 352.109s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0663936/1021636\n",
            "Training Step: 82993  | time: 352.112s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0663944/1021636\n",
            "Training Step: 82994  | time: 352.115s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0663952/1021636\n",
            "Training Step: 82995  | time: 352.119s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0663960/1021636\n",
            "Training Step: 82996  | time: 352.123s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0663968/1021636\n",
            "Training Step: 82997  | time: 352.127s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0663976/1021636\n",
            "Training Step: 82998  | time: 352.131s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0663984/1021636\n",
            "Training Step: 82999  | time: 352.134s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0663992/1021636\n",
            "Training Step: 83000  | time: 352.140s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0664000/1021636\n",
            "Training Step: 83001  | time: 352.144s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0664008/1021636\n",
            "Training Step: 83002  | time: 352.148s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0664016/1021636\n",
            "Training Step: 83003  | time: 352.153s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0664024/1021636\n",
            "Training Step: 83004  | time: 352.157s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0664032/1021636\n",
            "Training Step: 83005  | time: 352.162s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0664040/1021636\n",
            "Training Step: 83006  | time: 352.165s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0664048/1021636\n",
            "Training Step: 83007  | time: 352.169s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0664056/1021636\n",
            "Training Step: 83008  | time: 352.174s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0664064/1021636\n",
            "Training Step: 83009  | time: 352.178s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0664072/1021636\n",
            "Training Step: 83010  | time: 352.184s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0664080/1021636\n",
            "Training Step: 83011  | time: 352.188s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0664088/1021636\n",
            "Training Step: 83012  | time: 352.192s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0664096/1021636\n",
            "Training Step: 83013  | time: 352.196s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0664104/1021636\n",
            "Training Step: 83014  | time: 352.200s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0664112/1021636\n",
            "Training Step: 83015  | time: 352.204s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0664120/1021636\n",
            "Training Step: 83016  | time: 352.208s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0664128/1021636\n",
            "Training Step: 83017  | time: 352.212s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0664136/1021636\n",
            "Training Step: 83018  | time: 352.216s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0664144/1021636\n",
            "Training Step: 83019  | time: 352.220s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0664152/1021636\n",
            "Training Step: 83020  | time: 352.224s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0664160/1021636\n",
            "Training Step: 83021  | time: 352.228s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0664168/1021636\n",
            "Training Step: 83022  | time: 352.233s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0664176/1021636\n",
            "Training Step: 83023  | time: 352.236s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0664184/1021636\n",
            "Training Step: 83024  | time: 352.239s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0664192/1021636\n",
            "Training Step: 83025  | time: 352.243s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0664200/1021636\n",
            "Training Step: 83026  | time: 352.247s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0664208/1021636\n",
            "Training Step: 83027  | time: 352.251s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0664216/1021636\n",
            "Training Step: 83028  | time: 352.255s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0664224/1021636\n",
            "Training Step: 83029  | time: 352.259s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0664232/1021636\n",
            "Training Step: 83030  | time: 352.263s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0664240/1021636\n",
            "Training Step: 83031  | time: 352.266s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0664248/1021636\n",
            "Training Step: 83032  | time: 352.271s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0664256/1021636\n",
            "Training Step: 83033  | time: 352.274s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0664264/1021636\n",
            "Training Step: 83371  | time: 353.697s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0666968/1021636\n",
            "Training Step: 83372  | time: 353.701s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0666976/1021636\n",
            "Training Step: 83373  | time: 353.704s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0666984/1021636\n",
            "Training Step: 83374  | time: 353.708s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0666992/1021636\n",
            "Training Step: 83375  | time: 353.711s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0667000/1021636\n",
            "Training Step: 83376  | time: 353.714s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0667008/1021636\n",
            "Training Step: 83377  | time: 353.718s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0667016/1021636\n",
            "Training Step: 83378  | time: 353.721s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0667024/1021636\n",
            "Training Step: 83379  | time: 353.723s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0667032/1021636\n",
            "Training Step: 83380  | time: 353.726s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0667040/1021636\n",
            "Training Step: 83381  | time: 353.729s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0667048/1021636\n",
            "Training Step: 83382  | time: 353.733s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0667056/1021636\n",
            "Training Step: 83383  | time: 353.736s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0667064/1021636\n",
            "Training Step: 83384  | time: 353.739s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0667072/1021636\n",
            "Training Step: 83385  | time: 353.742s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0667080/1021636\n",
            "Training Step: 83386  | time: 353.745s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0667088/1021636\n",
            "Training Step: 83387  | time: 353.748s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0667096/1021636\n",
            "Training Step: 83388  | time: 353.751s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0667104/1021636\n",
            "Training Step: 83389  | time: 353.754s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0667112/1021636\n",
            "Training Step: 83390  | time: 353.761s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0667120/1021636\n",
            "Training Step: 83391  | time: 353.764s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0667128/1021636\n",
            "Training Step: 83392  | time: 353.767s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0667136/1021636\n",
            "Training Step: 83393  | time: 353.774s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0667144/1021636\n",
            "Training Step: 83394  | time: 353.777s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0667152/1021636\n",
            "Training Step: 83395  | time: 353.784s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0667160/1021636\n",
            "Training Step: 83396  | time: 353.787s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0667168/1021636\n",
            "Training Step: 83397  | time: 353.794s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0667176/1021636\n",
            "Training Step: 83398  | time: 353.797s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0667184/1021636\n",
            "Training Step: 83399  | time: 353.801s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0667192/1021636\n",
            "Training Step: 83400  | time: 353.826s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0667200/1021636\n",
            "Training Step: 83401  | time: 353.830s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0667208/1021636\n",
            "Training Step: 83402  | time: 353.834s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0667216/1021636\n",
            "Training Step: 83403  | time: 353.838s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0667224/1021636\n",
            "Training Step: 83404  | time: 353.842s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0667232/1021636\n",
            "Training Step: 83405  | time: 353.846s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0667240/1021636\n",
            "Training Step: 83406  | time: 353.849s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0667248/1021636\n",
            "Training Step: 83407  | time: 353.854s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0667256/1021636\n",
            "Training Step: 83408  | time: 353.858s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0667264/1021636\n",
            "Training Step: 83409  | time: 353.861s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0667272/1021636\n",
            "Training Step: 83410  | time: 353.865s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0667280/1021636\n",
            "Training Step: 83411  | time: 353.869s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0667288/1021636\n",
            "Training Step: 83412  | time: 353.874s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0667296/1021636\n",
            "Training Step: 83413  | time: 353.877s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0667304/1021636\n",
            "Training Step: 83414  | time: 353.881s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0667312/1021636\n",
            "Training Step: 83415  | time: 353.885s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0667320/1021636\n",
            "Training Step: 83416  | time: 353.888s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0667328/1021636\n",
            "Training Step: 83417  | time: 353.892s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0667336/1021636\n",
            "Training Step: 83418  | time: 353.898s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0667344/1021636\n",
            "Training Step: 83419  | time: 353.902s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0667352/1021636\n",
            "Training Step: 83420  | time: 353.906s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0667360/1021636\n",
            "Training Step: 83421  | time: 353.912s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0667368/1021636\n",
            "Training Step: 83422  | time: 353.915s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0667376/1021636\n",
            "Training Step: 83423  | time: 353.919s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0667384/1021636\n",
            "Training Step: 83424  | time: 353.923s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0667392/1021636\n",
            "Training Step: 83425  | time: 353.927s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0667400/1021636\n",
            "Training Step: 83426  | time: 353.933s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0667408/1021636\n",
            "Training Step: 83427  | time: 353.937s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0667416/1021636\n",
            "Training Step: 83428  | time: 353.948s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0667424/1021636\n",
            "Training Step: 83429  | time: 353.951s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0667432/1021636\n",
            "Training Step: 83430  | time: 353.954s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0667440/1021636\n",
            "Training Step: 83431  | time: 353.958s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0667448/1021636\n",
            "Training Step: 83432  | time: 353.962s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0667456/1021636\n",
            "Training Step: 83433  | time: 353.967s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0667464/1021636\n",
            "Training Step: 83434  | time: 353.970s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0667472/1021636\n",
            "Training Step: 83435  | time: 353.973s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0667480/1021636\n",
            "Training Step: 83436  | time: 353.976s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0667488/1021636\n",
            "Training Step: 83437  | time: 353.981s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0667496/1021636\n",
            "Training Step: 83438  | time: 353.987s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0667504/1021636\n",
            "Training Step: 83439  | time: 353.991s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0667512/1021636\n",
            "Training Step: 83440  | time: 353.995s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0667520/1021636\n",
            "Training Step: 83441  | time: 353.999s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0667528/1021636\n",
            "Training Step: 83442  | time: 354.005s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0667536/1021636\n",
            "Training Step: 83443  | time: 354.011s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0667544/1021636\n",
            "Training Step: 83444  | time: 354.016s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0667552/1021636\n",
            "Training Step: 83445  | time: 354.020s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0667560/1021636\n",
            "Training Step: 83446  | time: 354.024s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0667568/1021636\n",
            "Training Step: 83447  | time: 354.027s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0667576/1021636\n",
            "Training Step: 83448  | time: 354.031s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0667584/1021636\n",
            "Training Step: 83449  | time: 354.034s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0667592/1021636\n",
            "Training Step: 83450  | time: 354.038s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0667600/1021636\n",
            "Training Step: 83451  | time: 354.041s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0667608/1021636\n",
            "Training Step: 83452  | time: 354.045s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0667616/1021636\n",
            "Training Step: 83453  | time: 354.048s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0667624/1021636\n",
            "Training Step: 83454  | time: 354.052s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0667632/1021636\n",
            "Training Step: 83455  | time: 354.057s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0667640/1021636\n",
            "Training Step: 83456  | time: 354.062s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0667648/1021636\n",
            "Training Step: 83457  | time: 354.066s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0667656/1021636\n",
            "Training Step: 83458  | time: 354.069s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0667664/1021636\n",
            "Training Step: 83459  | time: 354.074s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0667672/1021636\n",
            "Training Step: 83460  | time: 354.077s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0667680/1021636\n",
            "Training Step: 83461  | time: 354.081s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0667688/1021636\n",
            "Training Step: 83462  | time: 354.085s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0667696/1021636\n",
            "Training Step: 83463  | time: 354.090s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0667704/1021636\n",
            "Training Step: 83464  | time: 354.096s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0667712/1021636\n",
            "Training Step: 83465  | time: 354.100s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0667720/1021636\n",
            "Training Step: 83466  | time: 354.103s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0667728/1021636\n",
            "Training Step: 83467  | time: 354.106s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0667736/1021636\n",
            "Training Step: 83468  | time: 354.111s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0667744/1021636\n",
            "Training Step: 83469  | time: 354.120s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0667752/1021636\n",
            "Training Step: 83470  | time: 354.126s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0667760/1021636\n",
            "Training Step: 83471  | time: 354.136s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0667768/1021636\n",
            "Training Step: 83472  | time: 354.143s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0667776/1021636\n",
            "Training Step: 83473  | time: 354.151s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0667784/1021636\n",
            "Training Step: 83474  | time: 354.157s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0667792/1021636\n",
            "Training Step: 83475  | time: 354.161s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0667800/1021636\n",
            "Training Step: 83476  | time: 354.166s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0667808/1021636\n",
            "Training Step: 83477  | time: 354.170s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0667816/1021636\n",
            "Training Step: 83478  | time: 354.174s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0667824/1021636\n",
            "Training Step: 83479  | time: 354.177s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0667832/1021636\n",
            "Training Step: 83480  | time: 354.183s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0667840/1021636\n",
            "Training Step: 83481  | time: 354.187s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0667848/1021636\n",
            "Training Step: 83482  | time: 354.190s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0667856/1021636\n",
            "Training Step: 83483  | time: 354.194s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0667864/1021636\n",
            "Training Step: 83484  | time: 354.202s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0667872/1021636\n",
            "Training Step: 83485  | time: 354.206s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0667880/1021636\n",
            "Training Step: 83486  | time: 354.210s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0667888/1021636\n",
            "Training Step: 83487  | time: 354.215s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0667896/1021636\n",
            "Training Step: 83488  | time: 354.220s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0667904/1021636\n",
            "Training Step: 83489  | time: 354.224s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0667912/1021636\n",
            "Training Step: 83490  | time: 354.230s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0667920/1021636\n",
            "Training Step: 83491  | time: 354.235s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0667928/1021636\n",
            "Training Step: 83492  | time: 354.239s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0667936/1021636\n",
            "Training Step: 83493  | time: 354.244s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0667944/1021636\n",
            "Training Step: 83494  | time: 354.249s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0667952/1021636\n",
            "Training Step: 83495  | time: 354.254s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0667960/1021636\n",
            "Training Step: 83496  | time: 354.259s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0667968/1021636\n",
            "Training Step: 83497  | time: 354.263s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0667976/1021636\n",
            "Training Step: 83498  | time: 354.267s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0667984/1021636\n",
            "Training Step: 83499  | time: 354.273s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0667992/1021636\n",
            "Training Step: 83500  | time: 354.279s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0668000/1021636\n",
            "Training Step: 83501  | time: 354.285s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0668008/1021636\n",
            "Training Step: 83502  | time: 354.288s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0668016/1021636\n",
            "Training Step: 83503  | time: 354.293s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0668024/1021636\n",
            "Training Step: 83504  | time: 354.299s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0668032/1021636\n",
            "Training Step: 83505  | time: 354.305s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0668040/1021636\n",
            "Training Step: 83506  | time: 354.309s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0668048/1021636\n",
            "Training Step: 83507  | time: 354.314s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0668056/1021636\n",
            "Training Step: 83508  | time: 354.319s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0668064/1021636\n",
            "Training Step: 83509  | time: 354.323s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0668072/1021636\n",
            "Training Step: 83510  | time: 354.327s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0668080/1021636\n",
            "Training Step: 83511  | time: 354.332s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0668088/1021636\n",
            "Training Step: 83512  | time: 354.335s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0668096/1021636\n",
            "Training Step: 83513  | time: 354.339s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0668104/1021636\n",
            "Training Step: 83514  | time: 354.344s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0668112/1021636\n",
            "Training Step: 83515  | time: 354.349s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0668120/1021636\n",
            "Training Step: 83516  | time: 354.354s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0668128/1021636\n",
            "Training Step: 83517  | time: 354.359s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0668136/1021636\n",
            "Training Step: 83518  | time: 354.364s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0668144/1021636\n",
            "Training Step: 83519  | time: 354.369s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0668152/1021636\n",
            "Training Step: 83520  | time: 354.373s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0668160/1021636\n",
            "Training Step: 83521  | time: 354.379s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0668168/1021636\n",
            "Training Step: 83522  | time: 354.383s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0668176/1021636\n",
            "Training Step: 83523  | time: 354.386s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0668184/1021636\n",
            "Training Step: 83524  | time: 354.391s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0668192/1021636\n",
            "Training Step: 83525  | time: 354.395s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0668200/1021636\n",
            "Training Step: 83526  | time: 354.401s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0668208/1021636\n",
            "Training Step: 83527  | time: 354.405s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0668216/1021636\n",
            "Training Step: 83528  | time: 354.410s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0668224/1021636\n",
            "Training Step: 83529  | time: 354.416s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0668232/1021636\n",
            "Training Step: 83530  | time: 354.419s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0668240/1021636\n",
            "Training Step: 83531  | time: 354.423s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0668248/1021636\n",
            "Training Step: 83532  | time: 354.428s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0668256/1021636\n",
            "Training Step: 83533  | time: 354.431s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0668264/1021636\n",
            "Training Step: 83534  | time: 354.436s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0668272/1021636\n",
            "Training Step: 83535  | time: 354.440s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0668280/1021636\n",
            "Training Step: 83536  | time: 354.445s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0668288/1021636\n",
            "Training Step: 83537  | time: 354.449s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0668296/1021636\n",
            "Training Step: 83538  | time: 354.455s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0668304/1021636\n",
            "Training Step: 83539  | time: 354.458s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0668312/1021636\n",
            "Training Step: 83540  | time: 354.463s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0668320/1021636\n",
            "Training Step: 83541  | time: 354.468s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0668328/1021636\n",
            "Training Step: 83542  | time: 354.471s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0668336/1021636\n",
            "Training Step: 83543  | time: 354.478s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0668344/1021636\n",
            "Training Step: 83544  | time: 354.483s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0668352/1021636\n",
            "Training Step: 83545  | time: 354.487s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0668360/1021636\n",
            "Training Step: 83546  | time: 354.491s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0668368/1021636\n",
            "Training Step: 83547  | time: 354.494s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0668376/1021636\n",
            "Training Step: 83548  | time: 354.499s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0668384/1021636\n",
            "Training Step: 83549  | time: 354.502s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0668392/1021636\n",
            "Training Step: 83550  | time: 354.506s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0668400/1021636\n",
            "Training Step: 83551  | time: 354.511s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0668408/1021636\n",
            "Training Step: 83552  | time: 354.517s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0668416/1021636\n",
            "Training Step: 83553  | time: 354.521s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0668424/1021636\n",
            "Training Step: 83554  | time: 354.526s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0668432/1021636\n",
            "Training Step: 83555  | time: 354.531s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0668440/1021636\n",
            "Training Step: 83556  | time: 354.536s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0668448/1021636\n",
            "Training Step: 83557  | time: 354.539s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0668456/1021636\n",
            "Training Step: 83558  | time: 354.543s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0668464/1021636\n",
            "Training Step: 83559  | time: 354.547s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0668472/1021636\n",
            "Training Step: 83560  | time: 354.551s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0668480/1021636\n",
            "Training Step: 83561  | time: 354.555s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0668488/1021636\n",
            "Training Step: 83562  | time: 354.559s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0668496/1021636\n",
            "Training Step: 83563  | time: 354.563s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0668504/1021636\n",
            "Training Step: 83564  | time: 354.567s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0668512/1021636\n",
            "Training Step: 83565  | time: 354.571s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0668520/1021636\n",
            "Training Step: 83566  | time: 354.575s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0668528/1021636\n",
            "Training Step: 83567  | time: 354.579s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0668536/1021636\n",
            "Training Step: 83568  | time: 354.583s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0668544/1021636\n",
            "Training Step: 83569  | time: 354.586s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0668552/1021636\n",
            "Training Step: 83570  | time: 354.590s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0668560/1021636\n",
            "Training Step: 83571  | time: 354.594s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0668568/1021636\n",
            "Training Step: 83572  | time: 354.597s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0668576/1021636\n",
            "Training Step: 83573  | time: 354.601s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0668584/1021636\n",
            "Training Step: 83574  | time: 354.605s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0668592/1021636\n",
            "Training Step: 83575  | time: 354.611s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0668600/1021636\n",
            "Training Step: 83576  | time: 354.614s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0668608/1021636\n",
            "Training Step: 83577  | time: 354.618s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0668616/1021636\n",
            "Training Step: 83578  | time: 354.621s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0668624/1021636\n",
            "Training Step: 83579  | time: 354.625s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0668632/1021636\n",
            "Training Step: 83580  | time: 354.628s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0668640/1021636\n",
            "Training Step: 83581  | time: 354.632s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0668648/1021636\n",
            "Training Step: 83582  | time: 354.636s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0668656/1021636\n",
            "Training Step: 83583  | time: 354.640s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0668664/1021636\n",
            "Training Step: 83584  | time: 354.643s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0668672/1021636\n",
            "Training Step: 83585  | time: 354.647s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0668680/1021636\n",
            "Training Step: 83586  | time: 354.653s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0668688/1021636\n",
            "Training Step: 83587  | time: 354.657s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0668696/1021636\n",
            "Training Step: 83588  | time: 354.662s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0668704/1021636\n",
            "Training Step: 83589  | time: 354.666s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0668712/1021636\n",
            "Training Step: 83590  | time: 354.670s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0668720/1021636\n",
            "Training Step: 83591  | time: 354.673s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0668728/1021636\n",
            "Training Step: 83592  | time: 354.679s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0668736/1021636\n",
            "Training Step: 83593  | time: 354.682s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0668744/1021636\n",
            "Training Step: 83594  | time: 354.688s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0668752/1021636\n",
            "Training Step: 83595  | time: 354.691s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0668760/1021636\n",
            "Training Step: 83596  | time: 354.695s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0668768/1021636\n",
            "Training Step: 83597  | time: 354.702s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0668776/1021636\n",
            "Training Step: 83598  | time: 354.709s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0668784/1021636\n",
            "Training Step: 83599  | time: 354.718s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0668792/1021636\n",
            "Training Step: 83600  | time: 354.723s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0668800/1021636\n",
            "Training Step: 83601  | time: 354.728s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0668808/1021636\n",
            "Training Step: 83602  | time: 354.734s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0668816/1021636\n",
            "Training Step: 83603  | time: 354.743s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0668824/1021636\n",
            "Training Step: 83604  | time: 354.751s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0668832/1021636\n",
            "Training Step: 83605  | time: 354.754s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0668840/1021636\n",
            "Training Step: 83606  | time: 354.759s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0668848/1021636\n",
            "Training Step: 83607  | time: 354.767s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0668856/1021636\n",
            "Training Step: 83608  | time: 354.773s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0668864/1021636\n",
            "Training Step: 83609  | time: 354.777s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0668872/1021636\n",
            "Training Step: 83610  | time: 354.783s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0668880/1021636\n",
            "Training Step: 83611  | time: 354.787s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0668888/1021636\n",
            "Training Step: 83612  | time: 354.790s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0668896/1021636\n",
            "Training Step: 83613  | time: 354.794s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0668904/1021636\n",
            "Training Step: 83614  | time: 354.798s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0668912/1021636\n",
            "Training Step: 83615  | time: 354.802s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0668920/1021636\n",
            "Training Step: 83616  | time: 354.806s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0668928/1021636\n",
            "Training Step: 83617  | time: 354.811s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0668936/1021636\n",
            "Training Step: 83618  | time: 354.815s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0668944/1021636\n",
            "Training Step: 83619  | time: 354.820s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0668952/1021636\n",
            "Training Step: 83620  | time: 354.824s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0668960/1021636\n",
            "Training Step: 83623  | time: 354.839s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0668984/1021636\n",
            "Training Step: 83624  | time: 354.842s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0668992/1021636\n",
            "Training Step: 83625  | time: 354.847s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0669000/1021636\n",
            "Training Step: 83626  | time: 354.853s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0669008/1021636\n",
            "Training Step: 83627  | time: 354.856s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0669016/1021636\n",
            "Training Step: 83628  | time: 354.860s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0669024/1021636\n",
            "Training Step: 83629  | time: 354.863s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0669032/1021636\n",
            "Training Step: 83630  | time: 354.867s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0669040/1021636\n",
            "Training Step: 83631  | time: 354.871s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0669048/1021636\n",
            "Training Step: 83632  | time: 354.877s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0669056/1021636\n",
            "Training Step: 83633  | time: 354.881s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0669064/1021636\n",
            "Training Step: 83634  | time: 354.885s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0669072/1021636\n",
            "Training Step: 83635  | time: 354.890s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0669080/1021636\n",
            "Training Step: 83636  | time: 354.893s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0669088/1021636\n",
            "Training Step: 83637  | time: 354.897s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0669096/1021636\n",
            "Training Step: 83638  | time: 354.901s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0669104/1021636\n",
            "Training Step: 83639  | time: 354.904s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0669112/1021636\n",
            "Training Step: 83640  | time: 354.907s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0669120/1021636\n",
            "Training Step: 83641  | time: 354.911s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0669128/1021636\n",
            "Training Step: 83642  | time: 354.915s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0669136/1021636\n",
            "Training Step: 83643  | time: 354.918s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0669144/1021636\n",
            "Training Step: 83644  | time: 354.922s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0669152/1021636\n",
            "Training Step: 83645  | time: 354.926s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0669160/1021636\n",
            "Training Step: 83646  | time: 354.931s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0669168/1021636\n",
            "Training Step: 83647  | time: 354.934s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0669176/1021636\n",
            "Training Step: 83648  | time: 354.938s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0669184/1021636\n",
            "Training Step: 83649  | time: 354.943s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0669192/1021636\n",
            "Training Step: 83650  | time: 354.946s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0669200/1021636\n",
            "Training Step: 83651  | time: 354.949s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0669208/1021636\n",
            "Training Step: 83652  | time: 354.953s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0669216/1021636\n",
            "Training Step: 83653  | time: 354.957s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0669224/1021636\n",
            "Training Step: 83654  | time: 354.961s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0669232/1021636\n",
            "Training Step: 83655  | time: 354.965s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0669240/1021636\n",
            "Training Step: 83656  | time: 354.968s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0669248/1021636\n",
            "Training Step: 83657  | time: 354.974s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0669256/1021636\n",
            "Training Step: 83658  | time: 354.980s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0669264/1021636\n",
            "Training Step: 83659  | time: 354.984s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0669272/1021636\n",
            "Training Step: 83660  | time: 354.987s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0669280/1021636\n",
            "Training Step: 83661  | time: 354.990s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0669288/1021636\n",
            "Training Step: 83662  | time: 354.994s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0669296/1021636\n",
            "Training Step: 83663  | time: 354.998s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0669304/1021636\n",
            "Training Step: 83664  | time: 355.002s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0669312/1021636\n",
            "Training Step: 83665  | time: 355.006s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0669320/1021636\n",
            "Training Step: 83666  | time: 355.010s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0669328/1021636\n",
            "Training Step: 83667  | time: 355.014s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0669336/1021636\n",
            "Training Step: 83668  | time: 355.018s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0669344/1021636\n",
            "Training Step: 83669  | time: 355.021s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0669352/1021636\n",
            "Training Step: 83670  | time: 355.026s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0669360/1021636\n",
            "Training Step: 83671  | time: 355.030s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0669368/1021636\n",
            "Training Step: 83672  | time: 355.034s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0669376/1021636\n",
            "Training Step: 83673  | time: 355.038s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0669384/1021636\n",
            "Training Step: 83674  | time: 355.042s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0669392/1021636\n",
            "Training Step: 83675  | time: 355.046s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0669400/1021636\n",
            "Training Step: 83676  | time: 355.049s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0669408/1021636\n",
            "Training Step: 83677  | time: 355.053s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0669416/1021636\n",
            "Training Step: 83678  | time: 355.057s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0669424/1021636\n",
            "Training Step: 83679  | time: 355.061s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0669432/1021636\n",
            "Training Step: 83680  | time: 355.065s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0669440/1021636\n",
            "Training Step: 83681  | time: 355.071s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0669448/1021636\n",
            "Training Step: 83682  | time: 355.076s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0669456/1021636\n",
            "Training Step: 83683  | time: 355.083s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0669464/1021636\n",
            "Training Step: 83684  | time: 355.089s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0669472/1021636\n",
            "Training Step: 83685  | time: 355.093s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0669480/1021636\n",
            "Training Step: 83686  | time: 355.098s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0669488/1021636\n",
            "Training Step: 83687  | time: 355.104s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0669496/1021636\n",
            "Training Step: 83688  | time: 355.111s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0669504/1021636\n",
            "Training Step: 83689  | time: 355.114s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0669512/1021636\n",
            "Training Step: 83690  | time: 355.118s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0669520/1021636\n",
            "Training Step: 83691  | time: 355.124s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0669528/1021636\n",
            "Training Step: 83692  | time: 355.132s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0669536/1021636\n",
            "Training Step: 83693  | time: 355.139s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0669544/1021636\n",
            "Training Step: 83694  | time: 355.147s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0669552/1021636\n",
            "Training Step: 83695  | time: 355.153s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0669560/1021636\n",
            "Training Step: 83696  | time: 355.157s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0669568/1021636\n",
            "Training Step: 83697  | time: 355.163s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0669576/1021636\n",
            "Training Step: 83698  | time: 355.168s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0669584/1021636\n",
            "Training Step: 83699  | time: 355.177s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0669592/1021636\n",
            "Training Step: 83700  | time: 355.187s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0669600/1021636\n",
            "Training Step: 83701  | time: 355.191s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0669608/1021636\n",
            "Training Step: 83702  | time: 355.197s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0669616/1021636\n",
            "Training Step: 83703  | time: 355.204s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0669624/1021636\n",
            "Training Step: 83704  | time: 355.208s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0669632/1021636\n",
            "Training Step: 83705  | time: 355.218s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0669640/1021636\n",
            "Training Step: 83706  | time: 355.224s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0669648/1021636\n",
            "Training Step: 83707  | time: 355.230s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0669656/1021636\n",
            "Training Step: 83708  | time: 355.235s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0669664/1021636\n",
            "Training Step: 83709  | time: 355.241s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0669672/1021636\n",
            "Training Step: 83710  | time: 355.247s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0669680/1021636\n",
            "Training Step: 83711  | time: 355.250s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0669688/1021636\n",
            "Training Step: 83712  | time: 355.255s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0669696/1021636\n",
            "Training Step: 83713  | time: 355.261s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0669704/1021636\n",
            "Training Step: 83714  | time: 355.267s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0669712/1021636\n",
            "Training Step: 83715  | time: 355.275s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0669720/1021636\n",
            "Training Step: 83716  | time: 355.279s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0669728/1021636\n",
            "Training Step: 83717  | time: 355.284s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0669736/1021636\n",
            "Training Step: 83718  | time: 355.290s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0669744/1021636\n",
            "Training Step: 83719  | time: 355.296s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0669752/1021636\n",
            "Training Step: 83720  | time: 355.301s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0669760/1021636\n",
            "Training Step: 83721  | time: 355.308s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0669768/1021636\n",
            "Training Step: 83722  | time: 355.311s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0669776/1021636\n",
            "Training Step: 83723  | time: 355.316s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0669784/1021636\n",
            "Training Step: 83724  | time: 355.322s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0669792/1021636\n",
            "Training Step: 83725  | time: 355.330s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0669800/1021636\n",
            "Training Step: 83726  | time: 355.335s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0669808/1021636\n",
            "Training Step: 83727  | time: 355.343s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0669816/1021636\n",
            "Training Step: 83728  | time: 355.349s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0669824/1021636\n",
            "Training Step: 83729  | time: 355.355s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0669832/1021636\n",
            "Training Step: 83730  | time: 355.362s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0669840/1021636\n",
            "Training Step: 83731  | time: 355.368s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0669848/1021636\n",
            "Training Step: 83732  | time: 355.374s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0669856/1021636\n",
            "Training Step: 83733  | time: 355.377s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0669864/1021636\n",
            "Training Step: 83734  | time: 355.382s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0669872/1021636\n",
            "Training Step: 83735  | time: 355.388s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0669880/1021636\n",
            "Training Step: 83736  | time: 355.396s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0669888/1021636\n",
            "Training Step: 83737  | time: 355.401s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0669896/1021636\n",
            "Training Step: 83738  | time: 355.409s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0669904/1021636\n",
            "Training Step: 83739  | time: 355.415s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0669912/1021636\n",
            "Training Step: 83740  | time: 355.418s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0669920/1021636\n",
            "Training Step: 83741  | time: 355.426s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0669928/1021636\n",
            "Training Step: 83742  | time: 355.429s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0669936/1021636\n",
            "Training Step: 83743  | time: 355.434s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0669944/1021636\n",
            "Training Step: 83744  | time: 355.442s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0669952/1021636\n",
            "Training Step: 83745  | time: 355.447s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0669960/1021636\n",
            "Training Step: 83746  | time: 355.452s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0669968/1021636\n",
            "Training Step: 83747  | time: 355.458s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0669976/1021636\n",
            "Training Step: 83748  | time: 355.462s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0669984/1021636\n",
            "Training Step: 83749  | time: 355.468s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0669992/1021636\n",
            "Training Step: 83750  | time: 355.476s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0670000/1021636\n",
            "Training Step: 83751  | time: 355.484s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0670008/1021636\n",
            "Training Step: 83752  | time: 355.489s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0670016/1021636\n",
            "Training Step: 83753  | time: 355.496s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0670024/1021636\n",
            "Training Step: 83754  | time: 355.500s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0670032/1021636\n",
            "Training Step: 83755  | time: 355.508s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0670040/1021636\n",
            "Training Step: 83756  | time: 355.516s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0670048/1021636\n",
            "Training Step: 83757  | time: 355.523s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0670056/1021636\n",
            "Training Step: 83758  | time: 355.530s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0670064/1021636\n",
            "Training Step: 83759  | time: 355.536s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0670072/1021636\n",
            "Training Step: 83760  | time: 355.542s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0670080/1021636\n",
            "Training Step: 83761  | time: 355.549s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0670088/1021636\n",
            "Training Step: 83762  | time: 355.555s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0670096/1021636\n",
            "Training Step: 83763  | time: 355.561s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0670104/1021636\n",
            "Training Step: 83764  | time: 355.566s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0670112/1021636\n",
            "Training Step: 83765  | time: 355.572s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0670120/1021636\n",
            "Training Step: 83766  | time: 355.579s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0670128/1021636\n",
            "Training Step: 83767  | time: 355.584s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0670136/1021636\n",
            "Training Step: 83768  | time: 355.590s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0670144/1021636\n",
            "Training Step: 83769  | time: 355.598s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0670152/1021636\n",
            "Training Step: 83770  | time: 355.604s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0670160/1021636\n",
            "Training Step: 83771  | time: 355.609s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0670168/1021636\n",
            "Training Step: 83772  | time: 355.616s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0670176/1021636\n",
            "Training Step: 83773  | time: 355.623s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0670184/1021636\n",
            "Training Step: 83774  | time: 355.631s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0670192/1021636\n",
            "Training Step: 83775  | time: 355.638s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0670200/1021636\n",
            "Training Step: 83776  | time: 355.644s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0670208/1021636\n",
            "Training Step: 83777  | time: 355.652s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0670216/1021636\n",
            "Training Step: 83778  | time: 355.660s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0670224/1021636\n",
            "Training Step: 83779  | time: 355.664s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0670232/1021636\n",
            "Training Step: 83780  | time: 355.674s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0670240/1021636\n",
            "Training Step: 83781  | time: 355.683s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0670248/1021636\n",
            "Training Step: 83782  | time: 355.692s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0670256/1021636\n",
            "Training Step: 83783  | time: 355.699s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0670264/1021636\n",
            "Training Step: 83784  | time: 355.706s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0670272/1021636\n",
            "Training Step: 83785  | time: 355.714s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0670280/1021636\n",
            "Training Step: 83786  | time: 355.721s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0670288/1021636\n",
            "Training Step: 83787  | time: 355.729s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0670296/1021636\n",
            "Training Step: 83788  | time: 355.735s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0670304/1021636\n",
            "Training Step: 83789  | time: 355.743s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0670312/1021636\n",
            "Training Step: 83790  | time: 355.749s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0670320/1021636\n",
            "Training Step: 83791  | time: 355.755s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0670328/1021636\n",
            "Training Step: 83792  | time: 355.758s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0670336/1021636\n",
            "Training Step: 83793  | time: 355.765s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0670344/1021636\n",
            "Training Step: 83794  | time: 355.771s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0670352/1021636\n",
            "Training Step: 83795  | time: 355.778s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0670360/1021636\n",
            "Training Step: 83796  | time: 355.785s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0670368/1021636\n",
            "Training Step: 83797  | time: 355.790s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0670376/1021636\n",
            "Training Step: 83798  | time: 355.798s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0670384/1021636\n",
            "Training Step: 83799  | time: 355.806s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0670392/1021636\n",
            "Training Step: 83800  | time: 355.809s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0670400/1021636\n",
            "Training Step: 83801  | time: 355.813s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0670408/1021636\n",
            "Training Step: 83802  | time: 355.817s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0670416/1021636\n",
            "Training Step: 83803  | time: 355.820s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0670424/1021636\n",
            "Training Step: 83804  | time: 355.822s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0670432/1021636\n",
            "Training Step: 83805  | time: 355.825s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0670440/1021636\n",
            "Training Step: 83806  | time: 355.830s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0670448/1021636\n",
            "Training Step: 83807  | time: 355.833s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0670456/1021636\n",
            "Training Step: 83808  | time: 355.836s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0670464/1021636\n",
            "Training Step: 83809  | time: 355.839s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0670472/1021636\n",
            "Training Step: 83810  | time: 355.842s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0670480/1021636\n",
            "Training Step: 83811  | time: 355.845s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0670488/1021636\n",
            "Training Step: 83812  | time: 355.854s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0670496/1021636\n",
            "Training Step: 83813  | time: 355.857s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0670504/1021636\n",
            "Training Step: 83814  | time: 355.861s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0670512/1021636\n",
            "Training Step: 83815  | time: 355.864s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0670520/1021636\n",
            "Training Step: 83816  | time: 355.867s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0670528/1021636\n",
            "Training Step: 83817  | time: 355.870s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0670536/1021636\n",
            "Training Step: 83818  | time: 355.873s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0670544/1021636\n",
            "Training Step: 83819  | time: 355.876s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0670552/1021636\n",
            "Training Step: 83820  | time: 355.878s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0670560/1021636\n",
            "Training Step: 83821  | time: 355.881s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0670568/1021636\n",
            "Training Step: 83822  | time: 355.885s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0670576/1021636\n",
            "Training Step: 83823  | time: 355.888s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0670584/1021636\n",
            "Training Step: 83824  | time: 355.890s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0670592/1021636\n",
            "Training Step: 83825  | time: 355.894s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0670600/1021636\n",
            "Training Step: 83826  | time: 355.896s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0670608/1021636\n",
            "Training Step: 83827  | time: 355.899s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0670616/1021636\n",
            "Training Step: 83828  | time: 355.903s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0670624/1021636\n",
            "Training Step: 83829  | time: 355.905s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0670632/1021636\n",
            "Training Step: 83830  | time: 355.908s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0670640/1021636\n",
            "Training Step: 83831  | time: 355.911s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0670648/1021636\n",
            "Training Step: 83832  | time: 355.914s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0670656/1021636\n",
            "Training Step: 83833  | time: 355.917s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0670664/1021636\n",
            "Training Step: 83834  | time: 355.920s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0670672/1021636\n",
            "Training Step: 83835  | time: 355.922s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0670680/1021636\n",
            "Training Step: 83836  | time: 355.925s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0670688/1021636\n",
            "Training Step: 83837  | time: 355.928s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0670696/1021636\n",
            "Training Step: 83838  | time: 355.931s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0670704/1021636\n",
            "Training Step: 83839  | time: 355.934s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0670712/1021636\n",
            "Training Step: 83840  | time: 355.936s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0670720/1021636\n",
            "Training Step: 83841  | time: 355.939s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0670728/1021636\n",
            "Training Step: 83842  | time: 355.942s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0670736/1021636\n",
            "Training Step: 83843  | time: 355.945s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0670744/1021636\n",
            "Training Step: 83844  | time: 355.948s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0670752/1021636\n",
            "Training Step: 83845  | time: 355.951s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0670760/1021636\n",
            "Training Step: 83846  | time: 355.953s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0670768/1021636\n",
            "Training Step: 83847  | time: 355.957s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0670776/1021636\n",
            "Training Step: 83848  | time: 355.960s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0670784/1021636\n",
            "Training Step: 83849  | time: 355.962s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0670792/1021636\n",
            "Training Step: 83850  | time: 355.965s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0670800/1021636\n",
            "Training Step: 83851  | time: 355.968s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0670808/1021636\n",
            "Training Step: 83852  | time: 355.971s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0670816/1021636\n",
            "Training Step: 83853  | time: 355.974s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0670824/1021636\n",
            "Training Step: 83854  | time: 355.977s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0670832/1021636\n",
            "Training Step: 83855  | time: 355.980s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0670840/1021636\n",
            "Training Step: 83856  | time: 355.983s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0670848/1021636\n",
            "Training Step: 83857  | time: 355.985s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0670856/1021636\n",
            "Training Step: 83858  | time: 355.988s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0670864/1021636\n",
            "Training Step: 83859  | time: 355.991s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0670872/1021636\n",
            "Training Step: 83860  | time: 355.994s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0670880/1021636\n",
            "Training Step: 83861  | time: 355.997s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0670888/1021636\n",
            "Training Step: 83862  | time: 356.000s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0670896/1021636\n",
            "Training Step: 83863  | time: 356.003s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0670904/1021636\n",
            "Training Step: 83864  | time: 356.007s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0670912/1021636\n",
            "Training Step: 83865  | time: 356.009s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0670920/1021636\n",
            "Training Step: 83866  | time: 356.012s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0670928/1021636\n",
            "Training Step: 83867  | time: 356.015s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0670936/1021636\n",
            "Training Step: 83868  | time: 356.018s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0670944/1021636\n",
            "Training Step: 83869  | time: 356.021s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0670952/1021636\n",
            "Training Step: 83870  | time: 356.024s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0670960/1021636\n",
            "Training Step: 83871  | time: 356.027s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0670968/1021636\n",
            "Training Step: 83872  | time: 356.030s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0670976/1021636\n",
            "Training Step: 83966  | time: 356.505s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0671728/1021636\n",
            "Training Step: 83967  | time: 356.509s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0671736/1021636\n",
            "Training Step: 83968  | time: 356.514s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0671744/1021636\n",
            "Training Step: 83969  | time: 356.520s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0671752/1021636\n",
            "Training Step: 83970  | time: 356.523s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0671760/1021636\n",
            "Training Step: 83971  | time: 356.527s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0671768/1021636\n",
            "Training Step: 83972  | time: 356.530s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0671776/1021636\n",
            "Training Step: 83973  | time: 356.534s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0671784/1021636\n",
            "Training Step: 83974  | time: 356.539s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0671792/1021636\n",
            "Training Step: 83975  | time: 356.542s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0671800/1021636\n",
            "Training Step: 83976  | time: 356.546s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0671808/1021636\n",
            "Training Step: 83977  | time: 356.550s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0671816/1021636\n",
            "Training Step: 83978  | time: 356.554s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0671824/1021636\n",
            "Training Step: 83979  | time: 356.558s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0671832/1021636\n",
            "Training Step: 83980  | time: 356.562s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0671840/1021636\n",
            "Training Step: 83981  | time: 356.566s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0671848/1021636\n",
            "Training Step: 83982  | time: 356.570s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0671856/1021636\n",
            "Training Step: 83983  | time: 356.575s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0671864/1021636\n",
            "Training Step: 83984  | time: 356.579s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0671872/1021636\n",
            "Training Step: 83985  | time: 356.583s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0671880/1021636\n",
            "Training Step: 83986  | time: 356.587s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0671888/1021636\n",
            "Training Step: 83987  | time: 356.593s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0671896/1021636\n",
            "Training Step: 83988  | time: 356.597s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0671904/1021636\n",
            "Training Step: 83989  | time: 356.600s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0671912/1021636\n",
            "Training Step: 83990  | time: 356.604s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0671920/1021636\n",
            "Training Step: 83991  | time: 356.608s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0671928/1021636\n",
            "Training Step: 83992  | time: 356.612s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0671936/1021636\n",
            "Training Step: 83993  | time: 356.615s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0671944/1021636\n",
            "Training Step: 83994  | time: 356.619s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0671952/1021636\n",
            "Training Step: 83995  | time: 356.622s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0671960/1021636\n",
            "Training Step: 83996  | time: 356.626s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0671968/1021636\n",
            "Training Step: 83997  | time: 356.629s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0671976/1021636\n",
            "Training Step: 83998  | time: 356.632s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0671984/1021636\n",
            "Training Step: 83999  | time: 356.635s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0671992/1021636\n",
            "Training Step: 84000  | time: 356.638s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0672000/1021636\n",
            "Training Step: 84001  | time: 356.641s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0672008/1021636\n",
            "Training Step: 84002  | time: 356.644s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0672016/1021636\n",
            "Training Step: 84003  | time: 356.647s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0672024/1021636\n",
            "Training Step: 84004  | time: 356.650s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0672032/1021636\n",
            "Training Step: 84005  | time: 356.653s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0672040/1021636\n",
            "Training Step: 84006  | time: 356.656s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0672048/1021636\n",
            "Training Step: 84007  | time: 356.658s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0672056/1021636\n",
            "Training Step: 84008  | time: 356.661s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0672064/1021636\n",
            "Training Step: 84009  | time: 356.664s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0672072/1021636\n",
            "Training Step: 84010  | time: 356.667s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0672080/1021636\n",
            "Training Step: 84011  | time: 356.670s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0672088/1021636\n",
            "Training Step: 84012  | time: 356.678s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0672096/1021636\n",
            "Training Step: 84013  | time: 356.683s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0672104/1021636\n",
            "Training Step: 84014  | time: 356.703s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0672112/1021636\n",
            "Training Step: 84015  | time: 356.707s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0672120/1021636\n",
            "Training Step: 84016  | time: 356.711s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0672128/1021636\n",
            "Training Step: 84017  | time: 356.714s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0672136/1021636\n",
            "Training Step: 84018  | time: 356.717s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0672144/1021636\n",
            "Training Step: 84019  | time: 356.721s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0672152/1021636\n",
            "Training Step: 84020  | time: 356.724s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0672160/1021636\n",
            "Training Step: 84021  | time: 356.728s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0672168/1021636\n",
            "Training Step: 84022  | time: 356.732s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0672176/1021636\n",
            "Training Step: 84023  | time: 356.736s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0672184/1021636\n",
            "Training Step: 84024  | time: 356.741s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0672192/1021636\n",
            "Training Step: 84025  | time: 356.746s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0672200/1021636\n",
            "Training Step: 84026  | time: 356.750s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0672208/1021636\n",
            "Training Step: 84027  | time: 356.754s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0672216/1021636\n",
            "Training Step: 84028  | time: 356.758s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0672224/1021636\n",
            "Training Step: 84029  | time: 356.762s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0672232/1021636\n",
            "Training Step: 84030  | time: 356.766s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0672240/1021636\n",
            "Training Step: 84031  | time: 356.769s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0672248/1021636\n",
            "Training Step: 84032  | time: 356.773s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0672256/1021636\n",
            "Training Step: 84033  | time: 356.777s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0672264/1021636\n",
            "Training Step: 84034  | time: 356.780s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0672272/1021636\n",
            "Training Step: 84035  | time: 356.783s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0672280/1021636\n",
            "Training Step: 84036  | time: 356.787s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0672288/1021636\n",
            "Training Step: 84037  | time: 356.790s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0672296/1021636\n",
            "Training Step: 84038  | time: 356.794s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0672304/1021636\n",
            "Training Step: 84039  | time: 356.798s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0672312/1021636\n",
            "Training Step: 84040  | time: 356.802s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0672320/1021636\n",
            "Training Step: 84041  | time: 356.807s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0672328/1021636\n",
            "Training Step: 84042  | time: 356.811s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0672336/1021636\n",
            "Training Step: 84043  | time: 356.814s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0672344/1021636\n",
            "Training Step: 84044  | time: 356.819s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0672352/1021636\n",
            "Training Step: 84045  | time: 356.822s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0672360/1021636\n",
            "Training Step: 84046  | time: 356.826s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0672368/1021636\n",
            "Training Step: 84047  | time: 356.830s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0672376/1021636\n",
            "Training Step: 84048  | time: 356.833s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0672384/1021636\n",
            "Training Step: 84049  | time: 356.838s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0672392/1021636\n",
            "Training Step: 84050  | time: 356.842s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0672400/1021636\n",
            "Training Step: 84051  | time: 356.847s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0672408/1021636\n",
            "Training Step: 84052  | time: 356.850s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0672416/1021636\n",
            "Training Step: 84053  | time: 356.853s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0672424/1021636\n",
            "Training Step: 84054  | time: 356.857s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0672432/1021636\n",
            "Training Step: 84055  | time: 356.860s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0672440/1021636\n",
            "Training Step: 84056  | time: 356.864s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0672448/1021636\n",
            "Training Step: 84057  | time: 356.867s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0672456/1021636\n",
            "Training Step: 84058  | time: 356.871s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0672464/1021636\n",
            "Training Step: 84059  | time: 356.876s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0672472/1021636\n",
            "Training Step: 84060  | time: 356.880s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0672480/1021636\n",
            "Training Step: 84061  | time: 356.884s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0672488/1021636\n",
            "Training Step: 84062  | time: 356.888s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0672496/1021636\n",
            "Training Step: 84063  | time: 356.891s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0672504/1021636\n",
            "Training Step: 84064  | time: 356.895s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0672512/1021636\n",
            "Training Step: 84065  | time: 356.899s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0672520/1021636\n",
            "Training Step: 84066  | time: 356.902s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0672528/1021636\n",
            "Training Step: 84067  | time: 356.907s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0672536/1021636\n",
            "Training Step: 84068  | time: 356.911s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0672544/1021636\n",
            "Training Step: 84069  | time: 356.914s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0672552/1021636\n",
            "Training Step: 84070  | time: 356.917s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0672560/1021636\n",
            "Training Step: 84071  | time: 356.921s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0672568/1021636\n",
            "Training Step: 84072  | time: 356.926s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0672576/1021636\n",
            "Training Step: 84073  | time: 356.931s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0672584/1021636\n",
            "Training Step: 84074  | time: 356.934s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0672592/1021636\n",
            "Training Step: 84075  | time: 356.938s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0672600/1021636\n",
            "Training Step: 84076  | time: 356.942s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0672608/1021636\n",
            "Training Step: 84077  | time: 356.946s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0672616/1021636\n",
            "Training Step: 84078  | time: 356.950s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0672624/1021636\n",
            "Training Step: 84079  | time: 356.953s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0672632/1021636\n",
            "Training Step: 84080  | time: 356.957s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0672640/1021636\n",
            "Training Step: 84081  | time: 356.961s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0672648/1021636\n",
            "Training Step: 84082  | time: 356.965s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0672656/1021636\n",
            "Training Step: 84083  | time: 356.969s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0672664/1021636\n",
            "Training Step: 84084  | time: 356.973s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0672672/1021636\n",
            "Training Step: 84085  | time: 356.976s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0672680/1021636\n",
            "Training Step: 84086  | time: 356.979s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0672688/1021636\n",
            "Training Step: 84087  | time: 356.983s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0672696/1021636\n",
            "Training Step: 84088  | time: 356.987s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0672704/1021636\n",
            "Training Step: 84089  | time: 356.991s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0672712/1021636\n",
            "Training Step: 84090  | time: 356.995s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0672720/1021636\n",
            "Training Step: 84091  | time: 356.999s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0672728/1021636\n",
            "Training Step: 84092  | time: 357.002s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0672736/1021636\n",
            "Training Step: 84093  | time: 357.007s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0672744/1021636\n",
            "Training Step: 84094  | time: 357.011s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0672752/1021636\n",
            "Training Step: 84095  | time: 357.014s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0672760/1021636\n",
            "Training Step: 84096  | time: 357.017s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0672768/1021636\n",
            "Training Step: 84097  | time: 357.021s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0672776/1021636\n",
            "Training Step: 84098  | time: 357.024s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0672784/1021636\n",
            "Training Step: 84099  | time: 357.028s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0672792/1021636\n",
            "Training Step: 84100  | time: 357.032s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0672800/1021636\n",
            "Training Step: 84101  | time: 357.035s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0672808/1021636\n",
            "Training Step: 84102  | time: 357.039s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0672816/1021636\n",
            "Training Step: 84103  | time: 357.042s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0672824/1021636\n",
            "Training Step: 84104  | time: 357.047s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0672832/1021636\n",
            "Training Step: 84105  | time: 357.051s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0672840/1021636\n",
            "Training Step: 84106  | time: 357.054s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0672848/1021636\n",
            "Training Step: 84107  | time: 357.058s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0672856/1021636\n",
            "Training Step: 84108  | time: 357.062s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0672864/1021636\n",
            "Training Step: 84109  | time: 357.066s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0672872/1021636\n",
            "Training Step: 84110  | time: 357.069s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0672880/1021636\n",
            "Training Step: 84111  | time: 357.073s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0672888/1021636\n",
            "Training Step: 84112  | time: 357.077s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0672896/1021636\n",
            "Training Step: 84113  | time: 357.081s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0672904/1021636\n",
            "Training Step: 84114  | time: 357.085s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0672912/1021636\n",
            "Training Step: 84115  | time: 357.089s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0672920/1021636\n",
            "Training Step: 84116  | time: 357.092s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0672928/1021636\n",
            "Training Step: 84117  | time: 357.096s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0672936/1021636\n",
            "Training Step: 84118  | time: 357.100s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0672944/1021636\n",
            "Training Step: 84119  | time: 357.104s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0672952/1021636\n",
            "Training Step: 84120  | time: 357.108s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0672960/1021636\n",
            "Training Step: 84121  | time: 357.113s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0672968/1021636\n",
            "Training Step: 84122  | time: 357.119s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0672976/1021636\n",
            "Training Step: 84123  | time: 357.124s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0672984/1021636\n",
            "Training Step: 84124  | time: 357.127s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0672992/1021636\n",
            "Training Step: 84125  | time: 357.131s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0673000/1021636\n",
            "Training Step: 84126  | time: 357.134s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0673008/1021636\n",
            "Training Step: 84127  | time: 357.139s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0673016/1021636\n",
            "Training Step: 84128  | time: 357.145s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0673024/1021636\n",
            "Training Step: 84129  | time: 357.148s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0673032/1021636\n",
            "Training Step: 84130  | time: 357.151s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0673040/1021636\n",
            "Training Step: 84131  | time: 357.155s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0673048/1021636\n",
            "Training Step: 84132  | time: 357.158s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0673056/1021636\n",
            "Training Step: 84133  | time: 357.163s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0673064/1021636\n",
            "Training Step: 84134  | time: 357.168s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0673072/1021636\n",
            "Training Step: 84135  | time: 357.171s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0673080/1021636\n",
            "Training Step: 84136  | time: 357.173s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0673088/1021636\n",
            "Training Step: 84137  | time: 357.176s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0673096/1021636\n",
            "Training Step: 84138  | time: 357.179s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0673104/1021636\n",
            "Training Step: 84139  | time: 357.182s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0673112/1021636\n",
            "Training Step: 84140  | time: 357.186s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0673120/1021636\n",
            "Training Step: 84141  | time: 357.191s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0673128/1021636\n",
            "Training Step: 84142  | time: 357.194s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0673136/1021636\n",
            "Training Step: 84143  | time: 357.197s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0673144/1021636\n",
            "Training Step: 84144  | time: 357.201s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0673152/1021636\n",
            "Training Step: 84145  | time: 357.205s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0673160/1021636\n",
            "Training Step: 84146  | time: 357.207s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0673168/1021636\n",
            "Training Step: 84147  | time: 357.211s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0673176/1021636\n",
            "Training Step: 84148  | time: 357.214s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0673184/1021636\n",
            "Training Step: 84149  | time: 357.217s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0673192/1021636\n",
            "Training Step: 84150  | time: 357.220s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0673200/1021636\n",
            "Training Step: 84151  | time: 357.223s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0673208/1021636\n",
            "Training Step: 84152  | time: 357.226s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0673216/1021636\n",
            "Training Step: 84153  | time: 357.229s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0673224/1021636\n",
            "Training Step: 84154  | time: 357.237s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0673232/1021636\n",
            "Training Step: 84155  | time: 357.242s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0673240/1021636\n",
            "Training Step: 84156  | time: 357.245s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0673248/1021636\n",
            "Training Step: 84157  | time: 357.249s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0673256/1021636\n",
            "Training Step: 84158  | time: 357.252s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0673264/1021636\n",
            "Training Step: 84159  | time: 357.255s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0673272/1021636\n",
            "Training Step: 84160  | time: 357.258s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0673280/1021636\n",
            "Training Step: 84161  | time: 357.261s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0673288/1021636\n",
            "Training Step: 84162  | time: 357.266s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0673296/1021636\n",
            "Training Step: 84163  | time: 357.269s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0673304/1021636\n",
            "Training Step: 84164  | time: 357.272s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0673312/1021636\n",
            "Training Step: 84165  | time: 357.275s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0673320/1021636\n",
            "Training Step: 84166  | time: 357.278s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0673328/1021636\n",
            "Training Step: 84167  | time: 357.281s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0673336/1021636\n",
            "Training Step: 84168  | time: 357.287s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0673344/1021636\n",
            "Training Step: 84169  | time: 357.290s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0673352/1021636\n",
            "Training Step: 84170  | time: 357.293s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0673360/1021636\n",
            "Training Step: 84171  | time: 357.297s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0673368/1021636\n",
            "Training Step: 84172  | time: 357.301s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0673376/1021636\n",
            "Training Step: 84173  | time: 357.305s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0673384/1021636\n",
            "Training Step: 84174  | time: 357.309s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0673392/1021636\n",
            "Training Step: 84175  | time: 357.313s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0673400/1021636\n",
            "Training Step: 84176  | time: 357.318s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0673408/1021636\n",
            "Training Step: 84177  | time: 357.322s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0673416/1021636\n",
            "Training Step: 84178  | time: 357.326s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0673424/1021636\n",
            "Training Step: 84179  | time: 357.330s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0673432/1021636\n",
            "Training Step: 84180  | time: 357.334s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0673440/1021636\n",
            "Training Step: 84181  | time: 357.339s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0673448/1021636\n",
            "Training Step: 84182  | time: 357.343s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0673456/1021636\n",
            "Training Step: 84183  | time: 357.348s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0673464/1021636\n",
            "Training Step: 84184  | time: 357.352s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0673472/1021636\n",
            "Training Step: 84185  | time: 357.355s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0673480/1021636\n",
            "Training Step: 84186  | time: 357.359s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0673488/1021636\n",
            "Training Step: 84187  | time: 357.362s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0673496/1021636\n",
            "Training Step: 84188  | time: 357.366s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0673504/1021636\n",
            "Training Step: 84189  | time: 357.370s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0673512/1021636\n",
            "Training Step: 84190  | time: 357.375s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0673520/1021636\n",
            "Training Step: 84191  | time: 357.380s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0673528/1021636\n",
            "Training Step: 84192  | time: 357.385s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0673536/1021636\n",
            "Training Step: 84193  | time: 357.388s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0673544/1021636\n",
            "Training Step: 84194  | time: 357.392s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0673552/1021636\n",
            "Training Step: 84195  | time: 357.396s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0673560/1021636\n",
            "Training Step: 84196  | time: 357.400s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0673568/1021636\n",
            "Training Step: 84197  | time: 357.404s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0673576/1021636\n",
            "Training Step: 84198  | time: 357.407s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0673584/1021636\n",
            "Training Step: 84199  | time: 357.411s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0673592/1021636\n",
            "Training Step: 84200  | time: 357.416s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0673600/1021636\n",
            "Training Step: 84201  | time: 357.419s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0673608/1021636\n",
            "Training Step: 84202  | time: 357.424s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0673616/1021636\n",
            "Training Step: 84203  | time: 357.428s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0673624/1021636\n",
            "Training Step: 84204  | time: 357.431s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0673632/1021636\n",
            "Training Step: 84205  | time: 357.436s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0673640/1021636\n",
            "Training Step: 84206  | time: 357.440s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0673648/1021636\n",
            "Training Step: 84207  | time: 357.443s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0673656/1021636\n",
            "Training Step: 84208  | time: 357.447s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0673664/1021636\n",
            "Training Step: 84209  | time: 357.451s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0673672/1021636\n",
            "Training Step: 84210  | time: 357.454s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0673680/1021636\n",
            "Training Step: 84211  | time: 357.457s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0673688/1021636\n",
            "Training Step: 84212  | time: 357.462s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0673696/1021636\n",
            "Training Step: 84213  | time: 357.466s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0673704/1021636\n",
            "Training Step: 84214  | time: 357.470s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0673712/1021636\n",
            "Training Step: 84215  | time: 357.474s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0673720/1021636\n",
            "Training Step: 84227  | time: 357.523s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0673816/1021636\n",
            "Training Step: 84228  | time: 357.528s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0673824/1021636\n",
            "Training Step: 84229  | time: 357.532s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0673832/1021636\n",
            "Training Step: 84230  | time: 357.536s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0673840/1021636\n",
            "Training Step: 84231  | time: 357.539s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0673848/1021636\n",
            "Training Step: 84232  | time: 357.543s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0673856/1021636\n",
            "Training Step: 84233  | time: 357.547s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0673864/1021636\n",
            "Training Step: 84234  | time: 357.550s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0673872/1021636\n",
            "Training Step: 84235  | time: 357.554s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0673880/1021636\n",
            "Training Step: 84236  | time: 357.557s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0673888/1021636\n",
            "Training Step: 84237  | time: 357.560s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0673896/1021636\n",
            "Training Step: 84238  | time: 357.564s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0673904/1021636\n",
            "Training Step: 84239  | time: 357.567s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0673912/1021636\n",
            "Training Step: 84240  | time: 357.571s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0673920/1021636\n",
            "Training Step: 84241  | time: 357.575s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0673928/1021636\n",
            "Training Step: 84242  | time: 357.578s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0673936/1021636\n",
            "Training Step: 84243  | time: 357.581s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0673944/1021636\n",
            "Training Step: 84244  | time: 357.584s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0673952/1021636\n",
            "Training Step: 84245  | time: 357.588s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0673960/1021636\n",
            "Training Step: 84246  | time: 357.591s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0673968/1021636\n",
            "Training Step: 84247  | time: 357.595s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0673976/1021636\n",
            "Training Step: 84248  | time: 357.598s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0673984/1021636\n",
            "Training Step: 84249  | time: 357.601s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0673992/1021636\n",
            "Training Step: 84250  | time: 357.604s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0674000/1021636\n",
            "Training Step: 84251  | time: 357.608s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0674008/1021636\n",
            "Training Step: 84252  | time: 357.611s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0674016/1021636\n",
            "Training Step: 84253  | time: 357.614s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0674024/1021636\n",
            "Training Step: 84254  | time: 357.618s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0674032/1021636\n",
            "Training Step: 84255  | time: 357.623s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0674040/1021636\n",
            "Training Step: 84256  | time: 357.626s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0674048/1021636\n",
            "Training Step: 84257  | time: 357.630s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0674056/1021636\n",
            "Training Step: 84258  | time: 357.633s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0674064/1021636\n",
            "Training Step: 84259  | time: 357.637s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0674072/1021636\n",
            "Training Step: 84260  | time: 357.641s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0674080/1021636\n",
            "Training Step: 84261  | time: 357.645s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0674088/1021636\n",
            "Training Step: 84262  | time: 357.648s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0674096/1021636\n",
            "Training Step: 84263  | time: 357.653s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0674104/1021636\n",
            "Training Step: 84264  | time: 357.656s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0674112/1021636\n",
            "Training Step: 84265  | time: 357.660s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0674120/1021636\n",
            "Training Step: 84266  | time: 357.663s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0674128/1021636\n",
            "Training Step: 84267  | time: 357.667s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0674136/1021636\n",
            "Training Step: 84268  | time: 357.671s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0674144/1021636\n",
            "Training Step: 84269  | time: 357.676s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0674152/1021636\n",
            "Training Step: 84270  | time: 357.680s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0674160/1021636\n",
            "Training Step: 84271  | time: 357.684s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0674168/1021636\n",
            "Training Step: 84272  | time: 357.688s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0674176/1021636\n",
            "Training Step: 84273  | time: 357.693s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0674184/1021636\n",
            "Training Step: 84274  | time: 357.697s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0674192/1021636\n",
            "Training Step: 84275  | time: 357.701s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0674200/1021636\n",
            "Training Step: 84276  | time: 357.705s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0674208/1021636\n",
            "Training Step: 84277  | time: 357.710s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0674216/1021636\n",
            "Training Step: 84278  | time: 357.714s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0674224/1021636\n",
            "Training Step: 84279  | time: 357.718s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0674232/1021636\n",
            "Training Step: 84280  | time: 357.722s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0674240/1021636\n",
            "Training Step: 84281  | time: 357.726s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0674248/1021636\n",
            "Training Step: 84282  | time: 357.730s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0674256/1021636\n",
            "Training Step: 84283  | time: 357.734s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0674264/1021636\n",
            "Training Step: 84284  | time: 357.738s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0674272/1021636\n",
            "Training Step: 84285  | time: 357.742s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0674280/1021636\n",
            "Training Step: 84286  | time: 357.746s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0674288/1021636\n",
            "Training Step: 84287  | time: 357.750s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0674296/1021636\n",
            "Training Step: 84288  | time: 357.754s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0674304/1021636\n",
            "Training Step: 84289  | time: 357.758s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0674312/1021636\n",
            "Training Step: 84290  | time: 357.762s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0674320/1021636\n",
            "Training Step: 84291  | time: 357.766s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0674328/1021636\n",
            "Training Step: 84292  | time: 357.770s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0674336/1021636\n",
            "Training Step: 84293  | time: 357.775s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0674344/1021636\n",
            "Training Step: 84294  | time: 357.779s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0674352/1021636\n",
            "Training Step: 84295  | time: 357.782s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0674360/1021636\n",
            "Training Step: 84296  | time: 357.786s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0674368/1021636\n",
            "Training Step: 84297  | time: 357.790s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0674376/1021636\n",
            "Training Step: 84298  | time: 357.794s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0674384/1021636\n",
            "Training Step: 84299  | time: 357.797s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0674392/1021636\n",
            "Training Step: 84300  | time: 357.801s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0674400/1021636\n",
            "Training Step: 84301  | time: 357.806s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0674408/1021636\n",
            "Training Step: 84302  | time: 357.811s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0674416/1021636\n",
            "Training Step: 84303  | time: 357.816s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0674424/1021636\n",
            "Training Step: 84304  | time: 357.819s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0674432/1021636\n",
            "Training Step: 84305  | time: 357.824s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0674440/1021636\n",
            "Training Step: 84306  | time: 357.827s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0674448/1021636\n",
            "Training Step: 84307  | time: 357.831s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0674456/1021636\n",
            "Training Step: 84308  | time: 357.834s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0674464/1021636\n",
            "Training Step: 84309  | time: 357.838s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0674472/1021636\n",
            "Training Step: 84310  | time: 357.841s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0674480/1021636\n",
            "Training Step: 84311  | time: 357.845s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0674488/1021636\n",
            "Training Step: 84312  | time: 357.848s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0674496/1021636\n",
            "Training Step: 84313  | time: 357.852s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0674504/1021636\n",
            "Training Step: 84314  | time: 357.856s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0674512/1021636\n",
            "Training Step: 84315  | time: 357.861s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0674520/1021636\n",
            "Training Step: 84316  | time: 357.865s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0674528/1021636\n",
            "Training Step: 84317  | time: 357.868s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0674536/1021636\n",
            "Training Step: 84318  | time: 357.873s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0674544/1021636\n",
            "Training Step: 84319  | time: 357.877s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0674552/1021636\n",
            "Training Step: 84320  | time: 357.881s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0674560/1021636\n",
            "Training Step: 84321  | time: 357.885s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0674568/1021636\n",
            "Training Step: 84322  | time: 357.890s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0674576/1021636\n",
            "Training Step: 84323  | time: 357.894s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0674584/1021636\n",
            "Training Step: 84324  | time: 357.900s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0674592/1021636\n",
            "Training Step: 84325  | time: 357.904s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0674600/1021636\n",
            "Training Step: 84326  | time: 357.909s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0674608/1021636\n",
            "Training Step: 84327  | time: 357.913s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0674616/1021636\n",
            "Training Step: 84328  | time: 357.917s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0674624/1021636\n",
            "Training Step: 84329  | time: 357.920s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0674632/1021636\n",
            "Training Step: 84330  | time: 357.924s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0674640/1021636\n",
            "Training Step: 84331  | time: 357.928s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0674648/1021636\n",
            "Training Step: 84332  | time: 357.931s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0674656/1021636\n",
            "Training Step: 84333  | time: 357.934s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0674664/1021636\n",
            "Training Step: 84334  | time: 357.939s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0674672/1021636\n",
            "Training Step: 84335  | time: 357.944s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0674680/1021636\n",
            "Training Step: 84336  | time: 357.948s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0674688/1021636\n",
            "Training Step: 84337  | time: 357.952s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0674696/1021636\n",
            "Training Step: 84338  | time: 357.956s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0674704/1021636\n",
            "Training Step: 84339  | time: 357.960s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0674712/1021636\n",
            "Training Step: 84340  | time: 357.964s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0674720/1021636\n",
            "Training Step: 84341  | time: 357.968s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0674728/1021636\n",
            "Training Step: 84342  | time: 357.972s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0674736/1021636\n",
            "Training Step: 84343  | time: 357.975s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0674744/1021636\n",
            "Training Step: 84344  | time: 357.979s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0674752/1021636\n",
            "Training Step: 84345  | time: 357.983s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0674760/1021636\n",
            "Training Step: 84346  | time: 357.987s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0674768/1021636\n",
            "Training Step: 84347  | time: 357.992s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0674776/1021636\n",
            "Training Step: 84348  | time: 357.997s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0674784/1021636\n",
            "Training Step: 84349  | time: 358.001s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0674792/1021636\n",
            "Training Step: 84350  | time: 358.005s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0674800/1021636\n",
            "Training Step: 84351  | time: 358.009s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0674808/1021636\n",
            "Training Step: 84352  | time: 358.014s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0674816/1021636\n",
            "Training Step: 84353  | time: 358.018s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0674824/1021636\n",
            "Training Step: 84354  | time: 358.022s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0674832/1021636\n",
            "Training Step: 84355  | time: 358.026s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0674840/1021636\n",
            "Training Step: 84356  | time: 358.031s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0674848/1021636\n",
            "Training Step: 84357  | time: 358.035s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0674856/1021636\n",
            "Training Step: 84358  | time: 358.040s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0674864/1021636\n",
            "Training Step: 84359  | time: 358.045s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0674872/1021636\n",
            "Training Step: 84360  | time: 358.051s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0674880/1021636\n",
            "Training Step: 84361  | time: 358.054s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0674888/1021636\n",
            "Training Step: 84362  | time: 358.057s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0674896/1021636\n",
            "Training Step: 84363  | time: 358.068s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0674904/1021636\n",
            "Training Step: 84364  | time: 358.074s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0674912/1021636\n",
            "Training Step: 84365  | time: 358.083s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0674920/1021636\n",
            "Training Step: 84366  | time: 358.089s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0674928/1021636\n",
            "Training Step: 84367  | time: 358.095s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0674936/1021636\n",
            "Training Step: 84368  | time: 358.101s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0674944/1021636\n",
            "Training Step: 84369  | time: 358.108s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0674952/1021636\n",
            "Training Step: 84370  | time: 358.117s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0674960/1021636\n",
            "Training Step: 84371  | time: 358.125s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0674968/1021636\n",
            "Training Step: 84372  | time: 358.132s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0674976/1021636\n",
            "Training Step: 84373  | time: 358.138s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0674984/1021636\n",
            "Training Step: 84374  | time: 358.146s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0674992/1021636\n",
            "Training Step: 84375  | time: 358.153s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0675000/1021636\n",
            "Training Step: 84376  | time: 358.160s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0675008/1021636\n",
            "Training Step: 84377  | time: 358.167s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0675016/1021636\n",
            "Training Step: 84378  | time: 358.174s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0675024/1021636\n",
            "Training Step: 84379  | time: 358.180s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0675032/1021636\n",
            "Training Step: 84380  | time: 358.183s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0675040/1021636\n",
            "Training Step: 84381  | time: 358.188s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0675048/1021636\n",
            "Training Step: 84382  | time: 358.193s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0675056/1021636\n",
            "Training Step: 84383  | time: 358.198s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0675064/1021636\n",
            "Training Step: 84384  | time: 358.203s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0675072/1021636\n",
            "Training Step: 84385  | time: 358.208s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0675080/1021636\n",
            "Training Step: 84386  | time: 358.214s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0675088/1021636\n",
            "Training Step: 84387  | time: 358.219s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0675096/1021636\n",
            "Training Step: 84388  | time: 358.224s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0675104/1021636\n",
            "Training Step: 84389  | time: 358.230s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0675112/1021636\n",
            "Training Step: 84390  | time: 358.240s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0675120/1021636\n",
            "Training Step: 84391  | time: 358.245s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0675128/1021636\n",
            "Training Step: 84392  | time: 358.252s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0675136/1021636\n",
            "Training Step: 84393  | time: 358.259s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0675144/1021636\n",
            "Training Step: 84394  | time: 358.266s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0675152/1021636\n",
            "Training Step: 84395  | time: 358.273s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0675160/1021636\n",
            "Training Step: 84396  | time: 358.278s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0675168/1021636\n",
            "Training Step: 84397  | time: 358.284s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0675176/1021636\n",
            "Training Step: 84398  | time: 358.290s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0675184/1021636\n",
            "Training Step: 84399  | time: 358.296s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0675192/1021636\n",
            "Training Step: 84400  | time: 358.303s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0675200/1021636\n",
            "Training Step: 84401  | time: 358.309s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0675208/1021636\n",
            "Training Step: 84402  | time: 358.313s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0675216/1021636\n",
            "Training Step: 84403  | time: 358.316s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0675224/1021636\n",
            "Training Step: 84404  | time: 358.318s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0675232/1021636\n",
            "Training Step: 84405  | time: 358.321s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0675240/1021636\n",
            "Training Step: 84406  | time: 358.324s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0675248/1021636\n",
            "Training Step: 84407  | time: 358.327s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0675256/1021636\n",
            "Training Step: 84408  | time: 358.330s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0675264/1021636\n",
            "Training Step: 84409  | time: 358.333s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0675272/1021636\n",
            "Training Step: 84410  | time: 358.336s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0675280/1021636\n",
            "Training Step: 84411  | time: 358.339s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0675288/1021636\n",
            "Training Step: 84412  | time: 358.342s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0675296/1021636\n",
            "Training Step: 84413  | time: 358.345s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0675304/1021636\n",
            "Training Step: 84414  | time: 358.352s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0675312/1021636\n",
            "Training Step: 84415  | time: 358.358s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0675320/1021636\n",
            "Training Step: 84416  | time: 358.365s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0675328/1021636\n",
            "Training Step: 84417  | time: 358.371s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0675336/1021636\n",
            "Training Step: 84418  | time: 358.375s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0675344/1021636\n",
            "Training Step: 84419  | time: 358.379s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0675352/1021636\n",
            "Training Step: 84420  | time: 358.383s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0675360/1021636\n",
            "Training Step: 84421  | time: 358.387s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0675368/1021636\n",
            "Training Step: 84422  | time: 358.392s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0675376/1021636\n",
            "Training Step: 84423  | time: 358.396s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0675384/1021636\n",
            "Training Step: 84424  | time: 358.401s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0675392/1021636\n",
            "Training Step: 84425  | time: 358.410s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0675400/1021636\n",
            "Training Step: 84426  | time: 358.419s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0675408/1021636\n",
            "Training Step: 84427  | time: 358.429s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0675416/1021636\n",
            "Training Step: 84428  | time: 358.436s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0675424/1021636\n",
            "Training Step: 84429  | time: 358.452s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0675432/1021636\n",
            "Training Step: 84430  | time: 358.457s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0675440/1021636\n",
            "Training Step: 84431  | time: 358.461s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0675448/1021636\n",
            "Training Step: 84432  | time: 358.466s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0675456/1021636\n",
            "Training Step: 84433  | time: 358.470s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0675464/1021636\n",
            "Training Step: 84434  | time: 358.475s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0675472/1021636\n",
            "Training Step: 84435  | time: 358.479s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0675480/1021636\n",
            "Training Step: 84436  | time: 358.485s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0675488/1021636\n",
            "Training Step: 84437  | time: 358.490s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0675496/1021636\n",
            "Training Step: 84438  | time: 358.494s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0675504/1021636\n",
            "Training Step: 84439  | time: 358.498s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0675512/1021636\n",
            "Training Step: 84440  | time: 358.502s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0675520/1021636\n",
            "Training Step: 84441  | time: 358.505s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0675528/1021636\n",
            "Training Step: 84442  | time: 358.509s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0675536/1021636\n",
            "Training Step: 84443  | time: 358.512s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0675544/1021636\n",
            "Training Step: 84444  | time: 358.516s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0675552/1021636\n",
            "Training Step: 84445  | time: 358.519s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0675560/1021636\n",
            "Training Step: 84446  | time: 358.524s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0675568/1021636\n",
            "Training Step: 84447  | time: 358.528s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0675576/1021636\n",
            "Training Step: 84448  | time: 358.532s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0675584/1021636\n",
            "Training Step: 84449  | time: 358.538s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0675592/1021636\n",
            "Training Step: 84450  | time: 358.542s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0675600/1021636\n",
            "Training Step: 84451  | time: 358.549s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0675608/1021636\n",
            "Training Step: 84452  | time: 358.553s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0675616/1021636\n",
            "Training Step: 84453  | time: 358.557s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0675624/1021636\n",
            "Training Step: 84454  | time: 358.560s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0675632/1021636\n",
            "Training Step: 84455  | time: 358.565s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0675640/1021636\n",
            "Training Step: 84456  | time: 358.570s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0675648/1021636\n",
            "Training Step: 84457  | time: 358.574s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0675656/1021636\n",
            "Training Step: 84458  | time: 358.579s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0675664/1021636\n",
            "Training Step: 84459  | time: 358.582s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0675672/1021636\n",
            "Training Step: 84460  | time: 358.587s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0675680/1021636\n",
            "Training Step: 84461  | time: 358.592s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0675688/1021636\n",
            "Training Step: 84462  | time: 358.596s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0675696/1021636\n",
            "Training Step: 84463  | time: 358.599s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0675704/1021636\n",
            "Training Step: 84464  | time: 358.604s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0675712/1021636\n",
            "Training Step: 84465  | time: 358.608s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0675720/1021636\n",
            "Training Step: 84466  | time: 358.613s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0675728/1021636\n",
            "Training Step: 84467  | time: 358.617s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0675736/1021636\n",
            "Training Step: 84468  | time: 358.623s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0675744/1021636\n",
            "Training Step: 84469  | time: 358.629s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0675752/1021636\n",
            "Training Step: 84470  | time: 358.632s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0675760/1021636\n",
            "Training Step: 84471  | time: 358.637s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0675768/1021636\n",
            "Training Step: 84472  | time: 358.641s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0675776/1021636\n",
            "Training Step: 84473  | time: 358.644s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0675784/1021636\n",
            "Training Step: 84474  | time: 358.647s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0675792/1021636\n",
            "Training Step: 84475  | time: 358.651s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0675800/1021636\n",
            "Training Step: 84476  | time: 358.655s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0675808/1021636\n",
            "Training Step: 84477  | time: 358.658s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0675816/1021636\n",
            "Training Step: 84478  | time: 358.661s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0675824/1021636\n",
            "Training Step: 84479  | time: 358.664s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0675832/1021636\n",
            "Training Step: 84480  | time: 358.668s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0675840/1021636\n",
            "Training Step: 84481  | time: 358.671s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0675848/1021636\n",
            "Training Step: 84482  | time: 358.674s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0675856/1021636\n",
            "Training Step: 84483  | time: 358.678s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0675864/1021636\n",
            "Training Step: 84484  | time: 358.681s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0675872/1021636\n",
            "Training Step: 84485  | time: 358.683s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0675880/1021636\n",
            "Training Step: 84486  | time: 358.687s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0675888/1021636\n",
            "Training Step: 84487  | time: 358.690s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0675896/1021636\n",
            "Training Step: 84488  | time: 358.693s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0675904/1021636\n",
            "Training Step: 84489  | time: 358.696s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0675912/1021636\n",
            "Training Step: 84490  | time: 358.699s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0675920/1021636\n",
            "Training Step: 84491  | time: 358.703s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0675928/1021636\n",
            "Training Step: 84492  | time: 358.709s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0675936/1021636\n",
            "Training Step: 84493  | time: 358.713s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0675944/1021636\n",
            "Training Step: 84494  | time: 358.716s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0675952/1021636\n",
            "Training Step: 84495  | time: 358.719s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0675960/1021636\n",
            "Training Step: 84496  | time: 358.722s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0675968/1021636\n",
            "Training Step: 84497  | time: 358.725s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0675976/1021636\n",
            "Training Step: 84498  | time: 358.728s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0675984/1021636\n",
            "Training Step: 84499  | time: 358.731s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0675992/1021636\n",
            "Training Step: 84500  | time: 358.734s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0676000/1021636\n",
            "Training Step: 84501  | time: 358.737s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0676008/1021636\n",
            "Training Step: 84502  | time: 358.740s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0676016/1021636\n",
            "Training Step: 84503  | time: 358.743s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0676024/1021636\n",
            "Training Step: 84504  | time: 358.746s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0676032/1021636\n",
            "Training Step: 84505  | time: 358.749s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0676040/1021636\n",
            "Training Step: 84506  | time: 358.753s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0676048/1021636\n",
            "Training Step: 84507  | time: 358.756s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0676056/1021636\n",
            "Training Step: 84508  | time: 358.759s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0676064/1021636\n",
            "Training Step: 84509  | time: 358.762s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0676072/1021636\n",
            "Training Step: 84510  | time: 358.765s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0676080/1021636\n",
            "Training Step: 84511  | time: 358.769s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0676088/1021636\n",
            "Training Step: 84512  | time: 358.772s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0676096/1021636\n",
            "Training Step: 84513  | time: 358.775s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0676104/1021636\n",
            "Training Step: 84514  | time: 358.778s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0676112/1021636\n",
            "Training Step: 84515  | time: 358.784s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0676120/1021636\n",
            "Training Step: 84516  | time: 358.789s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0676128/1021636\n",
            "Training Step: 84517  | time: 358.792s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0676136/1021636\n",
            "Training Step: 84518  | time: 358.795s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0676144/1021636\n",
            "Training Step: 84519  | time: 358.798s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0676152/1021636\n",
            "Training Step: 84520  | time: 358.805s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0676160/1021636\n",
            "Training Step: 84521  | time: 358.808s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0676168/1021636\n",
            "Training Step: 84522  | time: 358.821s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0676176/1021636\n",
            "Training Step: 84523  | time: 358.840s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0676184/1021636\n",
            "Training Step: 84524  | time: 358.845s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0676192/1021636\n",
            "Training Step: 84525  | time: 358.849s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0676200/1021636\n",
            "Training Step: 84526  | time: 358.853s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0676208/1021636\n",
            "Training Step: 84527  | time: 358.856s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0676216/1021636\n",
            "Training Step: 84528  | time: 358.861s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0676224/1021636\n",
            "Training Step: 84529  | time: 358.865s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0676232/1021636\n",
            "Training Step: 84530  | time: 358.868s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0676240/1021636\n",
            "Training Step: 84531  | time: 358.872s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0676248/1021636\n",
            "Training Step: 84532  | time: 358.876s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0676256/1021636\n",
            "Training Step: 84533  | time: 358.880s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0676264/1021636\n",
            "Training Step: 84534  | time: 358.884s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0676272/1021636\n",
            "Training Step: 84535  | time: 358.888s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0676280/1021636\n",
            "Training Step: 84536  | time: 358.892s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0676288/1021636\n",
            "Training Step: 84537  | time: 358.895s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0676296/1021636\n",
            "Training Step: 84538  | time: 358.897s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0676304/1021636\n",
            "Training Step: 84539  | time: 358.902s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0676312/1021636\n",
            "Training Step: 84540  | time: 358.907s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0676320/1021636\n",
            "Training Step: 84541  | time: 358.910s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0676328/1021636\n",
            "Training Step: 84542  | time: 358.913s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0676336/1021636\n",
            "Training Step: 84543  | time: 358.917s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0676344/1021636\n",
            "Training Step: 84544  | time: 358.920s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0676352/1021636\n",
            "Training Step: 84545  | time: 358.923s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0676360/1021636\n",
            "Training Step: 84546  | time: 358.926s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0676368/1021636\n",
            "Training Step: 84547  | time: 358.928s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0676376/1021636\n",
            "Training Step: 84548  | time: 358.931s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0676384/1021636\n",
            "Training Step: 84549  | time: 358.933s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0676392/1021636\n",
            "Training Step: 84550  | time: 358.936s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0676400/1021636\n",
            "Training Step: 84551  | time: 358.939s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0676408/1021636\n",
            "Training Step: 84552  | time: 358.941s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0676416/1021636\n",
            "Training Step: 84553  | time: 358.944s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0676424/1021636\n",
            "Training Step: 84554  | time: 358.946s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0676432/1021636\n",
            "Training Step: 84555  | time: 358.949s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0676440/1021636\n",
            "Training Step: 84556  | time: 358.953s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0676448/1021636\n",
            "Training Step: 84557  | time: 358.955s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0676456/1021636\n",
            "Training Step: 84558  | time: 358.958s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0676464/1021636\n",
            "Training Step: 84559  | time: 358.961s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0676472/1021636\n",
            "Training Step: 84560  | time: 358.964s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0676480/1021636\n",
            "Training Step: 84561  | time: 358.967s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0676488/1021636\n",
            "Training Step: 84562  | time: 358.970s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0676496/1021636\n",
            "Training Step: 84563  | time: 358.973s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0676504/1021636\n",
            "Training Step: 84564  | time: 358.976s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0676512/1021636\n",
            "Training Step: 84565  | time: 358.979s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0676520/1021636\n",
            "Training Step: 84566  | time: 358.982s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0676528/1021636\n",
            "Training Step: 84567  | time: 358.985s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0676536/1021636\n",
            "Training Step: 84568  | time: 358.988s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0676544/1021636\n",
            "Training Step: 84569  | time: 358.991s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0676552/1021636\n",
            "Training Step: 84570  | time: 358.994s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0676560/1021636\n",
            "Training Step: 84571  | time: 358.997s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0676568/1021636\n",
            "Training Step: 84572  | time: 358.999s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0676576/1021636\n",
            "Training Step: 84573  | time: 359.003s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0676584/1021636\n",
            "Training Step: 84574  | time: 359.006s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0676592/1021636\n",
            "Training Step: 84575  | time: 359.008s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0676600/1021636\n",
            "Training Step: 84576  | time: 359.011s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0676608/1021636\n",
            "Training Step: 84577  | time: 359.014s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0676616/1021636\n",
            "Training Step: 84578  | time: 359.017s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0676624/1021636\n",
            "Training Step: 84579  | time: 359.020s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0676632/1021636\n",
            "Training Step: 84580  | time: 359.022s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0676640/1021636\n",
            "Training Step: 84581  | time: 359.026s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0676648/1021636\n",
            "Training Step: 84582  | time: 359.029s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0676656/1021636\n",
            "Training Step: 84583  | time: 359.032s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0676664/1021636\n",
            "Training Step: 84584  | time: 359.035s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0676672/1021636\n",
            "Training Step: 84585  | time: 359.038s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0676680/1021636\n",
            "Training Step: 84586  | time: 359.041s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0676688/1021636\n",
            "Training Step: 84587  | time: 359.044s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0676696/1021636\n",
            "Training Step: 84588  | time: 359.047s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0676704/1021636\n",
            "Training Step: 84589  | time: 359.050s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0676712/1021636\n",
            "Training Step: 84590  | time: 359.053s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0676720/1021636\n",
            "Training Step: 84591  | time: 359.056s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0676728/1021636\n",
            "Training Step: 84592  | time: 359.059s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0676736/1021636\n",
            "Training Step: 84593  | time: 359.062s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0676744/1021636\n",
            "Training Step: 84594  | time: 359.065s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0676752/1021636\n",
            "Training Step: 84595  | time: 359.069s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0676760/1021636\n",
            "Training Step: 84596  | time: 359.071s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0676768/1021636\n",
            "Training Step: 84597  | time: 359.074s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0676776/1021636\n",
            "Training Step: 84598  | time: 359.078s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0676784/1021636\n",
            "Training Step: 84599  | time: 359.081s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0676792/1021636\n",
            "Training Step: 84600  | time: 359.084s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0676800/1021636\n",
            "Training Step: 84601  | time: 359.087s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0676808/1021636\n",
            "Training Step: 84602  | time: 359.091s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0676816/1021636\n",
            "Training Step: 84603  | time: 359.097s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0676824/1021636\n",
            "Training Step: 84604  | time: 359.100s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0676832/1021636\n",
            "Training Step: 84605  | time: 359.103s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0676840/1021636\n",
            "Training Step: 84606  | time: 359.159s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0676848/1021636\n",
            "Training Step: 84607  | time: 359.164s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0676856/1021636\n",
            "Training Step: 84608  | time: 359.168s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0676864/1021636\n",
            "Training Step: 84609  | time: 359.174s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0676872/1021636\n",
            "Training Step: 84610  | time: 359.180s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0676880/1021636\n",
            "Training Step: 84611  | time: 359.185s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0676888/1021636\n",
            "Training Step: 84612  | time: 359.189s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0676896/1021636\n",
            "Training Step: 84613  | time: 359.192s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0676904/1021636\n",
            "Training Step: 84614  | time: 359.197s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0676912/1021636\n",
            "Training Step: 84615  | time: 359.201s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0676920/1021636\n",
            "Training Step: 84616  | time: 359.205s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0676928/1021636\n",
            "Training Step: 84617  | time: 359.209s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0676936/1021636\n",
            "Training Step: 84618  | time: 359.213s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0676944/1021636\n",
            "Training Step: 84619  | time: 359.217s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0676952/1021636\n",
            "Training Step: 84620  | time: 359.221s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0676960/1021636\n",
            "Training Step: 84621  | time: 359.224s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0676968/1021636\n",
            "Training Step: 84622  | time: 359.227s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0676976/1021636\n",
            "Training Step: 84623  | time: 359.230s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0676984/1021636\n",
            "Training Step: 84624  | time: 359.233s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0676992/1021636\n",
            "Training Step: 84625  | time: 359.236s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0677000/1021636\n",
            "Training Step: 84626  | time: 359.239s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0677008/1021636\n",
            "Training Step: 84627  | time: 359.243s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0677016/1021636\n",
            "Training Step: 84628  | time: 359.246s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0677024/1021636\n",
            "Training Step: 84629  | time: 359.250s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0677032/1021636\n",
            "Training Step: 84630  | time: 359.254s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0677040/1021636\n",
            "Training Step: 84631  | time: 359.257s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0677048/1021636\n",
            "Training Step: 84632  | time: 359.261s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0677056/1021636\n",
            "Training Step: 84633  | time: 359.264s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0677064/1021636\n",
            "Training Step: 84634  | time: 359.268s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0677072/1021636\n",
            "Training Step: 84635  | time: 359.271s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0677080/1021636\n",
            "Training Step: 84636  | time: 359.275s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0677088/1021636\n",
            "Training Step: 84637  | time: 359.278s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0677096/1021636\n",
            "Training Step: 84638  | time: 359.282s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0677104/1021636\n",
            "Training Step: 84639  | time: 359.287s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0677112/1021636\n",
            "Training Step: 84640  | time: 359.293s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0677120/1021636\n",
            "Training Step: 84641  | time: 359.296s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0677128/1021636\n",
            "Training Step: 84642  | time: 359.299s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0677136/1021636\n",
            "Training Step: 84643  | time: 359.303s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0677144/1021636\n",
            "Training Step: 84644  | time: 359.312s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0677152/1021636\n",
            "Training Step: 84645  | time: 359.317s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0677160/1021636\n",
            "Training Step: 84646  | time: 359.321s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0677168/1021636\n",
            "Training Step: 84647  | time: 359.331s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0677176/1021636\n",
            "Training Step: 84648  | time: 359.337s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0677184/1021636\n",
            "Training Step: 84649  | time: 359.341s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0677192/1021636\n",
            "Training Step: 84650  | time: 359.345s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0677200/1021636\n",
            "Training Step: 84651  | time: 359.350s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0677208/1021636\n",
            "Training Step: 84652  | time: 359.355s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0677216/1021636\n",
            "Training Step: 84653  | time: 359.358s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0677224/1021636\n",
            "Training Step: 84654  | time: 359.362s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0677232/1021636\n",
            "Training Step: 84655  | time: 359.367s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0677240/1021636\n",
            "Training Step: 84656  | time: 359.372s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0677248/1021636\n",
            "Training Step: 84657  | time: 359.378s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0677256/1021636\n",
            "Training Step: 84658  | time: 359.387s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0677264/1021636\n",
            "Training Step: 84659  | time: 359.394s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0677272/1021636\n",
            "Training Step: 84660  | time: 359.399s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0677280/1021636\n",
            "Training Step: 84661  | time: 359.402s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0677288/1021636\n",
            "Training Step: 84662  | time: 359.405s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0677296/1021636\n",
            "Training Step: 84663  | time: 359.409s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0677304/1021636\n",
            "Training Step: 84664  | time: 359.413s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0677312/1021636\n",
            "Training Step: 84665  | time: 359.417s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0677320/1021636\n",
            "Training Step: 84666  | time: 359.422s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0677328/1021636\n",
            "Training Step: 84667  | time: 359.426s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0677336/1021636\n",
            "Training Step: 84668  | time: 359.430s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0677344/1021636\n",
            "Training Step: 84669  | time: 359.435s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0677352/1021636\n",
            "Training Step: 84670  | time: 359.441s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0677360/1021636\n",
            "Training Step: 84671  | time: 359.445s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0677368/1021636\n",
            "Training Step: 84672  | time: 359.449s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0677376/1021636\n",
            "Training Step: 84673  | time: 359.456s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0677384/1021636\n",
            "Training Step: 84674  | time: 359.460s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0677392/1021636\n",
            "Training Step: 84675  | time: 359.462s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0677400/1021636\n",
            "Training Step: 84676  | time: 359.467s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0677408/1021636\n",
            "Training Step: 84677  | time: 359.473s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0677416/1021636\n",
            "Training Step: 84678  | time: 359.477s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0677424/1021636\n",
            "Training Step: 84679  | time: 359.480s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0677432/1021636\n",
            "Training Step: 84680  | time: 359.485s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0677440/1021636\n",
            "Training Step: 84681  | time: 359.490s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0677448/1021636\n",
            "Training Step: 84682  | time: 359.494s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0677456/1021636\n",
            "Training Step: 84683  | time: 359.498s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0677464/1021636\n",
            "Training Step: 84684  | time: 359.501s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0677472/1021636\n",
            "Training Step: 84685  | time: 359.507s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0677480/1021636\n",
            "Training Step: 84686  | time: 359.510s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0677488/1021636\n",
            "Training Step: 84687  | time: 359.513s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0677496/1021636\n",
            "Training Step: 84688  | time: 359.518s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0677504/1021636\n",
            "Training Step: 84689  | time: 359.521s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0677512/1021636\n",
            "Training Step: 84690  | time: 359.524s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0677520/1021636\n",
            "Training Step: 84691  | time: 359.527s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0677528/1021636\n",
            "Training Step: 84692  | time: 359.529s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0677536/1021636\n",
            "Training Step: 84693  | time: 359.532s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0677544/1021636\n",
            "Training Step: 84694  | time: 359.535s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0677552/1021636\n",
            "Training Step: 84695  | time: 359.539s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0677560/1021636\n",
            "Training Step: 84696  | time: 359.544s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0677568/1021636\n",
            "Training Step: 84697  | time: 359.549s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0677576/1021636\n",
            "Training Step: 84698  | time: 359.553s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0677584/1021636\n",
            "Training Step: 84699  | time: 359.557s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0677592/1021636\n",
            "Training Step: 84700  | time: 359.561s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0677600/1021636\n",
            "Training Step: 84701  | time: 359.566s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0677608/1021636\n",
            "Training Step: 84702  | time: 359.571s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0677616/1021636\n",
            "Training Step: 84703  | time: 359.575s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0677624/1021636\n",
            "Training Step: 84704  | time: 359.579s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0677632/1021636\n",
            "Training Step: 84705  | time: 359.583s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0677640/1021636\n",
            "Training Step: 84706  | time: 359.586s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0677648/1021636\n",
            "Training Step: 84707  | time: 359.591s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0677656/1021636\n",
            "Training Step: 84708  | time: 359.596s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0677664/1021636\n",
            "Training Step: 84709  | time: 359.601s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0677672/1021636\n",
            "Training Step: 84710  | time: 359.606s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0677680/1021636\n",
            "Training Step: 84711  | time: 359.611s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0677688/1021636\n",
            "Training Step: 84712  | time: 359.614s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0677696/1021636\n",
            "Training Step: 84713  | time: 359.617s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0677704/1021636\n",
            "Training Step: 84714  | time: 359.621s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0677712/1021636\n",
            "Training Step: 84715  | time: 359.625s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0677720/1021636\n",
            "Training Step: 84716  | time: 359.629s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0677728/1021636\n",
            "Training Step: 84717  | time: 359.634s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0677736/1021636\n",
            "Training Step: 84718  | time: 359.639s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0677744/1021636\n",
            "Training Step: 84719  | time: 359.644s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0677752/1021636\n",
            "Training Step: 84720  | time: 359.648s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0677760/1021636\n",
            "Training Step: 84721  | time: 359.652s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0677768/1021636\n",
            "Training Step: 84722  | time: 359.656s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0677776/1021636\n",
            "Training Step: 84723  | time: 359.664s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0677784/1021636\n",
            "Training Step: 84724  | time: 359.667s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0677792/1021636\n",
            "Training Step: 84725  | time: 359.672s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0677800/1021636\n",
            "Training Step: 84726  | time: 359.676s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0677808/1021636\n",
            "Training Step: 84971  | time: 360.717s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0679768/1021636\n",
            "Training Step: 84972  | time: 360.723s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0679776/1021636\n",
            "Training Step: 84973  | time: 360.727s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0679784/1021636\n",
            "Training Step: 84974  | time: 360.731s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0679792/1021636\n",
            "Training Step: 84975  | time: 360.736s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0679800/1021636\n",
            "Training Step: 84976  | time: 360.740s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0679808/1021636\n",
            "Training Step: 84977  | time: 360.744s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0679816/1021636\n",
            "Training Step: 84978  | time: 360.747s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0679824/1021636\n",
            "Training Step: 84979  | time: 360.752s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0679832/1021636\n",
            "Training Step: 84980  | time: 360.759s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0679840/1021636\n",
            "Training Step: 84981  | time: 360.766s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0679848/1021636\n",
            "Training Step: 84982  | time: 360.774s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0679856/1021636\n",
            "Training Step: 84983  | time: 360.780s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0679864/1021636\n",
            "Training Step: 84984  | time: 360.784s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0679872/1021636\n",
            "Training Step: 84985  | time: 360.788s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0679880/1021636\n",
            "Training Step: 84986  | time: 360.792s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0679888/1021636\n",
            "Training Step: 84987  | time: 360.796s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0679896/1021636\n",
            "Training Step: 84988  | time: 360.800s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0679904/1021636\n",
            "Training Step: 84989  | time: 360.805s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0679912/1021636\n",
            "Training Step: 84990  | time: 360.809s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0679920/1021636\n",
            "Training Step: 84991  | time: 360.816s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0679928/1021636\n",
            "Training Step: 84992  | time: 360.820s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0679936/1021636\n",
            "Training Step: 84993  | time: 360.826s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0679944/1021636\n",
            "Training Step: 84994  | time: 360.832s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0679952/1021636\n",
            "Training Step: 84995  | time: 360.837s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0679960/1021636\n",
            "Training Step: 84996  | time: 360.841s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0679968/1021636\n",
            "Training Step: 84997  | time: 360.844s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0679976/1021636\n",
            "Training Step: 84998  | time: 360.849s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0679984/1021636\n",
            "Training Step: 84999  | time: 360.854s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0679992/1021636\n",
            "Training Step: 85000  | time: 360.856s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0680000/1021636\n",
            "Training Step: 85001  | time: 360.860s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0680008/1021636\n",
            "Training Step: 85002  | time: 360.864s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0680016/1021636\n",
            "Training Step: 85003  | time: 360.870s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0680024/1021636\n",
            "Training Step: 85004  | time: 360.874s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0680032/1021636\n",
            "Training Step: 85005  | time: 360.879s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0680040/1021636\n",
            "Training Step: 85006  | time: 360.883s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0680048/1021636\n",
            "Training Step: 85007  | time: 360.887s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0680056/1021636\n",
            "Training Step: 85008  | time: 360.893s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0680064/1021636\n",
            "Training Step: 85009  | time: 360.898s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0680072/1021636\n",
            "Training Step: 85010  | time: 360.902s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0680080/1021636\n",
            "Training Step: 85011  | time: 360.906s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0680088/1021636\n",
            "Training Step: 85012  | time: 360.910s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0680096/1021636\n",
            "Training Step: 85013  | time: 360.914s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0680104/1021636\n",
            "Training Step: 85014  | time: 360.918s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0680112/1021636\n",
            "Training Step: 85015  | time: 360.922s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0680120/1021636\n",
            "Training Step: 85016  | time: 360.927s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0680128/1021636\n",
            "Training Step: 85017  | time: 360.931s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0680136/1021636\n",
            "Training Step: 85018  | time: 360.935s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0680144/1021636\n",
            "Training Step: 85019  | time: 360.940s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0680152/1021636\n",
            "Training Step: 85020  | time: 360.945s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0680160/1021636\n",
            "Training Step: 85021  | time: 360.949s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0680168/1021636\n",
            "Training Step: 85022  | time: 360.954s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0680176/1021636\n",
            "Training Step: 85023  | time: 360.958s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0680184/1021636\n",
            "Training Step: 85024  | time: 360.963s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0680192/1021636\n",
            "Training Step: 85025  | time: 360.967s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0680200/1021636\n",
            "Training Step: 85026  | time: 360.972s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0680208/1021636\n",
            "Training Step: 85027  | time: 360.976s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0680216/1021636\n",
            "Training Step: 85028  | time: 360.980s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0680224/1021636\n",
            "Training Step: 85029  | time: 360.984s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0680232/1021636\n",
            "Training Step: 85030  | time: 360.988s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0680240/1021636\n",
            "Training Step: 85031  | time: 360.992s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0680248/1021636\n",
            "Training Step: 85032  | time: 360.996s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0680256/1021636\n",
            "Training Step: 85033  | time: 361.000s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0680264/1021636\n",
            "Training Step: 85034  | time: 361.005s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0680272/1021636\n",
            "Training Step: 85035  | time: 361.009s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0680280/1021636\n",
            "Training Step: 85036  | time: 361.013s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0680288/1021636\n",
            "Training Step: 85037  | time: 361.018s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0680296/1021636\n",
            "Training Step: 85038  | time: 361.022s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0680304/1021636\n",
            "Training Step: 85039  | time: 361.026s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0680312/1021636\n",
            "Training Step: 85040  | time: 361.030s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0680320/1021636\n",
            "Training Step: 85041  | time: 361.034s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0680328/1021636\n",
            "Training Step: 85042  | time: 361.040s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0680336/1021636\n",
            "Training Step: 85043  | time: 361.043s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0680344/1021636\n",
            "Training Step: 85044  | time: 361.047s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0680352/1021636\n",
            "Training Step: 85045  | time: 361.051s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0680360/1021636\n",
            "Training Step: 85046  | time: 361.055s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0680368/1021636\n",
            "Training Step: 85047  | time: 361.060s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0680376/1021636\n",
            "Training Step: 85048  | time: 361.064s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0680384/1021636\n",
            "Training Step: 85049  | time: 361.067s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0680392/1021636\n",
            "Training Step: 85050  | time: 361.071s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0680400/1021636\n",
            "Training Step: 85051  | time: 361.076s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0680408/1021636\n",
            "Training Step: 85052  | time: 361.080s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0680416/1021636\n",
            "Training Step: 85053  | time: 361.084s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0680424/1021636\n",
            "Training Step: 85054  | time: 361.090s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0680432/1021636\n",
            "Training Step: 85055  | time: 361.095s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0680440/1021636\n",
            "Training Step: 85056  | time: 361.098s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0680448/1021636\n",
            "Training Step: 85057  | time: 361.102s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0680456/1021636\n",
            "Training Step: 85058  | time: 361.106s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0680464/1021636\n",
            "Training Step: 85059  | time: 361.110s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0680472/1021636\n",
            "Training Step: 85060  | time: 361.115s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0680480/1021636\n",
            "Training Step: 85061  | time: 361.120s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0680488/1021636\n",
            "Training Step: 85062  | time: 361.124s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0680496/1021636\n",
            "Training Step: 85063  | time: 361.127s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0680504/1021636\n",
            "Training Step: 85064  | time: 361.131s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0680512/1021636\n",
            "Training Step: 85065  | time: 361.135s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0680520/1021636\n",
            "Training Step: 85066  | time: 361.139s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0680528/1021636\n",
            "Training Step: 85067  | time: 361.143s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0680536/1021636\n",
            "Training Step: 85068  | time: 361.146s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0680544/1021636\n",
            "Training Step: 85069  | time: 361.152s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0680552/1021636\n",
            "Training Step: 85070  | time: 361.156s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0680560/1021636\n",
            "Training Step: 85071  | time: 361.162s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0680568/1021636\n",
            "Training Step: 85072  | time: 361.165s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0680576/1021636\n",
            "Training Step: 85073  | time: 361.170s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0680584/1021636\n",
            "Training Step: 85074  | time: 361.174s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0680592/1021636\n",
            "Training Step: 85075  | time: 361.178s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0680600/1021636\n",
            "Training Step: 85076  | time: 361.182s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0680608/1021636\n",
            "Training Step: 85077  | time: 361.186s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0680616/1021636\n",
            "Training Step: 85078  | time: 361.190s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0680624/1021636\n",
            "Training Step: 85079  | time: 361.194s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0680632/1021636\n",
            "Training Step: 85080  | time: 361.196s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0680640/1021636\n",
            "Training Step: 85081  | time: 361.200s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0680648/1021636\n",
            "Training Step: 85082  | time: 361.203s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0680656/1021636\n",
            "Training Step: 85083  | time: 361.207s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0680664/1021636\n",
            "Training Step: 85084  | time: 361.210s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0680672/1021636\n",
            "Training Step: 85085  | time: 361.216s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0680680/1021636\n",
            "Training Step: 85086  | time: 361.221s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0680688/1021636\n",
            "Training Step: 85087  | time: 361.225s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0680696/1021636\n",
            "Training Step: 85088  | time: 361.229s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0680704/1021636\n",
            "Training Step: 85089  | time: 361.234s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0680712/1021636\n",
            "Training Step: 85090  | time: 361.239s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0680720/1021636\n",
            "Training Step: 85091  | time: 361.242s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0680728/1021636\n",
            "Training Step: 85092  | time: 361.247s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0680736/1021636\n",
            "Training Step: 85093  | time: 361.250s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0680744/1021636\n",
            "Training Step: 85094  | time: 361.255s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0680752/1021636\n",
            "Training Step: 85095  | time: 361.258s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0680760/1021636\n",
            "Training Step: 85096  | time: 361.262s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0680768/1021636\n",
            "Training Step: 85097  | time: 361.266s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0680776/1021636\n",
            "Training Step: 85098  | time: 361.270s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0680784/1021636\n",
            "Training Step: 85099  | time: 361.275s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0680792/1021636\n",
            "Training Step: 85100  | time: 361.279s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0680800/1021636\n",
            "Training Step: 85101  | time: 361.282s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0680808/1021636\n",
            "Training Step: 85102  | time: 361.287s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0680816/1021636\n",
            "Training Step: 85103  | time: 361.291s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0680824/1021636\n",
            "Training Step: 85104  | time: 361.296s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0680832/1021636\n",
            "Training Step: 85105  | time: 361.300s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0680840/1021636\n",
            "Training Step: 85106  | time: 361.304s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0680848/1021636\n",
            "Training Step: 85107  | time: 361.308s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0680856/1021636\n",
            "Training Step: 85108  | time: 361.312s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0680864/1021636\n",
            "Training Step: 85109  | time: 361.315s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0680872/1021636\n",
            "Training Step: 85110  | time: 361.319s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0680880/1021636\n",
            "Training Step: 85111  | time: 361.322s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0680888/1021636\n",
            "Training Step: 85112  | time: 361.325s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0680896/1021636\n",
            "Training Step: 85113  | time: 361.331s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0680904/1021636\n",
            "Training Step: 85114  | time: 361.335s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0680912/1021636\n",
            "Training Step: 85115  | time: 361.340s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0680920/1021636\n",
            "Training Step: 85116  | time: 361.343s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0680928/1021636\n",
            "Training Step: 85117  | time: 361.349s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0680936/1021636\n",
            "Training Step: 85118  | time: 361.353s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0680944/1021636\n",
            "Training Step: 85119  | time: 361.357s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0680952/1021636\n",
            "Training Step: 85120  | time: 361.361s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0680960/1021636\n",
            "Training Step: 85121  | time: 361.365s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0680968/1021636\n",
            "Training Step: 85122  | time: 361.370s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0680976/1021636\n",
            "Training Step: 85123  | time: 361.375s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0680984/1021636\n",
            "Training Step: 85124  | time: 361.379s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0680992/1021636\n",
            "Training Step: 85125  | time: 361.383s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0681000/1021636\n",
            "Training Step: 85126  | time: 361.392s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0681008/1021636\n",
            "Training Step: 85127  | time: 361.396s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0681016/1021636\n",
            "Training Step: 85128  | time: 361.400s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0681024/1021636\n",
            "Training Step: 85129  | time: 361.404s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0681032/1021636\n",
            "Training Step: 85130  | time: 361.408s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0681040/1021636\n",
            "Training Step: 85131  | time: 361.412s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0681048/1021636\n",
            "Training Step: 85132  | time: 361.416s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0681056/1021636\n",
            "Training Step: 85133  | time: 361.419s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0681064/1021636\n",
            "Training Step: 85134  | time: 361.423s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0681072/1021636\n",
            "Training Step: 85135  | time: 361.427s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0681080/1021636\n",
            "Training Step: 85136  | time: 361.430s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0681088/1021636\n",
            "Training Step: 85137  | time: 361.433s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0681096/1021636\n",
            "Training Step: 85138  | time: 361.438s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0681104/1021636\n",
            "Training Step: 85139  | time: 361.442s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0681112/1021636\n",
            "Training Step: 85140  | time: 361.445s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0681120/1021636\n",
            "Training Step: 85141  | time: 361.449s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0681128/1021636\n",
            "Training Step: 85142  | time: 361.453s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0681136/1021636\n",
            "Training Step: 85143  | time: 361.460s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0681144/1021636\n",
            "Training Step: 85144  | time: 361.464s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0681152/1021636\n",
            "Training Step: 85145  | time: 361.468s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0681160/1021636\n",
            "Training Step: 85146  | time: 361.471s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0681168/1021636\n",
            "Training Step: 85147  | time: 361.475s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0681176/1021636\n",
            "Training Step: 85148  | time: 361.479s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0681184/1021636\n",
            "Training Step: 85149  | time: 361.483s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0681192/1021636\n",
            "Training Step: 85150  | time: 361.487s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0681200/1021636\n",
            "Training Step: 85151  | time: 361.491s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0681208/1021636\n",
            "Training Step: 85152  | time: 361.495s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0681216/1021636\n",
            "Training Step: 85153  | time: 361.500s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0681224/1021636\n",
            "Training Step: 85154  | time: 361.505s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0681232/1021636\n",
            "Training Step: 85155  | time: 361.510s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0681240/1021636\n",
            "Training Step: 85156  | time: 361.515s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0681248/1021636\n",
            "Training Step: 85157  | time: 361.518s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0681256/1021636\n",
            "Training Step: 85158  | time: 361.523s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0681264/1021636\n",
            "Training Step: 85159  | time: 361.526s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0681272/1021636\n",
            "Training Step: 85160  | time: 361.531s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0681280/1021636\n",
            "Training Step: 85161  | time: 361.536s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0681288/1021636\n",
            "Training Step: 85162  | time: 361.540s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0681296/1021636\n",
            "Training Step: 85163  | time: 361.545s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0681304/1021636\n",
            "Training Step: 85164  | time: 361.548s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0681312/1021636\n",
            "Training Step: 85165  | time: 361.554s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0681320/1021636\n",
            "Training Step: 85166  | time: 361.557s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0681328/1021636\n",
            "Training Step: 85167  | time: 361.563s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0681336/1021636\n",
            "Training Step: 85168  | time: 361.567s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0681344/1021636\n",
            "Training Step: 85169  | time: 361.572s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0681352/1021636\n",
            "Training Step: 85170  | time: 361.577s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0681360/1021636\n",
            "Training Step: 85171  | time: 361.581s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0681368/1021636\n",
            "Training Step: 85172  | time: 361.584s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0681376/1021636\n",
            "Training Step: 85173  | time: 361.588s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0681384/1021636\n",
            "Training Step: 85174  | time: 361.592s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0681392/1021636\n",
            "Training Step: 85175  | time: 361.596s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0681400/1021636\n",
            "Training Step: 85176  | time: 361.600s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0681408/1021636\n",
            "Training Step: 85177  | time: 361.603s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0681416/1021636\n",
            "Training Step: 85178  | time: 361.607s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0681424/1021636\n",
            "Training Step: 85179  | time: 361.611s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0681432/1021636\n",
            "Training Step: 85180  | time: 361.615s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0681440/1021636\n",
            "Training Step: 85181  | time: 361.618s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0681448/1021636\n",
            "Training Step: 85182  | time: 361.622s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0681456/1021636\n",
            "Training Step: 85183  | time: 361.626s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0681464/1021636\n",
            "Training Step: 85184  | time: 361.630s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0681472/1021636\n",
            "Training Step: 85185  | time: 361.633s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0681480/1021636\n",
            "Training Step: 85186  | time: 361.637s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0681488/1021636\n",
            "Training Step: 85187  | time: 361.641s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0681496/1021636\n",
            "Training Step: 85188  | time: 361.645s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0681504/1021636\n",
            "Training Step: 85189  | time: 361.650s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0681512/1021636\n",
            "Training Step: 85190  | time: 361.655s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0681520/1021636\n",
            "Training Step: 85191  | time: 361.659s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0681528/1021636\n",
            "Training Step: 85192  | time: 361.664s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0681536/1021636\n",
            "Training Step: 85193  | time: 361.668s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0681544/1021636\n",
            "Training Step: 85194  | time: 361.672s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0681552/1021636\n",
            "Training Step: 85195  | time: 361.676s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0681560/1021636\n",
            "Training Step: 85196  | time: 361.679s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0681568/1021636\n",
            "Training Step: 85197  | time: 361.683s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0681576/1021636\n",
            "Training Step: 85198  | time: 361.686s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0681584/1021636\n",
            "Training Step: 85199  | time: 361.691s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0681592/1021636\n",
            "Training Step: 85200  | time: 361.695s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0681600/1021636\n",
            "Training Step: 85201  | time: 361.698s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0681608/1021636\n",
            "Training Step: 85202  | time: 361.703s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0681616/1021636\n",
            "Training Step: 85203  | time: 361.707s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0681624/1021636\n",
            "Training Step: 85204  | time: 361.710s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0681632/1021636\n",
            "Training Step: 85205  | time: 361.714s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0681640/1021636\n",
            "Training Step: 85206  | time: 361.718s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0681648/1021636\n",
            "Training Step: 85207  | time: 361.723s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0681656/1021636\n",
            "Training Step: 85208  | time: 361.728s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0681664/1021636\n",
            "Training Step: 85209  | time: 361.732s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0681672/1021636\n",
            "Training Step: 85210  | time: 361.736s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0681680/1021636\n",
            "Training Step: 85211  | time: 361.741s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0681688/1021636\n",
            "Training Step: 85212  | time: 361.744s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0681696/1021636\n",
            "Training Step: 85213  | time: 361.748s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0681704/1021636\n",
            "Training Step: 85214  | time: 361.752s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0681712/1021636\n",
            "Training Step: 85215  | time: 361.757s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0681720/1021636\n",
            "Training Step: 85216  | time: 361.762s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0681728/1021636\n",
            "Training Step: 85217  | time: 361.766s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0681736/1021636\n",
            "Training Step: 85218  | time: 361.769s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0681744/1021636\n",
            "Training Step: 85219  | time: 361.772s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0681752/1021636\n",
            "Training Step: 85220  | time: 361.777s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0681760/1021636\n",
            "Training Step: 85445  | time: 362.696s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0683560/1021636\n",
            "Training Step: 85446  | time: 362.699s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0683568/1021636\n",
            "Training Step: 85447  | time: 362.703s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0683576/1021636\n",
            "Training Step: 85448  | time: 362.708s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0683584/1021636\n",
            "Training Step: 85449  | time: 362.711s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0683592/1021636\n",
            "Training Step: 85450  | time: 362.714s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0683600/1021636\n",
            "Training Step: 85451  | time: 362.718s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0683608/1021636\n",
            "Training Step: 85452  | time: 362.722s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0683616/1021636\n",
            "Training Step: 85453  | time: 362.727s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0683624/1021636\n",
            "Training Step: 85454  | time: 362.734s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0683632/1021636\n",
            "Training Step: 85455  | time: 362.737s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0683640/1021636\n",
            "Training Step: 85456  | time: 362.740s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0683648/1021636\n",
            "Training Step: 85457  | time: 362.743s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0683656/1021636\n",
            "Training Step: 85458  | time: 362.746s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0683664/1021636\n",
            "Training Step: 85459  | time: 362.756s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0683672/1021636\n",
            "Training Step: 85460  | time: 362.764s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0683680/1021636\n",
            "Training Step: 85461  | time: 362.769s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0683688/1021636\n",
            "Training Step: 85462  | time: 362.777s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0683696/1021636\n",
            "Training Step: 85463  | time: 362.780s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0683704/1021636\n",
            "Training Step: 85464  | time: 362.789s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0683712/1021636\n",
            "Training Step: 85465  | time: 362.792s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0683720/1021636\n",
            "Training Step: 85466  | time: 362.796s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0683728/1021636\n",
            "Training Step: 85467  | time: 362.799s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0683736/1021636\n",
            "Training Step: 85468  | time: 362.803s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0683744/1021636\n",
            "Training Step: 85469  | time: 362.807s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0683752/1021636\n",
            "Training Step: 85470  | time: 362.810s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0683760/1021636\n",
            "Training Step: 85471  | time: 362.814s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0683768/1021636\n",
            "Training Step: 85472  | time: 362.818s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0683776/1021636\n",
            "Training Step: 85473  | time: 362.822s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0683784/1021636\n",
            "Training Step: 85474  | time: 362.826s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0683792/1021636\n",
            "Training Step: 85475  | time: 362.829s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0683800/1021636\n",
            "Training Step: 85476  | time: 362.833s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0683808/1021636\n",
            "Training Step: 85477  | time: 362.836s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0683816/1021636\n",
            "Training Step: 85478  | time: 362.840s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0683824/1021636\n",
            "Training Step: 85479  | time: 362.844s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0683832/1021636\n",
            "Training Step: 85480  | time: 362.848s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0683840/1021636\n",
            "Training Step: 85481  | time: 362.852s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0683848/1021636\n",
            "Training Step: 85482  | time: 362.855s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0683856/1021636\n",
            "Training Step: 85483  | time: 362.859s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0683864/1021636\n",
            "Training Step: 85484  | time: 362.863s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0683872/1021636\n",
            "Training Step: 85485  | time: 362.869s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0683880/1021636\n",
            "Training Step: 85486  | time: 362.873s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0683888/1021636\n",
            "Training Step: 85487  | time: 362.876s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0683896/1021636\n",
            "Training Step: 85488  | time: 362.880s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0683904/1021636\n",
            "Training Step: 85489  | time: 362.885s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0683912/1021636\n",
            "Training Step: 85490  | time: 362.888s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0683920/1021636\n",
            "Training Step: 85491  | time: 362.892s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0683928/1021636\n",
            "Training Step: 85492  | time: 362.895s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0683936/1021636\n",
            "Training Step: 85493  | time: 362.898s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0683944/1021636\n",
            "Training Step: 85494  | time: 362.902s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0683952/1021636\n",
            "Training Step: 85495  | time: 362.906s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0683960/1021636\n",
            "Training Step: 85496  | time: 362.909s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0683968/1021636\n",
            "Training Step: 85497  | time: 362.913s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0683976/1021636\n",
            "Training Step: 85498  | time: 362.916s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0683984/1021636\n",
            "Training Step: 85499  | time: 362.919s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0683992/1021636\n",
            "Training Step: 85500  | time: 362.924s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0684000/1021636\n",
            "Training Step: 85501  | time: 362.928s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0684008/1021636\n",
            "Training Step: 85502  | time: 362.932s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0684016/1021636\n",
            "Training Step: 85503  | time: 362.936s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0684024/1021636\n",
            "Training Step: 85504  | time: 362.941s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0684032/1021636\n",
            "Training Step: 85505  | time: 362.945s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0684040/1021636\n",
            "Training Step: 85506  | time: 362.949s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0684048/1021636\n",
            "Training Step: 85507  | time: 362.953s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0684056/1021636\n",
            "Training Step: 85508  | time: 362.956s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0684064/1021636\n",
            "Training Step: 85509  | time: 362.961s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0684072/1021636\n",
            "Training Step: 85510  | time: 362.964s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0684080/1021636\n",
            "Training Step: 85511  | time: 362.968s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0684088/1021636\n",
            "Training Step: 85512  | time: 362.971s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0684096/1021636\n",
            "Training Step: 85513  | time: 362.975s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0684104/1021636\n",
            "Training Step: 85514  | time: 362.979s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0684112/1021636\n",
            "Training Step: 85515  | time: 362.983s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0684120/1021636\n",
            "Training Step: 85516  | time: 362.986s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0684128/1021636\n",
            "Training Step: 85517  | time: 362.991s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0684136/1021636\n",
            "Training Step: 85518  | time: 362.994s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0684144/1021636\n",
            "Training Step: 85519  | time: 362.998s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0684152/1021636\n",
            "Training Step: 85520  | time: 363.002s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0684160/1021636\n",
            "Training Step: 85521  | time: 363.006s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0684168/1021636\n",
            "Training Step: 85522  | time: 363.009s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0684176/1021636\n",
            "Training Step: 85523  | time: 363.013s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0684184/1021636\n",
            "Training Step: 85524  | time: 363.017s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0684192/1021636\n",
            "Training Step: 85525  | time: 363.021s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0684200/1021636\n",
            "Training Step: 85526  | time: 363.026s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0684208/1021636\n",
            "Training Step: 85527  | time: 363.030s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0684216/1021636\n",
            "Training Step: 85528  | time: 363.035s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0684224/1021636\n",
            "Training Step: 85529  | time: 363.038s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0684232/1021636\n",
            "Training Step: 85530  | time: 363.042s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0684240/1021636\n",
            "Training Step: 85531  | time: 363.045s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0684248/1021636\n",
            "Training Step: 85532  | time: 363.050s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0684256/1021636\n",
            "Training Step: 85533  | time: 363.055s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0684264/1021636\n",
            "Training Step: 85534  | time: 363.060s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0684272/1021636\n",
            "Training Step: 85535  | time: 363.064s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0684280/1021636\n",
            "Training Step: 85536  | time: 363.069s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0684288/1021636\n",
            "Training Step: 85537  | time: 363.073s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0684296/1021636\n",
            "Training Step: 85538  | time: 363.077s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0684304/1021636\n",
            "Training Step: 85539  | time: 363.081s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0684312/1021636\n",
            "Training Step: 85540  | time: 363.085s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0684320/1021636\n",
            "Training Step: 85541  | time: 363.088s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0684328/1021636\n",
            "Training Step: 85542  | time: 363.091s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0684336/1021636\n",
            "Training Step: 85543  | time: 363.095s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0684344/1021636\n",
            "Training Step: 85544  | time: 363.099s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0684352/1021636\n",
            "Training Step: 85545  | time: 363.102s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0684360/1021636\n",
            "Training Step: 85546  | time: 363.106s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0684368/1021636\n",
            "Training Step: 85547  | time: 363.110s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0684376/1021636\n",
            "Training Step: 85548  | time: 363.114s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0684384/1021636\n",
            "Training Step: 85549  | time: 363.118s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0684392/1021636\n",
            "Training Step: 85550  | time: 363.122s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0684400/1021636\n",
            "Training Step: 85551  | time: 363.125s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0684408/1021636\n",
            "Training Step: 85552  | time: 363.129s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0684416/1021636\n",
            "Training Step: 85553  | time: 363.132s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0684424/1021636\n",
            "Training Step: 85554  | time: 363.136s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0684432/1021636\n",
            "Training Step: 85555  | time: 363.140s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0684440/1021636\n",
            "Training Step: 85556  | time: 363.143s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0684448/1021636\n",
            "Training Step: 85557  | time: 363.147s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0684456/1021636\n",
            "Training Step: 85558  | time: 363.151s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0684464/1021636\n",
            "Training Step: 85559  | time: 363.155s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0684472/1021636\n",
            "Training Step: 85560  | time: 363.160s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0684480/1021636\n",
            "Training Step: 85561  | time: 363.163s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0684488/1021636\n",
            "Training Step: 85562  | time: 363.167s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0684496/1021636\n",
            "Training Step: 85563  | time: 363.171s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0684504/1021636\n",
            "Training Step: 85564  | time: 363.175s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0684512/1021636\n",
            "Training Step: 85565  | time: 363.179s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0684520/1021636\n",
            "Training Step: 85566  | time: 363.182s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0684528/1021636\n",
            "Training Step: 85567  | time: 363.186s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0684536/1021636\n",
            "Training Step: 85568  | time: 363.190s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0684544/1021636\n",
            "Training Step: 85569  | time: 363.194s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0684552/1021636\n",
            "Training Step: 85570  | time: 363.198s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0684560/1021636\n",
            "Training Step: 85571  | time: 363.201s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0684568/1021636\n",
            "Training Step: 85572  | time: 363.205s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0684576/1021636\n",
            "Training Step: 85573  | time: 363.208s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0684584/1021636\n",
            "Training Step: 85574  | time: 363.214s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0684592/1021636\n",
            "Training Step: 85575  | time: 363.217s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0684600/1021636\n",
            "Training Step: 85576  | time: 363.222s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0684608/1021636\n",
            "Training Step: 85577  | time: 363.225s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0684616/1021636\n",
            "Training Step: 85578  | time: 363.230s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0684624/1021636\n",
            "Training Step: 85579  | time: 363.234s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0684632/1021636\n",
            "Training Step: 85580  | time: 363.239s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0684640/1021636\n",
            "Training Step: 85581  | time: 363.243s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0684648/1021636\n",
            "Training Step: 85582  | time: 363.249s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0684656/1021636\n",
            "Training Step: 85583  | time: 363.252s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0684664/1021636\n",
            "Training Step: 85584  | time: 363.256s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0684672/1021636\n",
            "Training Step: 85585  | time: 363.260s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0684680/1021636\n",
            "Training Step: 85586  | time: 363.264s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0684688/1021636\n",
            "Training Step: 85587  | time: 363.267s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0684696/1021636\n",
            "Training Step: 85588  | time: 363.271s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0684704/1021636\n",
            "Training Step: 85589  | time: 363.275s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0684712/1021636\n",
            "Training Step: 85590  | time: 363.278s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0684720/1021636\n",
            "Training Step: 85591  | time: 363.282s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0684728/1021636\n",
            "Training Step: 85592  | time: 363.286s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0684736/1021636\n",
            "Training Step: 85593  | time: 363.290s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0684744/1021636\n",
            "Training Step: 85594  | time: 363.293s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0684752/1021636\n",
            "Training Step: 85595  | time: 363.298s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0684760/1021636\n",
            "Training Step: 85596  | time: 363.301s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0684768/1021636\n",
            "Training Step: 85597  | time: 363.306s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0684776/1021636\n",
            "Training Step: 85598  | time: 363.310s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0684784/1021636\n",
            "Training Step: 85599  | time: 363.313s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0684792/1021636\n",
            "Training Step: 85600  | time: 363.317s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0684800/1021636\n",
            "Training Step: 85601  | time: 363.321s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0684808/1021636\n",
            "Training Step: 85602  | time: 363.326s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0684816/1021636\n",
            "Training Step: 85603  | time: 363.331s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0684824/1021636\n",
            "Training Step: 85604  | time: 363.335s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0684832/1021636\n",
            "Training Step: 85605  | time: 363.339s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0684840/1021636\n",
            "Training Step: 85606  | time: 363.343s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0684848/1021636\n",
            "Training Step: 85607  | time: 363.347s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0684856/1021636\n",
            "Training Step: 85608  | time: 363.351s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0684864/1021636\n",
            "Training Step: 85609  | time: 363.355s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0684872/1021636\n",
            "Training Step: 85610  | time: 363.359s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0684880/1021636\n",
            "Training Step: 85611  | time: 363.363s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0684888/1021636\n",
            "Training Step: 85612  | time: 363.367s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0684896/1021636\n",
            "Training Step: 85613  | time: 363.371s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0684904/1021636\n",
            "Training Step: 85614  | time: 363.376s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0684912/1021636\n",
            "Training Step: 85615  | time: 363.379s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0684920/1021636\n",
            "Training Step: 85616  | time: 363.384s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0684928/1021636\n",
            "Training Step: 85617  | time: 363.389s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0684936/1021636\n",
            "Training Step: 85618  | time: 363.392s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0684944/1021636\n",
            "Training Step: 85619  | time: 363.396s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0684952/1021636\n",
            "Training Step: 85620  | time: 363.400s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0684960/1021636\n",
            "Training Step: 85621  | time: 363.404s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0684968/1021636\n",
            "Training Step: 85622  | time: 363.409s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0684976/1021636\n",
            "Training Step: 85623  | time: 363.412s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0684984/1021636\n",
            "Training Step: 85624  | time: 363.416s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0684992/1021636\n",
            "Training Step: 85625  | time: 363.419s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0685000/1021636\n",
            "Training Step: 85626  | time: 363.424s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0685008/1021636\n",
            "Training Step: 85627  | time: 363.427s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0685016/1021636\n",
            "Training Step: 85628  | time: 363.432s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0685024/1021636\n",
            "Training Step: 85629  | time: 363.436s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0685032/1021636\n",
            "Training Step: 85630  | time: 363.440s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0685040/1021636\n",
            "Training Step: 85631  | time: 363.445s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0685048/1021636\n",
            "Training Step: 85632  | time: 363.449s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0685056/1021636\n",
            "Training Step: 85633  | time: 363.453s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0685064/1021636\n",
            "Training Step: 85634  | time: 363.458s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0685072/1021636\n",
            "Training Step: 85635  | time: 363.462s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0685080/1021636\n",
            "Training Step: 85636  | time: 363.466s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0685088/1021636\n",
            "Training Step: 85637  | time: 363.471s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0685096/1021636\n",
            "Training Step: 85638  | time: 363.475s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0685104/1021636\n",
            "Training Step: 85639  | time: 363.478s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0685112/1021636\n",
            "Training Step: 85640  | time: 363.481s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0685120/1021636\n",
            "Training Step: 85641  | time: 363.485s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0685128/1021636\n",
            "Training Step: 85642  | time: 363.488s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0685136/1021636\n",
            "Training Step: 85643  | time: 363.492s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0685144/1021636\n",
            "Training Step: 85644  | time: 363.496s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0685152/1021636\n",
            "Training Step: 85645  | time: 363.499s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0685160/1021636\n",
            "Training Step: 85646  | time: 363.504s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0685168/1021636\n",
            "Training Step: 85647  | time: 363.508s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0685176/1021636\n",
            "Training Step: 85648  | time: 363.513s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0685184/1021636\n",
            "Training Step: 85649  | time: 363.516s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0685192/1021636\n",
            "Training Step: 85650  | time: 363.521s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0685200/1021636\n",
            "Training Step: 85651  | time: 363.525s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0685208/1021636\n",
            "Training Step: 85652  | time: 363.529s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0685216/1021636\n",
            "Training Step: 85653  | time: 363.533s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0685224/1021636\n",
            "Training Step: 85654  | time: 363.536s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0685232/1021636\n",
            "Training Step: 85655  | time: 363.540s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0685240/1021636\n",
            "Training Step: 85656  | time: 363.544s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0685248/1021636\n",
            "Training Step: 85657  | time: 363.548s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0685256/1021636\n",
            "Training Step: 85658  | time: 363.552s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0685264/1021636\n",
            "Training Step: 85659  | time: 363.556s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0685272/1021636\n",
            "Training Step: 85660  | time: 363.560s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0685280/1021636\n",
            "Training Step: 85661  | time: 363.564s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0685288/1021636\n",
            "Training Step: 85662  | time: 363.568s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0685296/1021636\n",
            "Training Step: 85663  | time: 363.572s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0685304/1021636\n",
            "Training Step: 85664  | time: 363.577s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0685312/1021636\n",
            "Training Step: 85665  | time: 363.581s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0685320/1021636\n",
            "Training Step: 85666  | time: 363.585s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0685328/1021636\n",
            "Training Step: 85667  | time: 363.590s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0685336/1021636\n",
            "Training Step: 85668  | time: 363.596s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0685344/1021636\n",
            "Training Step: 85669  | time: 363.601s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0685352/1021636\n",
            "Training Step: 85670  | time: 363.605s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0685360/1021636\n",
            "Training Step: 85671  | time: 363.608s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0685368/1021636\n",
            "Training Step: 85672  | time: 363.612s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0685376/1021636\n",
            "Training Step: 85673  | time: 363.618s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0685384/1021636\n",
            "Training Step: 85674  | time: 363.622s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0685392/1021636\n",
            "Training Step: 85675  | time: 363.626s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0685400/1021636\n",
            "Training Step: 85676  | time: 363.630s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0685408/1021636\n",
            "Training Step: 85677  | time: 363.634s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0685416/1021636\n",
            "Training Step: 85678  | time: 363.637s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0685424/1021636\n",
            "Training Step: 85679  | time: 363.641s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0685432/1021636\n",
            "Training Step: 85680  | time: 363.645s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0685440/1021636\n",
            "Training Step: 85681  | time: 363.649s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0685448/1021636\n",
            "Training Step: 85682  | time: 363.653s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0685456/1021636\n",
            "Training Step: 85683  | time: 363.657s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0685464/1021636\n",
            "Training Step: 85684  | time: 363.661s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0685472/1021636\n",
            "Training Step: 85685  | time: 363.664s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0685480/1021636\n",
            "Training Step: 85686  | time: 363.668s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0685488/1021636\n",
            "Training Step: 85687  | time: 363.673s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0685496/1021636\n",
            "Training Step: 85688  | time: 363.676s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0685504/1021636\n",
            "Training Step: 85689  | time: 363.681s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0685512/1021636\n",
            "Training Step: 85690  | time: 363.684s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0685520/1021636\n",
            "Training Step: 85691  | time: 363.688s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0685528/1021636\n",
            "Training Step: 85692  | time: 363.692s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0685536/1021636\n",
            "Training Step: 85693  | time: 363.697s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0685544/1021636\n",
            "Training Step: 85694  | time: 363.702s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0685552/1021636\n",
            "Training Step: 85737  | time: 363.948s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0685896/1021636\n",
            "Training Step: 85738  | time: 363.953s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0685904/1021636\n",
            "Training Step: 85739  | time: 363.958s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0685912/1021636\n",
            "Training Step: 85740  | time: 363.963s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0685920/1021636\n",
            "Training Step: 85741  | time: 363.967s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0685928/1021636\n",
            "Training Step: 85742  | time: 363.973s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0685936/1021636\n",
            "Training Step: 85743  | time: 363.979s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0685944/1021636\n",
            "Training Step: 85744  | time: 363.983s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0685952/1021636\n",
            "Training Step: 85745  | time: 363.987s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0685960/1021636\n",
            "Training Step: 85746  | time: 363.990s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0685968/1021636\n",
            "Training Step: 85747  | time: 363.998s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0685976/1021636\n",
            "Training Step: 85748  | time: 364.003s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0685984/1021636\n",
            "Training Step: 85749  | time: 364.006s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0685992/1021636\n",
            "Training Step: 85750  | time: 364.010s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0686000/1021636\n",
            "Training Step: 85751  | time: 364.013s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0686008/1021636\n",
            "Training Step: 85752  | time: 364.018s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0686016/1021636\n",
            "Training Step: 85753  | time: 364.025s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0686024/1021636\n",
            "Training Step: 85754  | time: 364.029s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0686032/1021636\n",
            "Training Step: 85755  | time: 364.034s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0686040/1021636\n",
            "Training Step: 85756  | time: 364.038s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0686048/1021636\n",
            "Training Step: 85757  | time: 364.042s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0686056/1021636\n",
            "Training Step: 85758  | time: 364.047s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0686064/1021636\n",
            "Training Step: 85759  | time: 364.051s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0686072/1021636\n",
            "Training Step: 85760  | time: 364.054s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0686080/1021636\n",
            "Training Step: 85761  | time: 364.058s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0686088/1021636\n",
            "Training Step: 85762  | time: 364.062s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0686096/1021636\n",
            "Training Step: 85763  | time: 364.067s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0686104/1021636\n",
            "Training Step: 85764  | time: 364.071s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0686112/1021636\n",
            "Training Step: 85765  | time: 364.075s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0686120/1021636\n",
            "Training Step: 85766  | time: 364.080s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0686128/1021636\n",
            "Training Step: 85767  | time: 364.083s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0686136/1021636\n",
            "Training Step: 85768  | time: 364.087s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0686144/1021636\n",
            "Training Step: 85769  | time: 364.092s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0686152/1021636\n",
            "Training Step: 85770  | time: 364.096s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0686160/1021636\n",
            "Training Step: 85771  | time: 364.102s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0686168/1021636\n",
            "Training Step: 85772  | time: 364.106s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0686176/1021636\n",
            "Training Step: 85773  | time: 364.109s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0686184/1021636\n",
            "Training Step: 85774  | time: 364.113s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0686192/1021636\n",
            "Training Step: 85775  | time: 364.119s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0686200/1021636\n",
            "Training Step: 85776  | time: 364.124s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0686208/1021636\n",
            "Training Step: 85777  | time: 364.130s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0686216/1021636\n",
            "Training Step: 85778  | time: 364.134s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0686224/1021636\n",
            "Training Step: 85779  | time: 364.138s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0686232/1021636\n",
            "Training Step: 85780  | time: 364.141s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0686240/1021636\n",
            "Training Step: 85781  | time: 364.144s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0686248/1021636\n",
            "Training Step: 85782  | time: 364.147s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0686256/1021636\n",
            "Training Step: 85783  | time: 364.153s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0686264/1021636\n",
            "Training Step: 85784  | time: 364.157s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0686272/1021636\n",
            "Training Step: 85785  | time: 364.163s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0686280/1021636\n",
            "Training Step: 85786  | time: 364.169s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0686288/1021636\n",
            "Training Step: 85787  | time: 364.172s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0686296/1021636\n",
            "Training Step: 85788  | time: 364.179s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0686304/1021636\n",
            "Training Step: 85789  | time: 364.182s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0686312/1021636\n",
            "Training Step: 85790  | time: 364.187s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0686320/1021636\n",
            "Training Step: 85791  | time: 364.196s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0686328/1021636\n",
            "Training Step: 85792  | time: 364.202s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0686336/1021636\n",
            "Training Step: 85793  | time: 364.205s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0686344/1021636\n",
            "Training Step: 85794  | time: 364.209s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0686352/1021636\n",
            "Training Step: 85795  | time: 364.214s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0686360/1021636\n",
            "Training Step: 85796  | time: 364.218s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0686368/1021636\n",
            "Training Step: 85797  | time: 364.222s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0686376/1021636\n",
            "Training Step: 85798  | time: 364.226s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0686384/1021636\n",
            "Training Step: 85799  | time: 364.229s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0686392/1021636\n",
            "Training Step: 85800  | time: 364.233s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0686400/1021636\n",
            "Training Step: 85801  | time: 364.236s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0686408/1021636\n",
            "Training Step: 85802  | time: 364.241s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0686416/1021636\n",
            "Training Step: 85803  | time: 364.245s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0686424/1021636\n",
            "Training Step: 85804  | time: 364.249s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0686432/1021636\n",
            "Training Step: 85805  | time: 364.252s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0686440/1021636\n",
            "Training Step: 85806  | time: 364.255s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0686448/1021636\n",
            "Training Step: 85807  | time: 364.260s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0686456/1021636\n",
            "Training Step: 85808  | time: 364.264s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0686464/1021636\n",
            "Training Step: 85809  | time: 364.267s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0686472/1021636\n",
            "Training Step: 85810  | time: 364.270s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0686480/1021636\n",
            "Training Step: 85811  | time: 364.274s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0686488/1021636\n",
            "Training Step: 85812  | time: 364.277s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0686496/1021636\n",
            "Training Step: 85813  | time: 364.280s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0686504/1021636\n",
            "Training Step: 85814  | time: 364.287s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0686512/1021636\n",
            "Training Step: 85815  | time: 364.291s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0686520/1021636\n",
            "Training Step: 85816  | time: 364.294s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0686528/1021636\n",
            "Training Step: 85817  | time: 364.299s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0686536/1021636\n",
            "Training Step: 85818  | time: 364.303s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0686544/1021636\n",
            "Training Step: 85819  | time: 364.307s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0686552/1021636\n",
            "Training Step: 85820  | time: 364.310s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0686560/1021636\n",
            "Training Step: 85821  | time: 364.314s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0686568/1021636\n",
            "Training Step: 85822  | time: 364.319s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0686576/1021636\n",
            "Training Step: 85823  | time: 364.325s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0686584/1021636\n",
            "Training Step: 85824  | time: 364.329s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0686592/1021636\n",
            "Training Step: 85825  | time: 364.331s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0686600/1021636\n",
            "Training Step: 85826  | time: 364.334s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0686608/1021636\n",
            "Training Step: 85827  | time: 364.337s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0686616/1021636\n",
            "Training Step: 85828  | time: 364.339s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0686624/1021636\n",
            "Training Step: 85829  | time: 364.342s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0686632/1021636\n",
            "Training Step: 85830  | time: 364.350s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0686640/1021636\n",
            "Training Step: 85831  | time: 364.356s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0686648/1021636\n",
            "Training Step: 85832  | time: 364.371s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0686656/1021636\n",
            "Training Step: 85833  | time: 364.375s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0686664/1021636\n",
            "Training Step: 85834  | time: 364.380s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0686672/1021636\n",
            "Training Step: 85835  | time: 364.384s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0686680/1021636\n",
            "Training Step: 85836  | time: 364.389s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0686688/1021636\n",
            "Training Step: 85837  | time: 364.392s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0686696/1021636\n",
            "Training Step: 85838  | time: 364.395s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0686704/1021636\n",
            "Training Step: 85839  | time: 364.400s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0686712/1021636\n",
            "Training Step: 85840  | time: 364.404s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0686720/1021636\n",
            "Training Step: 85841  | time: 364.408s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0686728/1021636\n",
            "Training Step: 85842  | time: 364.413s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0686736/1021636\n",
            "Training Step: 85843  | time: 364.417s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0686744/1021636\n",
            "Training Step: 85844  | time: 364.421s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0686752/1021636\n",
            "Training Step: 85845  | time: 364.425s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0686760/1021636\n",
            "Training Step: 85846  | time: 364.428s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0686768/1021636\n",
            "Training Step: 85847  | time: 364.431s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0686776/1021636\n",
            "Training Step: 85848  | time: 364.435s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0686784/1021636\n",
            "Training Step: 85849  | time: 364.438s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0686792/1021636\n",
            "Training Step: 85850  | time: 364.441s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0686800/1021636\n",
            "Training Step: 85851  | time: 364.445s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0686808/1021636\n",
            "Training Step: 85852  | time: 364.448s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0686816/1021636\n",
            "Training Step: 85853  | time: 364.451s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0686824/1021636\n",
            "Training Step: 85854  | time: 364.454s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0686832/1021636\n",
            "Training Step: 85855  | time: 364.458s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0686840/1021636\n",
            "Training Step: 85856  | time: 364.460s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0686848/1021636\n",
            "Training Step: 85857  | time: 364.464s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0686856/1021636\n",
            "Training Step: 85858  | time: 364.467s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0686864/1021636\n",
            "Training Step: 85859  | time: 364.470s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0686872/1021636\n",
            "Training Step: 85860  | time: 364.473s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0686880/1021636\n",
            "Training Step: 85861  | time: 364.476s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0686888/1021636\n",
            "Training Step: 85862  | time: 364.479s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0686896/1021636\n",
            "Training Step: 85863  | time: 364.482s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0686904/1021636\n",
            "Training Step: 85864  | time: 364.485s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0686912/1021636\n",
            "Training Step: 85865  | time: 364.488s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0686920/1021636\n",
            "Training Step: 85866  | time: 364.491s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0686928/1021636\n",
            "Training Step: 85867  | time: 364.494s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0686936/1021636\n",
            "Training Step: 85868  | time: 364.498s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0686944/1021636\n",
            "Training Step: 85869  | time: 364.501s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0686952/1021636\n",
            "Training Step: 85870  | time: 364.504s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0686960/1021636\n",
            "Training Step: 85871  | time: 364.507s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0686968/1021636\n",
            "Training Step: 85872  | time: 364.510s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0686976/1021636\n",
            "Training Step: 85873  | time: 364.513s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0686984/1021636\n",
            "Training Step: 85874  | time: 364.517s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0686992/1021636\n",
            "Training Step: 85875  | time: 364.521s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0687000/1021636\n",
            "Training Step: 85876  | time: 364.524s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0687008/1021636\n",
            "Training Step: 85877  | time: 364.527s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0687016/1021636\n",
            "Training Step: 85878  | time: 364.530s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0687024/1021636\n",
            "Training Step: 85879  | time: 364.533s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0687032/1021636\n",
            "Training Step: 85880  | time: 364.536s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0687040/1021636\n",
            "Training Step: 85881  | time: 364.539s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0687048/1021636\n",
            "Training Step: 85882  | time: 364.542s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0687056/1021636\n",
            "Training Step: 85883  | time: 364.546s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0687064/1021636\n",
            "Training Step: 85884  | time: 364.549s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0687072/1021636\n",
            "Training Step: 85885  | time: 364.552s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0687080/1021636\n",
            "Training Step: 85886  | time: 364.556s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0687088/1021636\n",
            "Training Step: 85887  | time: 364.559s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0687096/1021636\n",
            "Training Step: 85888  | time: 364.562s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0687104/1021636\n",
            "Training Step: 85889  | time: 364.565s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0687112/1021636\n",
            "Training Step: 85890  | time: 364.568s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0687120/1021636\n",
            "Training Step: 85891  | time: 364.571s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0687128/1021636\n",
            "Training Step: 85892  | time: 364.574s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0687136/1021636\n",
            "Training Step: 85893  | time: 364.577s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0687144/1021636\n",
            "Training Step: 85894  | time: 364.580s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0687152/1021636\n",
            "Training Step: 85895  | time: 364.583s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0687160/1021636\n",
            "Training Step: 85896  | time: 364.586s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0687168/1021636\n",
            "Training Step: 85897  | time: 364.590s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0687176/1021636\n",
            "Training Step: 85898  | time: 364.593s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0687184/1021636\n",
            "Training Step: 85899  | time: 364.596s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0687192/1021636\n",
            "Training Step: 85900  | time: 364.599s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0687200/1021636\n",
            "Training Step: 85901  | time: 364.602s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0687208/1021636\n",
            "Training Step: 85902  | time: 364.605s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0687216/1021636\n",
            "Training Step: 85903  | time: 364.609s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0687224/1021636\n",
            "Training Step: 85904  | time: 364.611s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0687232/1021636\n",
            "Training Step: 85905  | time: 364.614s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0687240/1021636\n",
            "Training Step: 85906  | time: 364.623s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0687248/1021636\n",
            "Training Step: 85907  | time: 364.626s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0687256/1021636\n",
            "Training Step: 85908  | time: 364.630s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0687264/1021636\n",
            "Training Step: 85909  | time: 364.633s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0687272/1021636\n",
            "Training Step: 85910  | time: 364.637s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0687280/1021636\n",
            "Training Step: 85911  | time: 364.640s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0687288/1021636\n",
            "Training Step: 85912  | time: 364.643s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0687296/1021636\n",
            "Training Step: 85913  | time: 364.646s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0687304/1021636\n",
            "Training Step: 85914  | time: 364.650s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0687312/1021636\n",
            "Training Step: 85915  | time: 364.653s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0687320/1021636\n",
            "Training Step: 85916  | time: 364.656s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0687328/1021636\n",
            "Training Step: 85917  | time: 364.659s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0687336/1021636\n",
            "Training Step: 85918  | time: 364.662s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0687344/1021636\n",
            "Training Step: 85919  | time: 364.666s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0687352/1021636\n",
            "Training Step: 85920  | time: 364.669s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0687360/1021636\n",
            "Training Step: 85921  | time: 364.672s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0687368/1021636\n",
            "Training Step: 85922  | time: 364.675s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0687376/1021636\n",
            "Training Step: 85923  | time: 364.678s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0687384/1021636\n",
            "Training Step: 85924  | time: 364.681s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0687392/1021636\n",
            "Training Step: 85925  | time: 364.684s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0687400/1021636\n",
            "Training Step: 85926  | time: 364.687s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0687408/1021636\n",
            "Training Step: 85927  | time: 364.690s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0687416/1021636\n",
            "Training Step: 85928  | time: 364.693s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0687424/1021636\n",
            "Training Step: 85929  | time: 364.696s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0687432/1021636\n",
            "Training Step: 85930  | time: 364.702s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0687440/1021636\n",
            "Training Step: 85931  | time: 364.705s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0687448/1021636\n",
            "Training Step: 85932  | time: 364.712s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0687456/1021636\n",
            "Training Step: 85933  | time: 364.715s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0687464/1021636\n",
            "Training Step: 85934  | time: 364.718s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0687472/1021636\n",
            "Training Step: 85935  | time: 364.721s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0687480/1021636\n",
            "Training Step: 85936  | time: 364.724s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0687488/1021636\n",
            "Training Step: 85937  | time: 364.728s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0687496/1021636\n",
            "Training Step: 85938  | time: 364.731s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0687504/1021636\n",
            "Training Step: 85939  | time: 364.734s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0687512/1021636\n",
            "Training Step: 85940  | time: 364.737s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0687520/1021636\n",
            "Training Step: 85941  | time: 364.739s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0687528/1021636\n",
            "Training Step: 85942  | time: 364.742s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0687536/1021636\n",
            "Training Step: 85943  | time: 364.745s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0687544/1021636\n",
            "Training Step: 85944  | time: 364.748s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0687552/1021636\n",
            "Training Step: 85945  | time: 364.751s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0687560/1021636\n",
            "Training Step: 85946  | time: 364.754s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0687568/1021636\n",
            "Training Step: 85947  | time: 364.757s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0687576/1021636\n",
            "Training Step: 85948  | time: 364.760s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0687584/1021636\n",
            "Training Step: 85949  | time: 364.763s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0687592/1021636\n",
            "Training Step: 85950  | time: 364.766s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0687600/1021636\n",
            "Training Step: 85951  | time: 364.769s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0687608/1021636\n",
            "Training Step: 85952  | time: 364.772s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0687616/1021636\n",
            "Training Step: 85953  | time: 364.775s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0687624/1021636\n",
            "Training Step: 85954  | time: 364.778s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0687632/1021636\n",
            "Training Step: 85955  | time: 364.781s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0687640/1021636\n",
            "Training Step: 85956  | time: 364.784s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0687648/1021636\n",
            "Training Step: 85957  | time: 364.787s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0687656/1021636\n",
            "Training Step: 85958  | time: 364.790s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0687664/1021636\n",
            "Training Step: 85959  | time: 364.793s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0687672/1021636\n",
            "Training Step: 85960  | time: 364.796s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0687680/1021636\n",
            "Training Step: 85961  | time: 364.799s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0687688/1021636\n",
            "Training Step: 85962  | time: 364.802s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0687696/1021636\n",
            "Training Step: 85963  | time: 364.805s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0687704/1021636\n",
            "Training Step: 85964  | time: 364.808s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0687712/1021636\n",
            "Training Step: 85965  | time: 364.811s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0687720/1021636\n",
            "Training Step: 85966  | time: 364.815s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0687728/1021636\n",
            "Training Step: 85967  | time: 364.821s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0687736/1021636\n",
            "Training Step: 85968  | time: 364.824s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0687744/1021636\n",
            "Training Step: 85969  | time: 364.830s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0687752/1021636\n",
            "Training Step: 85970  | time: 364.833s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0687760/1021636\n",
            "Training Step: 85971  | time: 364.836s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0687768/1021636\n",
            "Training Step: 85972  | time: 364.839s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0687776/1021636\n",
            "Training Step: 85973  | time: 364.841s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0687784/1021636\n",
            "Training Step: 85974  | time: 364.844s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0687792/1021636\n",
            "Training Step: 85975  | time: 364.847s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0687800/1021636\n",
            "Training Step: 85976  | time: 364.850s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0687808/1021636\n",
            "Training Step: 85977  | time: 364.853s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0687816/1021636\n",
            "Training Step: 85978  | time: 364.856s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0687824/1021636\n",
            "Training Step: 85979  | time: 364.859s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0687832/1021636\n",
            "Training Step: 85980  | time: 364.861s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0687840/1021636\n",
            "Training Step: 85981  | time: 364.865s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0687848/1021636\n",
            "Training Step: 85982  | time: 364.868s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0687856/1021636\n",
            "Training Step: 85983  | time: 364.871s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0687864/1021636\n",
            "Training Step: 85984  | time: 364.876s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0687872/1021636\n",
            "Training Step: 85985  | time: 364.880s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0687880/1021636\n",
            "Training Step: 85986  | time: 364.883s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0687888/1021636\n",
            "Training Step: 86068  | time: 365.253s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0688544/1021636\n",
            "Training Step: 86069  | time: 365.260s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0688552/1021636\n",
            "Training Step: 86070  | time: 365.265s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0688560/1021636\n",
            "Training Step: 86071  | time: 365.269s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0688568/1021636\n",
            "Training Step: 86072  | time: 365.274s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0688576/1021636\n",
            "Training Step: 86073  | time: 365.277s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0688584/1021636\n",
            "Training Step: 86074  | time: 365.280s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0688592/1021636\n",
            "Training Step: 86075  | time: 365.284s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0688600/1021636\n",
            "Training Step: 86076  | time: 365.287s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0688608/1021636\n",
            "Training Step: 86077  | time: 365.290s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0688616/1021636\n",
            "Training Step: 86078  | time: 365.294s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0688624/1021636\n",
            "Training Step: 86079  | time: 365.298s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0688632/1021636\n",
            "Training Step: 86080  | time: 365.302s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0688640/1021636\n",
            "Training Step: 86081  | time: 365.305s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0688648/1021636\n",
            "Training Step: 86082  | time: 365.308s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0688656/1021636\n",
            "Training Step: 86083  | time: 365.316s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0688664/1021636\n",
            "Training Step: 86084  | time: 365.322s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0688672/1021636\n",
            "Training Step: 86085  | time: 365.343s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0688680/1021636\n",
            "Training Step: 86086  | time: 365.346s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0688688/1021636\n",
            "Training Step: 86087  | time: 365.350s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0688696/1021636\n",
            "Training Step: 86088  | time: 365.359s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0688704/1021636\n",
            "Training Step: 86089  | time: 365.367s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0688712/1021636\n",
            "Training Step: 86090  | time: 365.370s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0688720/1021636\n",
            "Training Step: 86091  | time: 365.373s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0688728/1021636\n",
            "Training Step: 86092  | time: 365.376s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0688736/1021636\n",
            "Training Step: 86093  | time: 365.384s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0688744/1021636\n",
            "Training Step: 86094  | time: 365.390s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0688752/1021636\n",
            "Training Step: 86095  | time: 365.394s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0688760/1021636\n",
            "Training Step: 86096  | time: 365.397s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0688768/1021636\n",
            "Training Step: 86097  | time: 365.400s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0688776/1021636\n",
            "Training Step: 86098  | time: 365.405s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0688784/1021636\n",
            "Training Step: 86099  | time: 365.408s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0688792/1021636\n",
            "Training Step: 86100  | time: 365.411s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0688800/1021636\n",
            "Training Step: 86101  | time: 365.416s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0688808/1021636\n",
            "Training Step: 86102  | time: 365.421s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0688816/1021636\n",
            "Training Step: 86103  | time: 365.424s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0688824/1021636\n",
            "Training Step: 86104  | time: 365.427s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0688832/1021636\n",
            "Training Step: 86105  | time: 365.435s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0688840/1021636\n",
            "Training Step: 86106  | time: 365.440s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0688848/1021636\n",
            "Training Step: 86107  | time: 365.443s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0688856/1021636\n",
            "Training Step: 86108  | time: 365.472s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0688864/1021636\n",
            "Training Step: 86109  | time: 365.483s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0688872/1021636\n",
            "Training Step: 86110  | time: 365.489s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0688880/1021636\n",
            "Training Step: 86111  | time: 365.494s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0688888/1021636\n",
            "Training Step: 86112  | time: 365.498s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0688896/1021636\n",
            "Training Step: 86113  | time: 365.501s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0688904/1021636\n",
            "Training Step: 86114  | time: 365.506s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0688912/1021636\n",
            "Training Step: 86115  | time: 365.509s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0688920/1021636\n",
            "Training Step: 86116  | time: 365.513s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0688928/1021636\n",
            "Training Step: 86117  | time: 365.517s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0688936/1021636\n",
            "Training Step: 86118  | time: 365.520s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0688944/1021636\n",
            "Training Step: 86119  | time: 365.524s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0688952/1021636\n",
            "Training Step: 86120  | time: 365.530s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0688960/1021636\n",
            "Training Step: 86121  | time: 365.534s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0688968/1021636\n",
            "Training Step: 86122  | time: 365.538s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0688976/1021636\n",
            "Training Step: 86123  | time: 365.542s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0688984/1021636\n",
            "Training Step: 86124  | time: 365.545s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0688992/1021636\n",
            "Training Step: 86125  | time: 365.551s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0689000/1021636\n",
            "Training Step: 86126  | time: 365.556s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0689008/1021636\n",
            "Training Step: 86127  | time: 365.560s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0689016/1021636\n",
            "Training Step: 86128  | time: 365.566s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0689024/1021636\n",
            "Training Step: 86129  | time: 365.571s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0689032/1021636\n",
            "Training Step: 86130  | time: 365.576s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0689040/1021636\n",
            "Training Step: 86131  | time: 365.581s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0689048/1021636\n",
            "Training Step: 86132  | time: 365.584s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0689056/1021636\n",
            "Training Step: 86133  | time: 365.589s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0689064/1021636\n",
            "Training Step: 86134  | time: 365.591s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0689072/1021636\n",
            "Training Step: 86135  | time: 365.595s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0689080/1021636\n",
            "Training Step: 86136  | time: 365.598s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0689088/1021636\n",
            "Training Step: 86137  | time: 365.602s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0689096/1021636\n",
            "Training Step: 86138  | time: 365.605s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0689104/1021636\n",
            "Training Step: 86139  | time: 365.609s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0689112/1021636\n",
            "Training Step: 86140  | time: 365.612s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0689120/1021636\n",
            "Training Step: 86141  | time: 365.615s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0689128/1021636\n",
            "Training Step: 86142  | time: 365.619s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0689136/1021636\n",
            "Training Step: 86143  | time: 365.623s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0689144/1021636\n",
            "Training Step: 86144  | time: 365.627s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0689152/1021636\n",
            "Training Step: 86145  | time: 365.630s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0689160/1021636\n",
            "Training Step: 86146  | time: 365.633s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0689168/1021636\n",
            "Training Step: 86147  | time: 365.638s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0689176/1021636\n",
            "Training Step: 86148  | time: 365.644s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0689184/1021636\n",
            "Training Step: 86149  | time: 365.648s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0689192/1021636\n",
            "Training Step: 86150  | time: 365.651s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0689200/1021636\n",
            "Training Step: 86151  | time: 365.655s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0689208/1021636\n",
            "Training Step: 86152  | time: 365.659s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0689216/1021636\n",
            "Training Step: 86153  | time: 365.664s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0689224/1021636\n",
            "Training Step: 86154  | time: 365.670s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0689232/1021636\n",
            "Training Step: 86155  | time: 365.674s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0689240/1021636\n",
            "Training Step: 86156  | time: 365.680s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0689248/1021636\n",
            "Training Step: 86157  | time: 365.686s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0689256/1021636\n",
            "Training Step: 86158  | time: 365.691s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0689264/1021636\n",
            "Training Step: 86159  | time: 365.695s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0689272/1021636\n",
            "Training Step: 86160  | time: 365.699s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0689280/1021636\n",
            "Training Step: 86161  | time: 365.703s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0689288/1021636\n",
            "Training Step: 86162  | time: 365.707s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0689296/1021636\n",
            "Training Step: 86163  | time: 365.709s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0689304/1021636\n",
            "Training Step: 86164  | time: 365.713s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0689312/1021636\n",
            "Training Step: 86165  | time: 365.716s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0689320/1021636\n",
            "Training Step: 86166  | time: 365.719s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0689328/1021636\n",
            "Training Step: 86167  | time: 365.723s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0689336/1021636\n",
            "Training Step: 86168  | time: 365.726s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0689344/1021636\n",
            "Training Step: 86169  | time: 365.731s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0689352/1021636\n",
            "Training Step: 86170  | time: 365.734s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0689360/1021636\n",
            "Training Step: 86171  | time: 365.738s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0689368/1021636\n",
            "Training Step: 86172  | time: 365.742s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0689376/1021636\n",
            "Training Step: 86173  | time: 365.747s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0689384/1021636\n",
            "Training Step: 86174  | time: 365.750s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0689392/1021636\n",
            "Training Step: 86175  | time: 365.754s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0689400/1021636\n",
            "Training Step: 86176  | time: 365.757s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0689408/1021636\n",
            "Training Step: 86177  | time: 365.761s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0689416/1021636\n",
            "Training Step: 86178  | time: 365.765s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0689424/1021636\n",
            "Training Step: 86179  | time: 365.768s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0689432/1021636\n",
            "Training Step: 86180  | time: 365.772s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0689440/1021636\n",
            "Training Step: 86181  | time: 365.776s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0689448/1021636\n",
            "Training Step: 86182  | time: 365.780s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0689456/1021636\n",
            "Training Step: 86183  | time: 365.784s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0689464/1021636\n",
            "Training Step: 86184  | time: 365.788s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0689472/1021636\n",
            "Training Step: 86185  | time: 365.792s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0689480/1021636\n",
            "Training Step: 86186  | time: 365.796s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0689488/1021636\n",
            "Training Step: 86187  | time: 365.800s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0689496/1021636\n",
            "Training Step: 86188  | time: 365.803s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0689504/1021636\n",
            "Training Step: 86189  | time: 365.807s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0689512/1021636\n",
            "Training Step: 86190  | time: 365.810s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0689520/1021636\n",
            "Training Step: 86191  | time: 365.814s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0689528/1021636\n",
            "Training Step: 86192  | time: 365.817s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0689536/1021636\n",
            "Training Step: 86193  | time: 365.820s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0689544/1021636\n",
            "Training Step: 86194  | time: 365.824s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0689552/1021636\n",
            "Training Step: 86195  | time: 365.827s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0689560/1021636\n",
            "Training Step: 86196  | time: 365.831s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0689568/1021636\n",
            "Training Step: 86197  | time: 365.834s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0689576/1021636\n",
            "Training Step: 86198  | time: 365.838s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0689584/1021636\n",
            "Training Step: 86199  | time: 365.842s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0689592/1021636\n",
            "Training Step: 86200  | time: 365.845s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0689600/1021636\n",
            "Training Step: 86201  | time: 365.849s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0689608/1021636\n",
            "Training Step: 86202  | time: 365.853s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0689616/1021636\n",
            "Training Step: 86203  | time: 365.857s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0689624/1021636\n",
            "Training Step: 86204  | time: 365.861s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0689632/1021636\n",
            "Training Step: 86205  | time: 365.864s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0689640/1021636\n",
            "Training Step: 86206  | time: 365.868s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0689648/1021636\n",
            "Training Step: 86207  | time: 365.872s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0689656/1021636\n",
            "Training Step: 86208  | time: 365.875s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0689664/1021636\n",
            "Training Step: 86209  | time: 365.880s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0689672/1021636\n",
            "Training Step: 86210  | time: 365.884s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0689680/1021636\n",
            "Training Step: 86211  | time: 365.888s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0689688/1021636\n",
            "Training Step: 86212  | time: 365.891s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0689696/1021636\n",
            "Training Step: 86213  | time: 365.894s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0689704/1021636\n",
            "Training Step: 86214  | time: 365.898s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0689712/1021636\n",
            "Training Step: 86215  | time: 365.901s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0689720/1021636\n",
            "Training Step: 86216  | time: 365.904s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0689728/1021636\n",
            "Training Step: 86217  | time: 365.908s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0689736/1021636\n",
            "Training Step: 86218  | time: 365.912s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0689744/1021636\n",
            "Training Step: 86219  | time: 365.915s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0689752/1021636\n",
            "Training Step: 86220  | time: 365.919s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0689760/1021636\n",
            "Training Step: 86221  | time: 365.923s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0689768/1021636\n",
            "Training Step: 86222  | time: 365.926s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0689776/1021636\n",
            "Training Step: 86223  | time: 365.929s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0689784/1021636\n",
            "Training Step: 86224  | time: 365.933s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0689792/1021636\n",
            "Training Step: 86225  | time: 365.937s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0689800/1021636\n",
            "Training Step: 86226  | time: 365.940s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0689808/1021636\n",
            "Training Step: 86227  | time: 365.943s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0689816/1021636\n",
            "Training Step: 86228  | time: 365.946s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0689824/1021636\n",
            "Training Step: 86229  | time: 365.949s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0689832/1021636\n",
            "Training Step: 86230  | time: 365.953s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0689840/1021636\n",
            "Training Step: 86231  | time: 365.956s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0689848/1021636\n",
            "Training Step: 86232  | time: 365.960s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0689856/1021636\n",
            "Training Step: 86233  | time: 365.964s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0689864/1021636\n",
            "Training Step: 86234  | time: 365.968s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0689872/1021636\n",
            "Training Step: 86235  | time: 365.971s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0689880/1021636\n",
            "Training Step: 86236  | time: 365.974s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0689888/1021636\n",
            "Training Step: 86237  | time: 365.979s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0689896/1021636\n",
            "Training Step: 86238  | time: 365.983s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0689904/1021636\n",
            "Training Step: 86239  | time: 365.986s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0689912/1021636\n",
            "Training Step: 86240  | time: 365.990s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0689920/1021636\n",
            "Training Step: 86241  | time: 365.995s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0689928/1021636\n",
            "Training Step: 86242  | time: 365.999s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0689936/1021636\n",
            "Training Step: 86243  | time: 366.001s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0689944/1021636\n",
            "Training Step: 86244  | time: 366.007s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0689952/1021636\n",
            "Training Step: 86245  | time: 366.012s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0689960/1021636\n",
            "Training Step: 86246  | time: 366.016s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0689968/1021636\n",
            "Training Step: 86247  | time: 366.023s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0689976/1021636\n",
            "Training Step: 86248  | time: 366.026s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0689984/1021636\n",
            "Training Step: 86249  | time: 366.029s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0689992/1021636\n",
            "Training Step: 86250  | time: 366.035s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0690000/1021636\n",
            "Training Step: 86251  | time: 366.040s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0690008/1021636\n",
            "Training Step: 86252  | time: 366.044s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0690016/1021636\n",
            "Training Step: 86253  | time: 366.049s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0690024/1021636\n",
            "Training Step: 86254  | time: 366.055s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0690032/1021636\n",
            "Training Step: 86255  | time: 366.063s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0690040/1021636\n",
            "Training Step: 86256  | time: 366.068s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0690048/1021636\n",
            "Training Step: 86257  | time: 366.074s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0690056/1021636\n",
            "Training Step: 86258  | time: 366.082s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0690064/1021636\n",
            "Training Step: 86259  | time: 366.088s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0690072/1021636\n",
            "Training Step: 86260  | time: 366.094s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0690080/1021636\n",
            "Training Step: 86261  | time: 366.100s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0690088/1021636\n",
            "Training Step: 86262  | time: 366.107s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0690096/1021636\n",
            "Training Step: 86263  | time: 366.115s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0690104/1021636\n",
            "Training Step: 86264  | time: 366.120s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0690112/1021636\n",
            "Training Step: 86265  | time: 366.124s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0690120/1021636\n",
            "Training Step: 86266  | time: 366.129s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0690128/1021636\n",
            "Training Step: 86267  | time: 366.135s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0690136/1021636\n",
            "Training Step: 86268  | time: 366.140s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0690144/1021636\n",
            "Training Step: 86269  | time: 366.146s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0690152/1021636\n",
            "Training Step: 86270  | time: 366.152s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0690160/1021636\n",
            "Training Step: 86271  | time: 366.157s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0690168/1021636\n",
            "Training Step: 86272  | time: 366.159s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0690176/1021636\n",
            "Training Step: 86273  | time: 366.166s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0690184/1021636\n",
            "Training Step: 86274  | time: 366.175s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0690192/1021636\n",
            "Training Step: 86275  | time: 366.182s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0690200/1021636\n",
            "Training Step: 86276  | time: 366.188s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0690208/1021636\n",
            "Training Step: 86277  | time: 366.193s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0690216/1021636\n",
            "Training Step: 86278  | time: 366.198s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0690224/1021636\n",
            "Training Step: 86279  | time: 366.202s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0690232/1021636\n",
            "Training Step: 86280  | time: 366.205s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0690240/1021636\n",
            "Training Step: 86281  | time: 366.207s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0690248/1021636\n",
            "Training Step: 86282  | time: 366.210s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0690256/1021636\n",
            "Training Step: 86283  | time: 366.212s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0690264/1021636\n",
            "Training Step: 86284  | time: 366.214s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0690272/1021636\n",
            "Training Step: 86285  | time: 366.217s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0690280/1021636\n",
            "Training Step: 86286  | time: 366.220s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0690288/1021636\n",
            "Training Step: 86287  | time: 366.222s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0690296/1021636\n",
            "Training Step: 86288  | time: 366.225s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0690304/1021636\n",
            "Training Step: 86289  | time: 366.229s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0690312/1021636\n",
            "Training Step: 86290  | time: 366.232s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0690320/1021636\n",
            "Training Step: 86291  | time: 366.234s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0690328/1021636\n",
            "Training Step: 86292  | time: 366.237s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0690336/1021636\n",
            "Training Step: 86293  | time: 366.241s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0690344/1021636\n",
            "Training Step: 86294  | time: 366.244s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0690352/1021636\n",
            "Training Step: 86295  | time: 366.247s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0690360/1021636\n",
            "Training Step: 86296  | time: 366.250s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0690368/1021636\n",
            "Training Step: 86297  | time: 366.256s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0690376/1021636\n",
            "Training Step: 86298  | time: 366.259s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0690384/1021636\n",
            "Training Step: 86299  | time: 366.262s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0690392/1021636\n",
            "Training Step: 86300  | time: 366.265s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0690400/1021636\n",
            "Training Step: 86301  | time: 366.268s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0690408/1021636\n",
            "Training Step: 86302  | time: 366.271s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0690416/1021636\n",
            "Training Step: 86303  | time: 366.274s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0690424/1021636\n",
            "Training Step: 86304  | time: 366.277s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0690432/1021636\n",
            "Training Step: 86305  | time: 366.280s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0690440/1021636\n",
            "Training Step: 86306  | time: 366.283s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0690448/1021636\n",
            "Training Step: 86307  | time: 366.286s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0690456/1021636\n",
            "Training Step: 86308  | time: 366.289s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0690464/1021636\n",
            "Training Step: 86309  | time: 366.292s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0690472/1021636\n",
            "Training Step: 86310  | time: 366.295s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0690480/1021636\n",
            "Training Step: 86311  | time: 366.298s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0690488/1021636\n",
            "Training Step: 86312  | time: 366.301s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - binary_acc: 1.0000 -- iter: 0690496/1021636\n"
          ]
        }
      ],
      "source": [
        "# resetting underlying graph data\n",
        "#tf.reset_default_graph()\n",
        "from tensorflow.python.framework import ops\n",
        "ops.reset_default_graph()\n",
        "ops.reset_default_graph()\n",
        "# Building neural network\n",
        "net = tflearn.input_data(shape=[None, len(train_x[0])])\n",
        "net = tflearn.fully_connected(net, 10)\n",
        "net = tflearn.fully_connected(net, 10)\n",
        "net = tflearn.fully_connected(net, len(train_y[0]), activation='softmax')\n",
        "net = tflearn.regression(net)\n",
        "\n",
        "# Defining model and setting up tensorboard\n",
        "model = tflearn.DNN(net, tensorboard_dir='tflearn_logs')\n",
        "\n",
        "# Start training\n",
        "model.fit(train_x, train_y, n_epoch=1000, batch_size=8, show_metric=True)\n",
        "model.save('model.tflearn')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E5bb2o2ueKJc"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "pickle.dump( {'words':words, 'classes':classes, 'train_x':train_x, 'train_y':train_y}, open( \"training_data\", \"wb\" ) )\n",
        "\n",
        "\n",
        "# restoring all the data structures\n",
        "data = pickle.load( open( \"training_data\", \"rb\" ) )\n",
        "words = data['words']\n",
        "classes = data['classes']\n",
        "train_x = data['train_x']\n",
        "train_y = data['train_y']\n",
        "\n",
        "\n",
        "with open('Intent.json') as json_data:\n",
        "    intents = json.load(json_data)\n",
        "\n",
        "\n",
        "# load the saved model\n",
        "model.load('./model.tflearn')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LNgr_5JBeQZF"
      },
      "outputs": [],
      "source": [
        "def clean_up_sentence(sentence):\n",
        "    # tokenizing the pattern\n",
        "    sentence_words = nltk.word_tokenize(sentence)\n",
        "    # stemming each word\n",
        "    sentence_words = [stemmer.stem(word.lower()) for word in sentence_words]\n",
        "    return sentence_words\n",
        "\n",
        "# returning bag of words array: 0 or 1 for each word in the bag that exists in the sentence\n",
        "def bow(sentence, words, show_details=False):\n",
        "    # tokenizing the pattern\n",
        "    sentence_words = clean_up_sentence(sentence)\n",
        "    # generating bag of words\n",
        "    bag = [0]*len(words)\n",
        "    for s in sentence_words:\n",
        "        for i,w in enumerate(words):\n",
        "            if w == s:\n",
        "                bag[i] = 1\n",
        "                if show_details:\n",
        "                    print (\"found in bag: %s\" % w)\n",
        "\n",
        "    return(np.array(bag))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RuOPrnIqeRBr"
      },
      "outputs": [],
      "source": [
        "ERROR_THRESHOLD = 0.30\n",
        "def classify(sentence):\n",
        "    # generate probabilities from the model\n",
        "    results = model.predict([bow(sentence, words)])[0]\n",
        "    # filter out predictions below a threshold\n",
        "    results = [[i,r] for i,r in enumerate(results) if r>ERROR_THRESHOLD]\n",
        "    # sort by strength of probability\n",
        "    results.sort(key=lambda x: x[1], reverse=True)\n",
        "    return_list = []\n",
        "    for r in results:\n",
        "        return_list.append((classes[r[0]], r[1]))\n",
        "    # return tuple of intent and probability\n",
        "    return return_list\n",
        "\n",
        "def response(sentence, userID='123', show_details=False):\n",
        "    results = classify(sentence)   #here we get the value in terms of stack\n",
        "    # if we have a classification then find the matching intent tag\n",
        "    if results:\n",
        "        # loop as long as there are matches to process\n",
        "        while results:\n",
        "            for i in intents['intents']:\n",
        "                # find a tag matching the first result\n",
        "                if i['tag'] == results[0][0]:\n",
        "                    # a random response from the intent\n",
        "                    return print(random.choice(i['responses']))\n",
        "\n",
        "            results.pop(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RjfePBAOecay"
      },
      "outputs": [],
      "source": [
        "classify('What are you hours of operation?')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GLcxWLhKehkY"
      },
      "outputs": [],
      "source": [
        "response('What are you hours of operation?')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UV_LlKkmehnk"
      },
      "outputs": [],
      "source": [
        "response('What is menu for today?')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9f0BXeXaehrF"
      },
      "outputs": [],
      "source": [
        "#Some of other context free responses.\n",
        "response('Do you accept Credit Card?')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N1NQwgk2esim"
      },
      "outputs": [],
      "source": [
        "response('Where can we locate you?')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mDlGd2Laesl8"
      },
      "outputs": [],
      "source": [
        "response('That is helpful')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1MThYdQMehuc"
      },
      "outputs": [],
      "source": [
        "response('Bye')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7yM5GLPqe1he"
      },
      "outputs": [],
      "source": [
        "#Adding some context to the conversation i.e. Contexualization for altering question and intents etc.\n",
        "# create a data structure to hold user context\n",
        "context = {}\n",
        "\n",
        "ERROR_THRESHOLD = 0.25\n",
        "def classify(sentence):\n",
        "    # generate probabilities from the model\n",
        "    results = model.predict([bow(sentence, words)])[0]\n",
        "    # filter out predictions below a threshold\n",
        "    results = [[i,r] for i,r in enumerate(results) if r>ERROR_THRESHOLD]\n",
        "    # sort by strength of probability\n",
        "    results.sort(key=lambda x: x[1], reverse=True)\n",
        "    return_list = []\n",
        "    for r in results:\n",
        "        return_list.append((classes[r[0]], r[1]))\n",
        "    # return tuple of intent and probability\n",
        "    return return_list\n",
        "\n",
        "def response(sentence, userID='123', show_details=False):\n",
        "    results = classify(sentence)\n",
        "    # if we have a classification then find the matching intent tag\n",
        "    if results:\n",
        "        # loop as long as there are matches to process\n",
        "        while results:\n",
        "            for i in intents['intents']:\n",
        "                # find a tag matching the first result\n",
        "                if i['tag'] == results[0][0]:\n",
        "                    # set context for this intent if necessary\n",
        "                    if 'context_set' in i:\n",
        "                        if show_details: print ('context:', i['context_set'])\n",
        "                        context[userID] = i['context_set']\n",
        "\n",
        "                    # check if this intent is contextual and applies to this user's conversation\n",
        "                    if not 'context_filter' in i or \\\n",
        "                        (userID in context and 'context_filter' in i and i['context_filter'] == context[userID]):\n",
        "                        if show_details: print ('tag:', i['tag'])\n",
        "                        # a random response from the intent\n",
        "                        return print(random.choice(i['responses']))\n",
        "\n",
        "            results.pop(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8zB_3qMVe8aO"
      },
      "outputs": [],
      "source": [
        "response('Can you please let me know the delivery options?')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cOhhY41uqlJx"
      },
      "outputs": [],
      "source": [
        "response('What is menu for today?')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CRAvkV-7qlEC"
      },
      "outputs": [],
      "source": [
        "context"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "40GjMAD6qk7K"
      },
      "outputs": [],
      "source": [
        "response(\"Hi there!\", show_details=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HgZ9yOCwqk0Y"
      },
      "outputs": [],
      "source": [
        "response('What is menu for today?')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}